{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from data_loader import Dataset,Options\n",
    "import models.unet_normals as unet\n",
    "from tensorboardX import SummaryWriter\n",
    "import OpenEXR, Imath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Options\n",
    "Set the various parameters:\n",
    "- dataroot: The folder where the training data is stored\n",
    "- file_list: List of filenames of images for training\n",
    "- batchSize: Batch size for model\n",
    "- shuffle: If true, will shuffle the dataset\n",
    "- phase: If 'train', then it's in training mode.\n",
    "- num_epochs: Number of epochs to train the model for\n",
    "- imsize: Dimensions of the image (square)\n",
    "- num_classes: Num of classes in the output\n",
    "- gpu: Which GPU device to use\n",
    "- logs_path: The path where the log files (tensorboard) will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPT():\n",
    "    def __init__(self):\n",
    "        self.dataroot = './data/'\n",
    "        self.file_list = './data/datalist'\n",
    "        self.batchSize = 24\n",
    "        self.shuffle = True\n",
    "        self.phase = 'train'\n",
    "        self.num_epochs = 1000\n",
    "        self.imsize = 224\n",
    "        self.num_classes = int(3)\n",
    "        self.gpu = '0'\n",
    "        self.logs_path = 'logs/exp4'\n",
    "        self.use_pretrained = False\n",
    "\n",
    "opt = OPT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling the dataset\n"
     ]
    }
   ],
   "source": [
    "###################### Options #############################\n",
    "phase = opt.phase\n",
    "device = torch.device(\"cuda:\"+ opt.gpu if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###################### TensorBoardX #############################\n",
    "if os.path.exists(opt.logs_path):\n",
    "    raise Exception('The folder \\\"{}\\\" already exists! Define a new log path or delete old contents.'.format(opt.logs_path))\n",
    "    \n",
    "writer = SummaryWriter(opt.logs_path, comment='create-graph')\n",
    "graph_created = False\n",
    "\n",
    "###################### DataLoader #############################\n",
    "dataloader = Dataset(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "We use a UNet model. The last few layers of this model are modified to return a 3 channel image, containing the x,y,z values of surface normal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### ModelBuilder #############################\n",
    "model = unet.Unet(num_classes=opt.num_classes)\n",
    "\n",
    "# Load weights from checkpoint\n",
    "if (opt.use_pretrained == True):\n",
    "    checkpoint_path = 'logs/exp2/checkpoints/checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "###################### Setup Optimazation #############################\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "###################### Loss fuction #############################\n",
    "'''\n",
    "@input: The 2 vectors whose cosine loss is to be calculated\n",
    "The dimensions of the matrices are expected to be (batchSize, 3, imsize, imsize). \n",
    "\n",
    "@return: \n",
    "elementwise_mean: will return the sum of all losses divided by num of elements\n",
    "none: The loss will be calculated to be of size (batchSize, imsize, imsize) containing cosine loss of each pixel\n",
    "'''\n",
    "def loss_fn(input_vec, target_vec, reduction='elementwise_mean'):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    loss_val = 1.0 - cos(input_vec, target_vec)\n",
    "    if (reduction=='elementwise_mean'):\n",
    "        return torch.mean(loss_val)\n",
    "    elif (reduction=='none'):\n",
    "        return loss_val\n",
    "    else:\n",
    "        raise Exception('Warning! The reduction is invalid. Please use \\'elementwise_mean\\' or \\'none\\''.format())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/999\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shrek/anaconda3/envs/pytorch-cuda9.0/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/shrek/anaconda3/envs/pytorch-cuda9.0/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1442\n",
      "Epoch 1/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.2863\n",
      "Epoch 2/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1843\n",
      "Epoch 3/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1750\n",
      "Epoch 4/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1766\n",
      "Epoch 5/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1737\n",
      "Epoch 6/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1706\n",
      "Epoch 7/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1645\n",
      "Epoch 8/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1860\n",
      "Epoch 9/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1782\n",
      "Epoch 10/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1632\n",
      "Epoch 11/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1812\n",
      "Epoch 12/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1602\n",
      "Epoch 13/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1589\n",
      "Epoch 14/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1704\n",
      "Epoch 15/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1858\n",
      "Epoch 16/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1700\n",
      "Epoch 17/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1623\n",
      "Epoch 18/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1816\n",
      "Epoch 19/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1843\n",
      "Epoch 20/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1777\n",
      "Epoch 21/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1620\n",
      "Epoch 22/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1671\n",
      "Epoch 23/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1663\n",
      "Epoch 24/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1837\n",
      "Epoch 25/999\n",
      "----------\n",
      "train Loss: 0.1672\n",
      "Epoch 26/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1661\n",
      "Epoch 27/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1780\n",
      "Epoch 28/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1700\n",
      "Epoch 29/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1870\n",
      "Epoch 30/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1501\n",
      "Epoch 31/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1686\n",
      "Epoch 32/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1855\n",
      "Epoch 33/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1546\n",
      "Epoch 34/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.2071\n",
      "Epoch 35/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1721\n",
      "Epoch 36/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1780\n",
      "Epoch 37/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1590\n",
      "Epoch 38/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1555\n",
      "Epoch 39/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1839\n",
      "Epoch 40/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1573\n",
      "Epoch 41/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1827\n",
      "Epoch 42/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1911\n",
      "Epoch 43/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1559\n",
      "Epoch 44/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1734\n",
      "Epoch 45/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1577\n",
      "Epoch 46/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1805\n",
      "Epoch 47/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1648\n",
      "Epoch 48/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1876\n",
      "Epoch 49/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1575\n",
      "Epoch 50/999\n",
      "----------\n",
      "train Loss: 0.1730\n",
      "Epoch 51/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1731\n",
      "Epoch 52/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1751\n",
      "Epoch 53/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1806\n",
      "Epoch 54/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1557\n",
      "Epoch 55/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1588\n",
      "Epoch 56/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1915\n",
      "Epoch 57/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1627\n",
      "Epoch 58/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1865\n",
      "Epoch 59/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1672\n",
      "Epoch 60/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1684\n",
      "Epoch 61/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1624\n",
      "Epoch 62/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1941\n",
      "Epoch 63/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1470\n",
      "Epoch 64/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1920\n",
      "Epoch 65/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1545\n",
      "Epoch 66/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1594\n",
      "Epoch 67/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1769\n",
      "Epoch 68/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1981\n",
      "Epoch 69/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1496\n",
      "Epoch 70/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1698\n",
      "Epoch 71/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1635\n",
      "Epoch 72/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1793\n",
      "Epoch 73/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1690\n",
      "Epoch 74/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1739\n",
      "Epoch 75/999\n",
      "----------\n",
      "train Loss: 0.1717\n",
      "Epoch 76/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1719\n",
      "Epoch 77/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1750\n",
      "Epoch 78/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1695\n",
      "Epoch 79/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1773\n",
      "Epoch 80/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1636\n",
      "Epoch 81/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1612\n",
      "Epoch 82/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1900\n",
      "Epoch 83/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1648\n",
      "Epoch 84/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1805\n",
      "Epoch 85/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1574\n",
      "Epoch 86/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1569\n",
      "Epoch 87/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1880\n",
      "Epoch 88/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1769\n",
      "Epoch 89/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1593\n",
      "Epoch 90/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1765\n",
      "Epoch 91/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1676\n",
      "Epoch 92/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1828\n",
      "Epoch 93/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1456\n",
      "Epoch 94/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1858\n",
      "Epoch 95/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1586\n",
      "Epoch 96/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1646\n",
      "Epoch 97/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1842\n",
      "Epoch 98/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1580\n",
      "Epoch 99/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1707\n",
      "Epoch 100/999\n",
      "----------\n",
      "train Loss: 0.1730\n",
      "Epoch 101/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1673\n",
      "Epoch 102/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1725\n",
      "Epoch 103/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1618\n",
      "Epoch 104/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1842\n",
      "Epoch 105/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1593\n",
      "Epoch 106/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1799\n",
      "Epoch 107/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1626\n",
      "Epoch 108/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1681\n",
      "Epoch 109/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1779\n",
      "Epoch 110/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1657\n",
      "Epoch 111/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1894\n",
      "Epoch 112/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1346\n",
      "Epoch 113/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.2012\n",
      "Epoch 114/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1492\n",
      "Epoch 115/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1743\n",
      "Epoch 116/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1564\n",
      "Epoch 117/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1729\n",
      "Epoch 118/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1585\n",
      "Epoch 119/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1802\n",
      "Epoch 120/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1427\n",
      "Epoch 121/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1742\n",
      "Epoch 122/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1786\n",
      "Epoch 123/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1638\n",
      "Epoch 124/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1591\n",
      "Epoch 125/999\n",
      "----------\n",
      "train Loss: 0.1713\n",
      "Epoch 126/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1513\n",
      "Epoch 127/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1664\n",
      "Epoch 128/999\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling the dataset\n",
      "train Loss: 0.1763\n",
      "Epoch 129/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1621\n",
      "Epoch 130/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1642\n",
      "Epoch 131/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1462\n",
      "Epoch 132/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1806\n",
      "Epoch 133/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1424\n",
      "Epoch 134/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1855\n",
      "Epoch 135/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1598\n",
      "Epoch 136/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1671\n",
      "Epoch 137/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1483\n",
      "Epoch 138/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1617\n",
      "Epoch 139/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1432\n",
      "Epoch 140/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1647\n",
      "Epoch 141/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1793\n",
      "Epoch 142/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1849\n",
      "Epoch 143/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1401\n",
      "Epoch 144/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1621\n",
      "Epoch 145/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1640\n",
      "Epoch 146/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1726\n",
      "Epoch 147/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1501\n",
      "Epoch 148/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "train Loss: 0.1577\n",
      "Epoch 149/999\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "###################### Train Model #############################\n",
    "# Calculate total iter_num\n",
    "total_iter_num = 0\n",
    "\n",
    "for epoch in range(opt.num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, opt.num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Iterate over data.\n",
    "    for i in range(int(dataloader.size()/opt.batchSize)):\n",
    "        total_iter_num += 1\n",
    "        \n",
    "        # Get data\n",
    "        inputs, labels =  dataloader.get_batch()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #ToDo: get labels into correct format\n",
    "        \n",
    "        ## Create Graph ##\n",
    "        if graph_created == False:\n",
    "            graph_created = True\n",
    "            writer.add_graph(model, inputs, verbose=False)\n",
    "        \n",
    "        # Forward + Backward Prop\n",
    "        optimizer.zero_grad()\n",
    "        torch.set_grad_enabled(True)\n",
    "        normal_vectors = model(inputs)\n",
    "        normal_vectors_norm = nn.functional.normalize(normal_vectors, p=2, dim=1)\n",
    "        \n",
    "        loss = loss_fn(normal_vectors_norm, labels, reduction='elementwise_mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar('running_loss', running_loss, total_iter_num)\n",
    "        writer.add_scalar('loss', loss.item(), total_iter_num)\n",
    "\n",
    "    epoch_loss = running_loss / (dataloader.size()/opt.batchSize)\n",
    "    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    directory = opt.logs_path+'/checkpoints/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    if (epoch % 25 == 0):\n",
    "        filename = opt.logs_path + '/checkpoints/checkpoint-ep_{}-iter_{}.pth'.format(epoch,i)\n",
    "        torch.save(model.state_dict(), filename)\n",
    "\n",
    "# Save final Checkpoint\n",
    "filename = opt.logs_path + '/checkpoints/checkpoint.pth'\n",
    "torch.save(model.state_dict(), filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the speed\n",
    "\n",
    "Run the script below to get the estimate of fps you can achieve on your machine.\n",
    "For this experiment we used ```timeit``` magic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import models.unet_normals as unet\n",
    "import numpy as np\n",
    "from data_loader import Dataset,Options\n",
    "\n",
    "class OPT():\n",
    "    def __init__(self):\n",
    "        self.dataroot = './data/'\n",
    "        self.file_list = './data/datalist'\n",
    "        self.batchSize = 24\n",
    "        self.shuffle = True\n",
    "        self.phase = 'eval'\n",
    "        self.num_epochs = 1000\n",
    "        self.imsize = 224\n",
    "        self.num_classes = int(3)\n",
    "        self.gpu = '0'\n",
    "        self.logs_path = 'logs/exp3'\n",
    "\n",
    "opt = OPT()\n",
    "dataloader = Dataset(opt)\n",
    "\n",
    "device = torch.device(\"cuda:\"+ opt.gpu if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_path = opt.logs_path + '/checkpoints/checkpoint.pth'\n",
    "\n",
    "# exr_ground_truth_path = 'data/000000011-normals.exr'\n",
    "for i in range(0, 61):\n",
    "    # Open and Transform Img\n",
    "    img_not_preprocessed = Image.open('data/test-imgs/%09d-rgb.jpg'%(i)).convert(\"RGB\")\n",
    "    img = dataloader.transformImage(img_not_preprocessed)\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    # Send img to device\n",
    "    img = Variable(img.to(device))\n",
    "\n",
    "    # Load Model\n",
    "    fcn = unet.Unet(num_classes=opt.num_classes)\n",
    "    fcn.load_state_dict(torch.load(checkpoint_path))\n",
    "    fcn.to(device)\n",
    "    fcn.eval()\n",
    "\n",
    "    # Inference\n",
    "    res = fcn(img)\n",
    "    res_norm = nn.functional.normalize(res, p=2, dim=1)\n",
    "    output = res_norm.squeeze(0)\n",
    "    output = output.data.cpu().numpy()\n",
    "    output2 = output.copy()\n",
    "    output3 = output.copy()\n",
    "    print('output.shape:')\n",
    "    print(output.shape)\n",
    "\n",
    "    # Display Results\n",
    "    # Orig image\n",
    "    plt.imshow(img_not_preprocessed)\n",
    "    plt.show()\n",
    "\n",
    "    # Output Normals visualized with RGB\n",
    "    camera_normal_rgb = dataloader.normals_to_rgb(output2)\n",
    "    camera_normal_rgb = np.transpose(camera_normal_rgb, (1,2,0))\n",
    "    plt.imshow(camera_normal_rgb)\n",
    "    plt.show()\n",
    "    plt.imsave('data/test-results/%09d-normals-result-negatives-removed.png'%(i), camera_normal_rgb)\n",
    "\n",
    "    # Output Normals visualized with RGB, negative normal values represented\n",
    "    camera_normal_rgb2 = dataloader.normal_to_rgb_with_negatives(output3)\n",
    "    camera_normal_rgb2 = np.transpose(camera_normal_rgb2, (1,2,0))\n",
    "    plt.imshow(camera_normal_rgb2)\n",
    "    plt.show()\n",
    "    plt.imsave('data/test-results/%09d-normals-result.png'%(i), camera_normal_rgb)\n",
    "\n",
    "    # Ground Truth Normals as RGB\n",
    "    truth_normal = dataloader.exr_loader(exr_ground_truth_path, ndim=3)\n",
    "    truth_normal_rgb = dataloader.normals_to_rgb(truth_normal)\n",
    "    truth_normal_rgb = np.transpose(truth_normal_rgb, (1,2,0))\n",
    "    plt.imshow(truth_normal_rgb)\n",
    "    plt.show()\n",
    "    plt.imsave('data/test-results/%09d-normals-groundtruth-origsize.png'%(i), truth_normal_rgb)\n",
    "\n",
    "    # Ground Truth Normals as RGB, with negatives and resized\n",
    "    truth_normal = 0\n",
    "    truth_normal_rgb = 0\n",
    "    truth_normal = dataloader.exr_loader('data/test-imgs/%09d-normals.exr'%(i), ndim=3)\n",
    "    \n",
    "    label_tensor = torch.from_numpy(truth_normal)\n",
    "    label_img = transforms.ToPILImage(mode='RGB')(label_tensor)\n",
    "    label_cropped = self.transformLabel(label_img)\n",
    "    label_np = label_cropped.numpy()\n",
    "    \n",
    "    truth_normal_rgb = dataloader.normal_to_rgb_with_negatives(label_np)\n",
    "    truth_normal_rgb = np.transpose(truth_normal_rgb, (1,2,0))\n",
    "    plt.imshow(truth_normal_rgb)\n",
    "    plt.show()\n",
    "    plt.imsave('data/test-results/%09d-normals-groundtruth.png'%(i), truth_normal_rgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
