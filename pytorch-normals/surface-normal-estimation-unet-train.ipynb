{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from data_loader import Dataset,Options\n",
    "import models.unet_normals as unet\n",
    "from tensorboardX import SummaryWriter\n",
    "# import OpenEXR, Imath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Options\n",
    "Set the various parameters:\n",
    "- dataroot: The folder where the training data is stored\n",
    "- file_list: List of filenames of images for training\n",
    "- batchSize: Batch size for model\n",
    "- shuffle: If true, will shuffle the dataset\n",
    "- phase: If 'train', then it's in training mode.\n",
    "- num_epochs: Number of epochs to train the model for\n",
    "- imsize: Dimensions of the image (square)\n",
    "- num_classes: Num of classes in the output\n",
    "- gpu: Which GPU device to use\n",
    "- logs_path: The path where the log files (tensorboard) will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPT():\n",
    "    def __init__(self):\n",
    "        self.dataroot = './data/'\n",
    "        self.file_list = './data/datalist'\n",
    "        self.batchSize = 32\n",
    "        self.shuffle = True\n",
    "        self.phase = 'train'\n",
    "        self.num_epochs = 500\n",
    "        self.imsize = 224\n",
    "        self.num_classes = int(3)\n",
    "        self.gpu = '0'\n",
    "        self.logs_path = 'logs/exp9'\n",
    "        self.use_pretrained = False\n",
    "\n",
    "opt = OPT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling the dataset\n"
     ]
    }
   ],
   "source": [
    "###################### Options #############################\n",
    "phase = opt.phase\n",
    "device = torch.device(\"cuda:\"+ opt.gpu if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###################### TensorBoardX #############################\n",
    "if os.path.exists(opt.logs_path):\n",
    "    raise Exception('The folder \\\"{}\\\" already exists! Define a new log path or delete old contents.'.format(opt.logs_path))\n",
    "    \n",
    "writer = SummaryWriter(opt.logs_path, comment='create-graph')\n",
    "graph_created = False\n",
    "\n",
    "###################### DataLoader #############################\n",
    "dataloader = Dataset(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "We use a UNet model. The last few layers of this model are modified to return a 3 channel image, containing the x,y,z values of surface normal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### ModelBuilder #############################\n",
    "model = unet.Unet(num_classes=opt.num_classes)\n",
    "\n",
    "# Load weights from checkpoint\n",
    "if (opt.use_pretrained == True):\n",
    "    checkpoint_path = 'logs/exp7/checkpoints/checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "###################### Setup Optimazation #############################\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "###################### Loss fuction #############################\n",
    "'''\n",
    "@input: The 2 vectors whose cosine loss is to be calculated\n",
    "The dimensions of the matrices are expected to be (batchSize, 3, imsize, imsize). \n",
    "\n",
    "@return: \n",
    "elementwise_mean: will return the sum of all losses divided by num of elements\n",
    "none: The loss will be calculated to be of size (batchSize, imsize, imsize) containing cosine loss of each pixel\n",
    "'''\n",
    "def loss_fn(input_vec, target_vec, reduction='elementwise_mean'):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    loss_val = 1.0 - cos(input_vec, target_vec)\n",
    "    if (reduction=='elementwise_mean'):\n",
    "        return torch.mean(loss_val)\n",
    "    elif (reduction=='none'):\n",
    "        return loss_val\n",
    "    else:\n",
    "        raise Exception('Warning! The reduction is invalid. Please use \\'elementwise_mean\\' or \\'none\\''.format())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "Epoch0 Batch0 Loss: 1.4844\n",
      "Epoch0 Batch10 Loss: 0.6621\n",
      "Epoch0 Batch20 Loss: 0.2250\n",
      "Epoch0 Batch30 Loss: 0.2829\n",
      "Epoch0 Batch40 Loss: 0.2167\n",
      "Epoch0 Batch50 Loss: 0.2113\n",
      "Epoch0 Batch60 Loss: 0.2824\n",
      "Epoch0 Batch70 Loss: 0.2510\n",
      "train Loss: 0.3921\n",
      "Epoch 1/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch1 Batch0 Loss: 0.2787\n",
      "Epoch1 Batch10 Loss: 0.2527\n",
      "Epoch1 Batch20 Loss: 0.2630\n",
      "Epoch1 Batch30 Loss: 0.3045\n",
      "Epoch1 Batch40 Loss: 0.2829\n",
      "Epoch1 Batch50 Loss: 0.2238\n",
      "Epoch1 Batch60 Loss: 0.2352\n",
      "Epoch1 Batch70 Loss: 0.2280\n",
      "train Loss: 0.2519\n",
      "Epoch 2/499\n",
      "----------\n",
      "Epoch2 Batch0 Loss: 0.2026\n",
      "shuffling the dataset\n",
      "Epoch2 Batch10 Loss: 0.2089\n",
      "Epoch2 Batch20 Loss: 0.2300\n",
      "Epoch2 Batch30 Loss: 0.2468\n",
      "Epoch2 Batch40 Loss: 0.2358\n",
      "Epoch2 Batch50 Loss: 0.2347\n",
      "Epoch2 Batch60 Loss: 0.2541\n",
      "Epoch2 Batch70 Loss: 0.2400\n",
      "train Loss: 0.2470\n",
      "Epoch 3/499\n",
      "----------\n",
      "Epoch3 Batch0 Loss: 0.2367\n",
      "shuffling the dataset\n",
      "Epoch3 Batch10 Loss: 0.1985\n",
      "Epoch3 Batch20 Loss: 0.2217\n",
      "Epoch3 Batch30 Loss: 0.2370\n",
      "Epoch3 Batch40 Loss: 0.2366\n",
      "Epoch3 Batch50 Loss: 0.2141\n",
      "Epoch3 Batch60 Loss: 0.2080\n",
      "Epoch3 Batch70 Loss: 0.2242\n",
      "train Loss: 0.2275\n",
      "Epoch 4/499\n",
      "----------\n",
      "Epoch4 Batch0 Loss: 0.2482\n",
      "shuffling the dataset\n",
      "Epoch4 Batch10 Loss: 0.2207\n",
      "Epoch4 Batch20 Loss: 0.2258\n",
      "Epoch4 Batch30 Loss: 0.2238\n",
      "Epoch4 Batch40 Loss: 0.1781\n",
      "Epoch4 Batch50 Loss: 0.2296\n",
      "Epoch4 Batch60 Loss: 0.1853\n",
      "Epoch4 Batch70 Loss: 0.1828\n",
      "train Loss: 0.2059\n",
      "Epoch 5/499\n",
      "----------\n",
      "Epoch5 Batch0 Loss: 0.2037\n",
      "shuffling the dataset\n",
      "Epoch5 Batch10 Loss: 0.2269\n",
      "Epoch5 Batch20 Loss: 0.1622\n",
      "Epoch5 Batch30 Loss: 0.2135\n",
      "Epoch5 Batch40 Loss: 0.1648\n",
      "Epoch5 Batch50 Loss: 0.2366\n",
      "Epoch5 Batch60 Loss: 0.1643\n",
      "Epoch5 Batch70 Loss: 0.1907\n",
      "train Loss: 0.1980\n",
      "Epoch 6/499\n",
      "----------\n",
      "Epoch6 Batch0 Loss: 0.1734\n",
      "shuffling the dataset\n",
      "Epoch6 Batch10 Loss: 0.2067\n",
      "Epoch6 Batch20 Loss: 0.1941\n",
      "Epoch6 Batch30 Loss: 0.1372\n",
      "Epoch6 Batch40 Loss: 0.1759\n",
      "Epoch6 Batch50 Loss: 0.1924\n",
      "Epoch6 Batch60 Loss: 0.1866\n",
      "Epoch6 Batch70 Loss: 0.2228\n",
      "train Loss: 0.1886\n",
      "Epoch 7/499\n",
      "----------\n",
      "Epoch7 Batch0 Loss: 0.1666\n",
      "shuffling the dataset\n",
      "Epoch7 Batch10 Loss: 0.1859\n",
      "Epoch7 Batch20 Loss: 0.1508\n",
      "Epoch7 Batch30 Loss: 0.2096\n",
      "Epoch7 Batch40 Loss: 0.1937\n",
      "Epoch7 Batch50 Loss: 0.1832\n",
      "Epoch7 Batch60 Loss: 0.1886\n",
      "Epoch7 Batch70 Loss: 0.1656\n",
      "train Loss: 0.1849\n",
      "Epoch 8/499\n",
      "----------\n",
      "Epoch8 Batch0 Loss: 0.1710\n",
      "shuffling the dataset\n",
      "Epoch8 Batch10 Loss: 0.2099\n",
      "Epoch8 Batch20 Loss: 0.1956\n",
      "Epoch8 Batch30 Loss: 0.1501\n",
      "Epoch8 Batch40 Loss: 0.1712\n",
      "Epoch8 Batch50 Loss: 0.1548\n",
      "Epoch8 Batch60 Loss: 0.1480\n",
      "Epoch8 Batch70 Loss: 0.1964\n",
      "train Loss: 0.1778\n",
      "Epoch 9/499\n",
      "----------\n",
      "Epoch9 Batch0 Loss: 0.2010\n",
      "shuffling the dataset\n",
      "Epoch9 Batch10 Loss: 0.1234\n",
      "Epoch9 Batch20 Loss: 0.1545\n",
      "Epoch9 Batch30 Loss: 0.1553\n",
      "Epoch9 Batch40 Loss: 0.1296\n",
      "Epoch9 Batch50 Loss: 0.1649\n",
      "Epoch9 Batch60 Loss: 0.1309\n",
      "Epoch9 Batch70 Loss: 0.1666\n",
      "train Loss: 0.1641\n",
      "Epoch 10/499\n",
      "----------\n",
      "Epoch10 Batch0 Loss: 0.1213\n",
      "shuffling the dataset\n",
      "Epoch10 Batch10 Loss: 0.1314\n",
      "Epoch10 Batch20 Loss: 0.2050\n",
      "Epoch10 Batch30 Loss: 0.1723\n",
      "Epoch10 Batch40 Loss: 0.1143\n",
      "Epoch10 Batch50 Loss: 0.1685\n",
      "Epoch10 Batch60 Loss: 0.1032\n",
      "Epoch10 Batch70 Loss: 0.1246\n",
      "train Loss: 0.1442\n",
      "Epoch 11/499\n",
      "----------\n",
      "Epoch11 Batch0 Loss: 0.1671\n",
      "shuffling the dataset\n",
      "Epoch11 Batch10 Loss: 0.1367\n",
      "Epoch11 Batch20 Loss: 0.1677\n",
      "Epoch11 Batch30 Loss: 0.1658\n",
      "Epoch11 Batch40 Loss: 0.1366\n",
      "Epoch11 Batch50 Loss: 0.1483\n",
      "Epoch11 Batch60 Loss: 0.1171\n",
      "Epoch11 Batch70 Loss: 0.1424\n",
      "train Loss: 0.1316\n",
      "Epoch 12/499\n",
      "----------\n",
      "Epoch12 Batch0 Loss: 0.1246\n",
      "shuffling the dataset\n",
      "Epoch12 Batch10 Loss: 0.1002\n",
      "Epoch12 Batch20 Loss: 0.0857\n",
      "Epoch12 Batch30 Loss: 0.1516\n",
      "Epoch12 Batch40 Loss: 0.1066\n",
      "Epoch12 Batch50 Loss: 0.1240\n",
      "Epoch12 Batch60 Loss: 0.0981\n",
      "Epoch12 Batch70 Loss: 0.0989\n",
      "train Loss: 0.1199\n",
      "Epoch 13/499\n",
      "----------\n",
      "Epoch13 Batch0 Loss: 0.1189\n",
      "Epoch13 Batch10 Loss: 0.1172\n",
      "shuffling the dataset\n",
      "Epoch13 Batch20 Loss: 0.1090\n",
      "Epoch13 Batch30 Loss: 0.1027\n",
      "Epoch13 Batch40 Loss: 0.1037\n",
      "Epoch13 Batch50 Loss: 0.0924\n",
      "Epoch13 Batch60 Loss: 0.1244\n",
      "Epoch13 Batch70 Loss: 0.0682\n",
      "train Loss: 0.0984\n",
      "Epoch 14/499\n",
      "----------\n",
      "Epoch14 Batch0 Loss: 0.1198\n",
      "Epoch14 Batch10 Loss: 0.1140\n",
      "shuffling the dataset\n",
      "Epoch14 Batch20 Loss: 0.0864\n",
      "Epoch14 Batch30 Loss: 0.0895\n",
      "Epoch14 Batch40 Loss: 0.0895\n",
      "Epoch14 Batch50 Loss: 0.0713\n",
      "Epoch14 Batch60 Loss: 0.1461\n",
      "Epoch14 Batch70 Loss: 0.1079\n",
      "train Loss: 0.0978\n",
      "Epoch 15/499\n",
      "----------\n",
      "Epoch15 Batch0 Loss: 0.1351\n",
      "Epoch15 Batch10 Loss: 0.0866\n",
      "shuffling the dataset\n",
      "Epoch15 Batch20 Loss: 0.0525\n",
      "Epoch15 Batch30 Loss: 0.1092\n",
      "Epoch15 Batch40 Loss: 0.1674\n",
      "Epoch15 Batch50 Loss: 0.0674\n",
      "Epoch15 Batch60 Loss: 0.1263\n",
      "Epoch15 Batch70 Loss: 0.1151\n",
      "train Loss: 0.0966\n",
      "Epoch 16/499\n",
      "----------\n",
      "Epoch16 Batch0 Loss: 0.0815\n",
      "Epoch16 Batch10 Loss: 0.0966\n",
      "shuffling the dataset\n",
      "Epoch16 Batch20 Loss: 0.1009\n",
      "Epoch16 Batch30 Loss: 0.0745\n",
      "Epoch16 Batch40 Loss: 0.0899\n",
      "Epoch16 Batch50 Loss: 0.1191\n",
      "Epoch16 Batch60 Loss: 0.0722\n",
      "Epoch16 Batch70 Loss: 0.1063\n",
      "train Loss: 0.0968\n",
      "Epoch 17/499\n",
      "----------\n",
      "Epoch17 Batch0 Loss: 0.0922\n",
      "Epoch17 Batch10 Loss: 0.0795\n",
      "shuffling the dataset\n",
      "Epoch17 Batch20 Loss: 0.1400\n",
      "Epoch17 Batch30 Loss: 0.1015\n",
      "Epoch17 Batch40 Loss: 0.1248\n",
      "Epoch17 Batch50 Loss: 0.0338\n",
      "Epoch17 Batch60 Loss: 0.1086\n",
      "Epoch17 Batch70 Loss: 0.1090\n",
      "train Loss: 0.0968\n",
      "Epoch 18/499\n",
      "----------\n",
      "Epoch18 Batch0 Loss: 0.0500\n",
      "Epoch18 Batch10 Loss: 0.0588\n",
      "shuffling the dataset\n",
      "Epoch18 Batch20 Loss: 0.1284\n",
      "Epoch18 Batch30 Loss: 0.1083\n",
      "Epoch18 Batch40 Loss: 0.1142\n",
      "Epoch18 Batch50 Loss: 0.1380\n",
      "Epoch18 Batch60 Loss: 0.0933\n",
      "Epoch18 Batch70 Loss: 0.0544\n",
      "train Loss: 0.0957\n",
      "Epoch 19/499\n",
      "----------\n",
      "Epoch19 Batch0 Loss: 0.1022\n",
      "Epoch19 Batch10 Loss: 0.0901\n",
      "shuffling the dataset\n",
      "Epoch19 Batch20 Loss: 0.1227\n",
      "Epoch19 Batch30 Loss: 0.0911\n",
      "Epoch19 Batch40 Loss: 0.1056\n",
      "Epoch19 Batch50 Loss: 0.0547\n",
      "Epoch19 Batch60 Loss: 0.0580\n",
      "Epoch19 Batch70 Loss: 0.0832\n",
      "train Loss: 0.0926\n",
      "Epoch 20/499\n",
      "----------\n",
      "Epoch20 Batch0 Loss: 0.0790\n",
      "Epoch20 Batch10 Loss: 0.1320\n",
      "shuffling the dataset\n",
      "Epoch20 Batch20 Loss: 0.0874\n",
      "Epoch20 Batch30 Loss: 0.1057\n",
      "Epoch20 Batch40 Loss: 0.0710\n",
      "Epoch20 Batch50 Loss: 0.1125\n",
      "Epoch20 Batch60 Loss: 0.1585\n",
      "Epoch20 Batch70 Loss: 0.1182\n",
      "train Loss: 0.0935\n",
      "Epoch 21/499\n",
      "----------\n",
      "Epoch21 Batch0 Loss: 0.0879\n",
      "Epoch21 Batch10 Loss: 0.0656\n",
      "shuffling the dataset\n",
      "Epoch21 Batch20 Loss: 0.0508\n",
      "Epoch21 Batch30 Loss: 0.1323\n",
      "Epoch21 Batch40 Loss: 0.1163\n",
      "Epoch21 Batch50 Loss: 0.0800\n",
      "Epoch21 Batch60 Loss: 0.0861\n",
      "Epoch21 Batch70 Loss: 0.1401\n",
      "train Loss: 0.0924\n",
      "Epoch 22/499\n",
      "----------\n",
      "Epoch22 Batch0 Loss: 0.1111\n",
      "Epoch22 Batch10 Loss: 0.1250\n",
      "shuffling the dataset\n",
      "Epoch22 Batch20 Loss: 0.0657\n",
      "Epoch22 Batch30 Loss: 0.0849\n",
      "Epoch22 Batch40 Loss: 0.1001\n",
      "Epoch22 Batch50 Loss: 0.0904\n",
      "Epoch22 Batch60 Loss: 0.0738\n",
      "Epoch22 Batch70 Loss: 0.1411\n",
      "train Loss: 0.0941\n",
      "Epoch 23/499\n",
      "----------\n",
      "Epoch23 Batch0 Loss: 0.1100\n",
      "Epoch23 Batch10 Loss: 0.1198\n",
      "shuffling the dataset\n",
      "Epoch23 Batch20 Loss: 0.0941\n",
      "Epoch23 Batch30 Loss: 0.0752\n",
      "Epoch23 Batch40 Loss: 0.1395\n",
      "Epoch23 Batch50 Loss: 0.0877\n",
      "Epoch23 Batch60 Loss: 0.1314\n",
      "Epoch23 Batch70 Loss: 0.0759\n",
      "train Loss: 0.0950\n",
      "Epoch 24/499\n",
      "----------\n",
      "Epoch24 Batch0 Loss: 0.0914\n",
      "Epoch24 Batch10 Loss: 0.0972\n",
      "shuffling the dataset\n",
      "Epoch24 Batch20 Loss: 0.0919\n",
      "Epoch24 Batch30 Loss: 0.0732\n",
      "Epoch24 Batch40 Loss: 0.0765\n",
      "Epoch24 Batch50 Loss: 0.1236\n",
      "Epoch24 Batch60 Loss: 0.1121\n",
      "Epoch24 Batch70 Loss: 0.0604\n",
      "train Loss: 0.0900\n",
      "Epoch 25/499\n",
      "----------\n",
      "Epoch25 Batch0 Loss: 0.0455\n",
      "Epoch25 Batch10 Loss: 0.1088\n",
      "Epoch25 Batch20 Loss: 0.1152\n",
      "shuffling the dataset\n",
      "Epoch25 Batch30 Loss: 0.0787\n",
      "Epoch25 Batch40 Loss: 0.0614\n",
      "Epoch25 Batch50 Loss: 0.1155\n",
      "Epoch25 Batch60 Loss: 0.1021\n",
      "Epoch25 Batch70 Loss: 0.0885\n",
      "train Loss: 0.0883\n",
      "Epoch 26/499\n",
      "----------\n",
      "Epoch26 Batch0 Loss: 0.1651\n",
      "Epoch26 Batch10 Loss: 0.1353\n",
      "Epoch26 Batch20 Loss: 0.1116\n",
      "shuffling the dataset\n",
      "Epoch26 Batch30 Loss: 0.0459\n",
      "Epoch26 Batch40 Loss: 0.0738\n",
      "Epoch26 Batch50 Loss: 0.1269\n",
      "Epoch26 Batch60 Loss: 0.1013\n",
      "Epoch26 Batch70 Loss: 0.1500\n",
      "train Loss: 0.0927\n",
      "Epoch 27/499\n",
      "----------\n",
      "Epoch27 Batch0 Loss: 0.1381\n",
      "Epoch27 Batch10 Loss: 0.0606\n",
      "Epoch27 Batch20 Loss: 0.1006\n",
      "shuffling the dataset\n",
      "Epoch27 Batch30 Loss: 0.0579\n",
      "Epoch27 Batch40 Loss: 0.1445\n",
      "Epoch27 Batch50 Loss: 0.0827\n",
      "Epoch27 Batch60 Loss: 0.1011\n",
      "Epoch27 Batch70 Loss: 0.0936\n",
      "train Loss: 0.0915\n",
      "Epoch 28/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch28 Batch0 Loss: 0.0882\n",
      "Epoch28 Batch10 Loss: 0.1710\n",
      "Epoch28 Batch20 Loss: 0.1334\n",
      "shuffling the dataset\n",
      "Epoch28 Batch30 Loss: 0.0907\n",
      "Epoch28 Batch40 Loss: 0.1218\n",
      "Epoch28 Batch50 Loss: 0.1117\n",
      "Epoch28 Batch60 Loss: 0.0944\n",
      "Epoch28 Batch70 Loss: 0.0836\n",
      "train Loss: 0.0907\n",
      "Epoch 29/499\n",
      "----------\n",
      "Epoch29 Batch0 Loss: 0.0897\n",
      "Epoch29 Batch10 Loss: 0.0821\n",
      "Epoch29 Batch20 Loss: 0.0550\n",
      "shuffling the dataset\n",
      "Epoch29 Batch30 Loss: 0.0886\n",
      "Epoch29 Batch40 Loss: 0.0742\n",
      "Epoch29 Batch50 Loss: 0.0716\n",
      "Epoch29 Batch60 Loss: 0.0908\n",
      "Epoch29 Batch70 Loss: 0.1421\n",
      "train Loss: 0.0917\n",
      "Epoch 30/499\n",
      "----------\n",
      "Epoch30 Batch0 Loss: 0.1259\n",
      "Epoch30 Batch10 Loss: 0.0940\n",
      "Epoch30 Batch20 Loss: 0.0623\n",
      "shuffling the dataset\n",
      "Epoch30 Batch30 Loss: 0.0594\n",
      "Epoch30 Batch40 Loss: 0.1186\n",
      "Epoch30 Batch50 Loss: 0.1049\n",
      "Epoch30 Batch60 Loss: 0.0959\n",
      "Epoch30 Batch70 Loss: 0.1138\n",
      "train Loss: 0.0895\n",
      "Epoch 31/499\n",
      "----------\n",
      "Epoch31 Batch0 Loss: 0.0369\n",
      "Epoch31 Batch10 Loss: 0.0882\n",
      "Epoch31 Batch20 Loss: 0.0896\n",
      "shuffling the dataset\n",
      "Epoch31 Batch30 Loss: 0.0619\n",
      "Epoch31 Batch40 Loss: 0.0717\n",
      "Epoch31 Batch50 Loss: 0.0812\n",
      "Epoch31 Batch60 Loss: 0.0934\n",
      "Epoch31 Batch70 Loss: 0.0837\n",
      "train Loss: 0.0911\n",
      "Epoch 32/499\n",
      "----------\n",
      "Epoch32 Batch0 Loss: 0.0876\n",
      "Epoch32 Batch10 Loss: 0.0969\n",
      "Epoch32 Batch20 Loss: 0.0849\n",
      "shuffling the dataset\n",
      "Epoch32 Batch30 Loss: 0.1162\n",
      "Epoch32 Batch40 Loss: 0.1058\n",
      "Epoch32 Batch50 Loss: 0.1221\n",
      "Epoch32 Batch60 Loss: 0.0925\n",
      "Epoch32 Batch70 Loss: 0.0601\n",
      "train Loss: 0.0896\n",
      "Epoch 33/499\n",
      "----------\n",
      "Epoch33 Batch0 Loss: 0.0911\n",
      "Epoch33 Batch10 Loss: 0.1249\n",
      "Epoch33 Batch20 Loss: 0.0573\n",
      "shuffling the dataset\n",
      "Epoch33 Batch30 Loss: 0.0870\n",
      "Epoch33 Batch40 Loss: 0.0841\n",
      "Epoch33 Batch50 Loss: 0.0652\n",
      "Epoch33 Batch60 Loss: 0.1511\n",
      "Epoch33 Batch70 Loss: 0.0818\n",
      "train Loss: 0.0872\n",
      "Epoch 34/499\n",
      "----------\n",
      "Epoch34 Batch0 Loss: 0.0652\n",
      "Epoch34 Batch10 Loss: 0.0946\n",
      "Epoch34 Batch20 Loss: 0.1140\n",
      "shuffling the dataset\n",
      "Epoch34 Batch30 Loss: 0.1303\n",
      "Epoch34 Batch40 Loss: 0.1492\n",
      "Epoch34 Batch50 Loss: 0.1029\n",
      "Epoch34 Batch60 Loss: 0.0720\n",
      "Epoch34 Batch70 Loss: 0.0957\n",
      "train Loss: 0.0908\n",
      "Epoch 35/499\n",
      "----------\n",
      "Epoch35 Batch0 Loss: 0.1010\n",
      "Epoch35 Batch10 Loss: 0.0631\n",
      "Epoch35 Batch20 Loss: 0.0488\n",
      "shuffling the dataset\n",
      "Epoch35 Batch30 Loss: 0.0464\n",
      "Epoch35 Batch40 Loss: 0.1085\n",
      "Epoch35 Batch50 Loss: 0.1112\n",
      "Epoch35 Batch60 Loss: 0.0965\n",
      "Epoch35 Batch70 Loss: 0.0886\n",
      "train Loss: 0.0848\n",
      "Epoch 36/499\n",
      "----------\n",
      "Epoch36 Batch0 Loss: 0.0797\n",
      "Epoch36 Batch10 Loss: 0.0724\n",
      "Epoch36 Batch20 Loss: 0.0758\n",
      "Epoch36 Batch30 Loss: 0.0792\n",
      "shuffling the dataset\n",
      "Epoch36 Batch40 Loss: 0.0709\n",
      "Epoch36 Batch50 Loss: 0.1343\n",
      "Epoch36 Batch60 Loss: 0.1488\n",
      "Epoch36 Batch70 Loss: 0.0794\n",
      "train Loss: 0.0880\n",
      "Epoch 37/499\n",
      "----------\n",
      "Epoch37 Batch0 Loss: 0.0855\n",
      "Epoch37 Batch10 Loss: 0.1069\n",
      "Epoch37 Batch20 Loss: 0.0792\n",
      "Epoch37 Batch30 Loss: 0.0229\n",
      "shuffling the dataset\n",
      "Epoch37 Batch40 Loss: 0.0737\n",
      "Epoch37 Batch50 Loss: 0.0789\n",
      "Epoch37 Batch60 Loss: 0.0639\n",
      "Epoch37 Batch70 Loss: 0.0948\n",
      "train Loss: 0.0834\n",
      "Epoch 38/499\n",
      "----------\n",
      "Epoch38 Batch0 Loss: 0.1125\n",
      "Epoch38 Batch10 Loss: 0.1232\n",
      "Epoch38 Batch20 Loss: 0.0871\n",
      "Epoch38 Batch30 Loss: 0.1112\n",
      "shuffling the dataset\n",
      "Epoch38 Batch40 Loss: 0.0656\n",
      "Epoch38 Batch50 Loss: 0.1016\n",
      "Epoch38 Batch60 Loss: 0.0983\n",
      "Epoch38 Batch70 Loss: 0.0791\n",
      "train Loss: 0.0891\n",
      "Epoch 39/499\n",
      "----------\n",
      "Epoch39 Batch0 Loss: 0.0935\n",
      "Epoch39 Batch10 Loss: 0.0982\n",
      "Epoch39 Batch20 Loss: 0.0873\n",
      "Epoch39 Batch30 Loss: 0.0637\n",
      "shuffling the dataset\n",
      "Epoch39 Batch40 Loss: 0.0677\n",
      "Epoch39 Batch50 Loss: 0.1429\n",
      "Epoch39 Batch60 Loss: 0.0608\n",
      "Epoch39 Batch70 Loss: 0.0857\n",
      "train Loss: 0.0830\n",
      "Epoch 40/499\n",
      "----------\n",
      "Epoch40 Batch0 Loss: 0.1048\n",
      "Epoch40 Batch10 Loss: 0.1126\n",
      "Epoch40 Batch20 Loss: 0.0888\n",
      "Epoch40 Batch30 Loss: 0.0761\n",
      "shuffling the dataset\n",
      "Epoch40 Batch40 Loss: 0.1285\n",
      "Epoch40 Batch50 Loss: 0.1027\n",
      "Epoch40 Batch60 Loss: 0.1181\n",
      "Epoch40 Batch70 Loss: 0.0635\n",
      "train Loss: 0.0835\n",
      "Epoch 41/499\n",
      "----------\n",
      "Epoch41 Batch0 Loss: 0.1200\n",
      "Epoch41 Batch10 Loss: 0.1078\n",
      "Epoch41 Batch20 Loss: 0.0598\n",
      "Epoch41 Batch30 Loss: 0.0858\n",
      "shuffling the dataset\n",
      "Epoch41 Batch40 Loss: 0.0710\n",
      "Epoch41 Batch50 Loss: 0.0745\n",
      "Epoch41 Batch60 Loss: 0.0743\n",
      "Epoch41 Batch70 Loss: 0.0679\n",
      "train Loss: 0.0855\n",
      "Epoch 42/499\n",
      "----------\n",
      "Epoch42 Batch0 Loss: 0.0484\n",
      "Epoch42 Batch10 Loss: 0.0842\n",
      "Epoch42 Batch20 Loss: 0.0895\n",
      "Epoch42 Batch30 Loss: 0.0796\n",
      "shuffling the dataset\n",
      "Epoch42 Batch40 Loss: 0.0629\n",
      "Epoch42 Batch50 Loss: 0.1074\n",
      "Epoch42 Batch60 Loss: 0.0706\n",
      "Epoch42 Batch70 Loss: 0.0955\n",
      "train Loss: 0.0791\n",
      "Epoch 43/499\n",
      "----------\n",
      "Epoch43 Batch0 Loss: 0.0986\n",
      "Epoch43 Batch10 Loss: 0.0752\n",
      "Epoch43 Batch20 Loss: 0.0927\n",
      "Epoch43 Batch30 Loss: 0.0755\n",
      "shuffling the dataset\n",
      "Epoch43 Batch40 Loss: 0.1042\n",
      "Epoch43 Batch50 Loss: 0.1475\n",
      "Epoch43 Batch60 Loss: 0.0784\n",
      "Epoch43 Batch70 Loss: 0.0983\n",
      "train Loss: 0.0833\n",
      "Epoch 44/499\n",
      "----------\n",
      "Epoch44 Batch0 Loss: 0.0742\n",
      "Epoch44 Batch10 Loss: 0.1085\n",
      "Epoch44 Batch20 Loss: 0.0761\n",
      "Epoch44 Batch30 Loss: 0.1174\n",
      "shuffling the dataset\n",
      "Epoch44 Batch40 Loss: 0.0759\n",
      "Epoch44 Batch50 Loss: 0.0813\n",
      "Epoch44 Batch60 Loss: 0.0932\n",
      "Epoch44 Batch70 Loss: 0.1098\n",
      "train Loss: 0.0825\n",
      "Epoch 45/499\n",
      "----------\n",
      "Epoch45 Batch0 Loss: 0.0637\n",
      "Epoch45 Batch10 Loss: 0.0710\n",
      "Epoch45 Batch20 Loss: 0.1056\n",
      "Epoch45 Batch30 Loss: 0.0732\n",
      "shuffling the dataset\n",
      "Epoch45 Batch40 Loss: 0.0533\n",
      "Epoch45 Batch50 Loss: 0.0979\n",
      "Epoch45 Batch60 Loss: 0.0941\n",
      "Epoch45 Batch70 Loss: 0.0683\n",
      "train Loss: 0.0794\n",
      "Epoch 46/499\n",
      "----------\n",
      "Epoch46 Batch0 Loss: 0.0554\n",
      "Epoch46 Batch10 Loss: 0.1226\n",
      "Epoch46 Batch20 Loss: 0.0678\n",
      "Epoch46 Batch30 Loss: 0.0775\n",
      "shuffling the dataset\n",
      "Epoch46 Batch40 Loss: 0.1298\n",
      "Epoch46 Batch50 Loss: 0.0807\n",
      "Epoch46 Batch60 Loss: 0.0781\n",
      "Epoch46 Batch70 Loss: 0.0583\n",
      "train Loss: 0.0773\n",
      "Epoch 47/499\n",
      "----------\n",
      "Epoch47 Batch0 Loss: 0.1475\n",
      "Epoch47 Batch10 Loss: 0.0861\n",
      "Epoch47 Batch20 Loss: 0.0854\n",
      "Epoch47 Batch30 Loss: 0.0483\n",
      "Epoch47 Batch40 Loss: 0.0625\n",
      "shuffling the dataset\n",
      "Epoch47 Batch50 Loss: 0.0587\n",
      "Epoch47 Batch60 Loss: 0.0739\n",
      "Epoch47 Batch70 Loss: 0.0703\n",
      "train Loss: 0.0794\n",
      "Epoch 48/499\n",
      "----------\n",
      "Epoch48 Batch0 Loss: 0.0992\n",
      "Epoch48 Batch10 Loss: 0.0458\n",
      "Epoch48 Batch20 Loss: 0.0524\n",
      "Epoch48 Batch30 Loss: 0.0617\n",
      "Epoch48 Batch40 Loss: 0.0941\n",
      "shuffling the dataset\n",
      "Epoch48 Batch50 Loss: 0.1015\n",
      "Epoch48 Batch60 Loss: 0.0821\n",
      "Epoch48 Batch70 Loss: 0.1184\n",
      "train Loss: 0.0785\n",
      "Epoch 49/499\n",
      "----------\n",
      "Epoch49 Batch0 Loss: 0.0736\n",
      "Epoch49 Batch10 Loss: 0.0904\n",
      "Epoch49 Batch20 Loss: 0.0710\n",
      "Epoch49 Batch30 Loss: 0.0995\n",
      "Epoch49 Batch40 Loss: 0.0785\n",
      "shuffling the dataset\n",
      "Epoch49 Batch50 Loss: 0.1088\n",
      "Epoch49 Batch60 Loss: 0.0678\n",
      "Epoch49 Batch70 Loss: 0.0853\n",
      "train Loss: 0.0741\n",
      "Epoch 50/499\n",
      "----------\n",
      "Epoch50 Batch0 Loss: 0.0997\n",
      "Epoch50 Batch10 Loss: 0.0816\n",
      "Epoch50 Batch20 Loss: 0.0639\n",
      "Epoch50 Batch30 Loss: 0.0990\n",
      "Epoch50 Batch40 Loss: 0.1055\n",
      "shuffling the dataset\n",
      "Epoch50 Batch50 Loss: 0.0890\n",
      "Epoch50 Batch60 Loss: 0.0897\n",
      "Epoch50 Batch70 Loss: 0.0872\n",
      "train Loss: 0.0839\n",
      "Epoch 51/499\n",
      "----------\n",
      "Epoch51 Batch0 Loss: 0.0796\n",
      "Epoch51 Batch10 Loss: 0.1015\n",
      "Epoch51 Batch20 Loss: 0.0615\n",
      "Epoch51 Batch30 Loss: 0.0907\n",
      "Epoch51 Batch40 Loss: 0.0688\n",
      "shuffling the dataset\n",
      "Epoch51 Batch50 Loss: 0.0589\n",
      "Epoch51 Batch60 Loss: 0.0814\n",
      "Epoch51 Batch70 Loss: 0.0790\n",
      "train Loss: 0.0763\n",
      "Epoch 52/499\n",
      "----------\n",
      "Epoch52 Batch0 Loss: 0.0677\n",
      "Epoch52 Batch10 Loss: 0.0551\n",
      "Epoch52 Batch20 Loss: 0.0775\n",
      "Epoch52 Batch30 Loss: 0.1021\n",
      "Epoch52 Batch40 Loss: 0.0582\n",
      "shuffling the dataset\n",
      "Epoch52 Batch50 Loss: 0.0494\n",
      "Epoch52 Batch60 Loss: 0.0474\n",
      "Epoch52 Batch70 Loss: 0.0706\n",
      "train Loss: 0.0749\n",
      "Epoch 53/499\n",
      "----------\n",
      "Epoch53 Batch0 Loss: 0.0932\n",
      "Epoch53 Batch10 Loss: 0.0679\n",
      "Epoch53 Batch20 Loss: 0.1313\n",
      "Epoch53 Batch30 Loss: 0.0948\n",
      "Epoch53 Batch40 Loss: 0.1202\n",
      "shuffling the dataset\n",
      "Epoch53 Batch50 Loss: 0.0732\n",
      "Epoch53 Batch60 Loss: 0.0597\n",
      "Epoch53 Batch70 Loss: 0.0930\n",
      "train Loss: 0.0725\n",
      "Epoch 54/499\n",
      "----------\n",
      "Epoch54 Batch0 Loss: 0.0990\n",
      "Epoch54 Batch10 Loss: 0.0771\n",
      "Epoch54 Batch20 Loss: 0.0616\n",
      "Epoch54 Batch30 Loss: 0.1012\n",
      "Epoch54 Batch40 Loss: 0.0905\n",
      "shuffling the dataset\n",
      "Epoch54 Batch50 Loss: 0.1116\n",
      "Epoch54 Batch60 Loss: 0.0737\n",
      "Epoch54 Batch70 Loss: 0.0835\n",
      "train Loss: 0.0723\n",
      "Epoch 55/499\n",
      "----------\n",
      "Epoch55 Batch0 Loss: 0.0752\n",
      "Epoch55 Batch10 Loss: 0.0630\n",
      "Epoch55 Batch20 Loss: 0.0438\n",
      "Epoch55 Batch30 Loss: 0.0777\n",
      "Epoch55 Batch40 Loss: 0.0663\n",
      "shuffling the dataset\n",
      "Epoch55 Batch50 Loss: 0.1280\n",
      "Epoch55 Batch60 Loss: 0.0610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch55 Batch70 Loss: 0.0658\n",
      "train Loss: 0.0719\n",
      "Epoch 56/499\n",
      "----------\n",
      "Epoch56 Batch0 Loss: 0.0940\n",
      "Epoch56 Batch10 Loss: 0.0385\n",
      "Epoch56 Batch20 Loss: 0.0747\n",
      "Epoch56 Batch30 Loss: 0.0678\n",
      "Epoch56 Batch40 Loss: 0.0855\n",
      "shuffling the dataset\n",
      "Epoch56 Batch50 Loss: 0.0618\n",
      "Epoch56 Batch60 Loss: 0.0526\n",
      "Epoch56 Batch70 Loss: 0.0561\n",
      "train Loss: 0.0761\n",
      "Epoch 57/499\n",
      "----------\n",
      "Epoch57 Batch0 Loss: 0.0636\n",
      "Epoch57 Batch10 Loss: 0.0675\n",
      "Epoch57 Batch20 Loss: 0.1135\n",
      "Epoch57 Batch30 Loss: 0.0522\n",
      "Epoch57 Batch40 Loss: 0.0542\n",
      "shuffling the dataset\n",
      "Epoch57 Batch50 Loss: 0.0809\n",
      "Epoch57 Batch60 Loss: 0.0969\n",
      "Epoch57 Batch70 Loss: 0.0845\n",
      "train Loss: 0.0687\n",
      "Epoch 58/499\n",
      "----------\n",
      "Epoch58 Batch0 Loss: 0.0707\n",
      "Epoch58 Batch10 Loss: 0.0842\n",
      "Epoch58 Batch20 Loss: 0.0567\n",
      "Epoch58 Batch30 Loss: 0.0755\n",
      "Epoch58 Batch40 Loss: 0.0774\n",
      "shuffling the dataset\n",
      "Epoch58 Batch50 Loss: 0.0545\n",
      "Epoch58 Batch60 Loss: 0.0557\n",
      "Epoch58 Batch70 Loss: 0.0636\n",
      "train Loss: 0.0681\n",
      "Epoch 59/499\n",
      "----------\n",
      "Epoch59 Batch0 Loss: 0.0636\n",
      "Epoch59 Batch10 Loss: 0.0383\n",
      "Epoch59 Batch20 Loss: 0.0679\n",
      "Epoch59 Batch30 Loss: 0.0653\n",
      "Epoch59 Batch40 Loss: 0.0538\n",
      "Epoch59 Batch50 Loss: 0.0605\n",
      "shuffling the dataset\n",
      "Epoch59 Batch60 Loss: 0.0554\n",
      "Epoch59 Batch70 Loss: 0.0787\n",
      "train Loss: 0.0655\n",
      "Epoch 60/499\n",
      "----------\n",
      "Epoch60 Batch0 Loss: 0.0450\n",
      "Epoch60 Batch10 Loss: 0.1424\n",
      "Epoch60 Batch20 Loss: 0.0617\n",
      "Epoch60 Batch30 Loss: 0.0450\n",
      "Epoch60 Batch40 Loss: 0.1053\n",
      "Epoch60 Batch50 Loss: 0.0512\n",
      "shuffling the dataset\n",
      "Epoch60 Batch60 Loss: 0.0418\n",
      "Epoch60 Batch70 Loss: 0.0526\n",
      "train Loss: 0.0657\n",
      "Epoch 61/499\n",
      "----------\n",
      "Epoch61 Batch0 Loss: 0.0873\n",
      "Epoch61 Batch10 Loss: 0.0536\n",
      "Epoch61 Batch20 Loss: 0.0699\n",
      "Epoch61 Batch30 Loss: 0.0730\n",
      "Epoch61 Batch40 Loss: 0.0792\n",
      "Epoch61 Batch50 Loss: 0.0701\n",
      "shuffling the dataset\n",
      "Epoch61 Batch60 Loss: 0.0686\n",
      "Epoch61 Batch70 Loss: 0.0303\n",
      "train Loss: 0.0659\n",
      "Epoch 62/499\n",
      "----------\n",
      "Epoch62 Batch0 Loss: 0.0330\n",
      "Epoch62 Batch10 Loss: 0.0279\n",
      "Epoch62 Batch20 Loss: 0.0711\n",
      "Epoch62 Batch30 Loss: 0.0736\n",
      "Epoch62 Batch40 Loss: 0.0792\n",
      "Epoch62 Batch50 Loss: 0.0504\n",
      "shuffling the dataset\n",
      "Epoch62 Batch60 Loss: 0.0580\n",
      "Epoch62 Batch70 Loss: 0.0459\n",
      "train Loss: 0.0626\n",
      "Epoch 63/499\n",
      "----------\n",
      "Epoch63 Batch0 Loss: 0.0456\n",
      "Epoch63 Batch10 Loss: 0.0583\n",
      "Epoch63 Batch20 Loss: 0.0907\n",
      "Epoch65 Batch20 Loss: 0.0689\n",
      "Epoch65 Batch30 Loss: 0.0758\n",
      "Epoch65 Batch40 Loss: 0.0689\n",
      "Epoch65 Batch50 Loss: 0.0616\n",
      "shuffling the dataset\n",
      "Epoch65 Batch60 Loss: 0.0947\n",
      "Epoch65 Batch70 Loss: 0.0448\n",
      "train Loss: 0.0614\n",
      "Epoch 66/499\n",
      "----------\n",
      "Epoch66 Batch0 Loss: 0.0488\n",
      "Epoch66 Batch10 Loss: 0.0561\n",
      "Epoch66 Batch20 Loss: 0.0701\n",
      "Epoch66 Batch30 Loss: 0.0613\n",
      "Epoch66 Batch40 Loss: 0.0523\n",
      "Epoch66 Batch50 Loss: 0.0488\n",
      "shuffling the dataset\n",
      "Epoch66 Batch60 Loss: 0.0422\n",
      "Epoch66 Batch70 Loss: 0.0594\n",
      "train Loss: 0.0585\n",
      "Epoch 67/499\n",
      "----------\n",
      "Epoch67 Batch0 Loss: 0.0614\n",
      "Epoch67 Batch10 Loss: 0.0484\n",
      "Epoch67 Batch20 Loss: 0.0700\n",
      "Epoch67 Batch30 Loss: 0.0470\n",
      "Epoch67 Batch40 Loss: 0.0625\n",
      "Epoch67 Batch50 Loss: 0.0554\n",
      "shuffling the dataset\n",
      "Epoch67 Batch60 Loss: 0.0783\n",
      "Epoch67 Batch70 Loss: 0.0649\n",
      "train Loss: 0.0571\n",
      "Epoch 68/499\n",
      "----------\n",
      "Epoch68 Batch0 Loss: 0.0392\n",
      "Epoch68 Batch10 Loss: 0.0633\n",
      "Epoch68 Batch20 Loss: 0.0724\n",
      "Epoch68 Batch30 Loss: 0.0824\n",
      "Epoch68 Batch40 Loss: 0.0614\n",
      "Epoch68 Batch50 Loss: 0.0842\n",
      "shuffling the dataset\n",
      "Epoch68 Batch60 Loss: 0.0586\n",
      "Epoch68 Batch70 Loss: 0.0480\n",
      "train Loss: 0.0540\n",
      "Epoch 69/499\n",
      "----------\n",
      "Epoch69 Batch0 Loss: 0.0920\n",
      "Epoch69 Batch10 Loss: 0.0850\n",
      "Epoch69 Batch20 Loss: 0.0458\n",
      "Epoch69 Batch30 Loss: 0.0831\n",
      "Epoch69 Batch40 Loss: 0.0625\n",
      "Epoch69 Batch50 Loss: 0.0394\n",
      "shuffling the dataset\n",
      "Epoch69 Batch60 Loss: 0.0556\n",
      "Epoch69 Batch70 Loss: 0.0819\n",
      "train Loss: 0.0546\n",
      "Epoch 70/499\n",
      "----------\n",
      "Epoch70 Batch0 Loss: 0.0509\n",
      "Epoch70 Batch10 Loss: 0.0761\n",
      "Epoch70 Batch20 Loss: 0.0392\n",
      "Epoch70 Batch30 Loss: 0.0419\n",
      "Epoch70 Batch40 Loss: 0.0673\n",
      "Epoch70 Batch50 Loss: 0.0725\n",
      "Epoch70 Batch60 Loss: 0.0428\n",
      "shuffling the dataset\n",
      "Epoch70 Batch70 Loss: 0.0625\n",
      "train Loss: 0.0513\n",
      "Epoch 71/499\n",
      "----------\n",
      "Epoch71 Batch0 Loss: 0.0598\n",
      "Epoch71 Batch10 Loss: 0.0874\n",
      "Epoch71 Batch20 Loss: 0.0489\n",
      "Epoch71 Batch30 Loss: 0.0858\n",
      "Epoch71 Batch40 Loss: 0.0443\n",
      "Epoch71 Batch50 Loss: 0.0511\n",
      "Epoch71 Batch60 Loss: 0.0544\n",
      "shuffling the dataset\n",
      "Epoch71 Batch70 Loss: 0.0321\n",
      "train Loss: 0.0508\n",
      "Epoch 72/499\n",
      "----------\n",
      "Epoch72 Batch0 Loss: 0.0530\n",
      "Epoch72 Batch10 Loss: 0.0578\n",
      "Epoch72 Batch20 Loss: 0.0505\n",
      "Epoch72 Batch30 Loss: 0.0521\n",
      "Epoch72 Batch40 Loss: 0.0592\n",
      "Epoch72 Batch50 Loss: 0.0331\n",
      "Epoch72 Batch60 Loss: 0.0725\n",
      "shuffling the dataset\n",
      "Epoch72 Batch70 Loss: 0.0785\n",
      "train Loss: 0.0489\n",
      "Epoch 73/499\n",
      "----------\n",
      "Epoch73 Batch0 Loss: 0.0548\n",
      "Epoch73 Batch10 Loss: 0.0514\n",
      "Epoch73 Batch20 Loss: 0.0642\n",
      "Epoch73 Batch30 Loss: 0.0423\n",
      "Epoch73 Batch40 Loss: 0.0467\n",
      "Epoch73 Batch50 Loss: 0.0523\n",
      "Epoch73 Batch60 Loss: 0.0677\n",
      "shuffling the dataset\n",
      "Epoch73 Batch70 Loss: 0.0479\n",
      "train Loss: 0.0481\n",
      "Epoch 74/499\n",
      "----------\n",
      "Epoch74 Batch0 Loss: 0.0451\n",
      "Epoch74 Batch10 Loss: 0.0562\n",
      "Epoch74 Batch20 Loss: 0.0502\n",
      "Epoch74 Batch30 Loss: 0.0393\n",
      "Epoch74 Batch40 Loss: 0.0441\n",
      "Epoch74 Batch50 Loss: 0.0466\n",
      "Epoch74 Batch60 Loss: 0.0919\n",
      "shuffling the dataset\n",
      "Epoch74 Batch70 Loss: 0.0379\n",
      "train Loss: 0.0458\n",
      "Epoch 75/499\n",
      "----------\n",
      "Epoch75 Batch0 Loss: 0.0321\n",
      "Epoch75 Batch10 Loss: 0.0475\n",
      "Epoch75 Batch20 Loss: 0.0600\n",
      "Epoch75 Batch30 Loss: 0.0464\n",
      "Epoch75 Batch40 Loss: 0.0412\n",
      "Epoch75 Batch50 Loss: 0.0334\n",
      "Epoch75 Batch60 Loss: 0.0432\n",
      "shuffling the dataset\n",
      "Epoch75 Batch70 Loss: 0.0518\n",
      "train Loss: 0.0459\n",
      "Epoch 76/499\n",
      "----------\n",
      "Epoch76 Batch0 Loss: 0.0425\n",
      "Epoch76 Batch10 Loss: 0.0381\n",
      "Epoch76 Batch20 Loss: 0.0278\n",
      "Epoch76 Batch30 Loss: 0.0431\n",
      "Epoch76 Batch40 Loss: 0.0587\n",
      "Epoch76 Batch50 Loss: 0.0558\n",
      "Epoch76 Batch60 Loss: 0.0370\n",
      "shuffling the dataset\n",
      "Epoch76 Batch70 Loss: 0.0481\n",
      "train Loss: 0.0466\n",
      "Epoch 77/499\n",
      "----------\n",
      "Epoch77 Batch0 Loss: 0.0457\n",
      "Epoch77 Batch10 Loss: 0.0341\n",
      "Epoch77 Batch20 Loss: 0.0588\n",
      "Epoch77 Batch30 Loss: 0.0492\n",
      "Epoch77 Batch40 Loss: 0.0399\n",
      "Epoch77 Batch50 Loss: 0.0482\n",
      "Epoch77 Batch60 Loss: 0.0433\n",
      "shuffling the dataset\n",
      "Epoch77 Batch70 Loss: 0.0416\n",
      "train Loss: 0.0449\n",
      "Epoch 78/499\n",
      "----------\n",
      "Epoch78 Batch0 Loss: 0.0388\n",
      "Epoch78 Batch10 Loss: 0.0431\n",
      "Epoch78 Batch20 Loss: 0.0323\n",
      "Epoch78 Batch30 Loss: 0.0604\n",
      "Epoch78 Batch40 Loss: 0.0364\n",
      "Epoch78 Batch50 Loss: 0.0590\n",
      "Epoch78 Batch60 Loss: 0.0622\n",
      "shuffling the dataset\n",
      "Epoch78 Batch70 Loss: 0.0803\n",
      "train Loss: 0.0447\n",
      "Epoch 79/499\n",
      "----------\n",
      "Epoch79 Batch0 Loss: 0.0420\n",
      "Epoch79 Batch10 Loss: 0.0395\n",
      "Epoch79 Batch20 Loss: 0.0617\n",
      "Epoch79 Batch30 Loss: 0.0357\n",
      "Epoch79 Batch40 Loss: 0.0367\n",
      "Epoch79 Batch50 Loss: 0.0332\n",
      "Epoch79 Batch60 Loss: 0.0416\n",
      "shuffling the dataset\n",
      "Epoch79 Batch70 Loss: 0.0299\n",
      "train Loss: 0.0417\n",
      "Epoch 80/499\n",
      "----------\n",
      "Epoch80 Batch0 Loss: 0.0680\n",
      "Epoch80 Batch10 Loss: 0.0294\n",
      "Epoch80 Batch20 Loss: 0.0462\n",
      "Epoch80 Batch30 Loss: 0.0937\n",
      "Epoch80 Batch40 Loss: 0.0339\n",
      "Epoch80 Batch50 Loss: 0.0411\n",
      "Epoch80 Batch60 Loss: 0.0487\n",
      "shuffling the dataset\n",
      "Epoch80 Batch70 Loss: 0.0471\n",
      "train Loss: 0.0402\n",
      "Epoch 81/499\n",
      "----------\n",
      "Epoch81 Batch0 Loss: 0.0500\n",
      "Epoch81 Batch10 Loss: 0.0449\n",
      "Epoch81 Batch20 Loss: 0.0452\n",
      "Epoch81 Batch30 Loss: 0.0291\n",
      "Epoch81 Batch40 Loss: 0.0608\n",
      "Epoch81 Batch50 Loss: 0.0388\n",
      "Epoch81 Batch60 Loss: 0.0402\n",
      "shuffling the dataset\n",
      "Epoch81 Batch70 Loss: 0.0520\n",
      "train Loss: 0.0391\n",
      "Epoch 82/499\n",
      "----------\n",
      "Epoch82 Batch0 Loss: 0.0452\n",
      "Epoch82 Batch10 Loss: 0.0269\n",
      "Epoch82 Batch20 Loss: 0.0576\n",
      "Epoch82 Batch30 Loss: 0.0339\n",
      "Epoch82 Batch40 Loss: 0.0380\n",
      "Epoch82 Batch50 Loss: 0.0332\n",
      "Epoch82 Batch60 Loss: 0.0396\n",
      "Epoch82 Batch70 Loss: 0.0360\n",
      "train Loss: 0.0387\n",
      "Epoch 83/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch83 Batch0 Loss: 0.0302\n",
      "Epoch83 Batch10 Loss: 0.0565\n",
      "Epoch83 Batch20 Loss: 0.0372\n",
      "Epoch83 Batch30 Loss: 0.0316\n",
      "Epoch83 Batch40 Loss: 0.0335\n",
      "Epoch83 Batch50 Loss: 0.0490\n",
      "Epoch83 Batch60 Loss: 0.0296\n",
      "Epoch83 Batch70 Loss: 0.0362\n",
      "train Loss: 0.0373\n",
      "Epoch 84/499\n",
      "----------\n",
      "Epoch84 Batch0 Loss: 0.0301\n",
      "shuffling the dataset\n",
      "Epoch84 Batch10 Loss: 0.0393\n",
      "Epoch84 Batch20 Loss: 0.0390\n",
      "Epoch84 Batch30 Loss: 0.0316\n",
      "Epoch84 Batch40 Loss: 0.0537\n",
      "Epoch84 Batch50 Loss: 0.0305\n",
      "Epoch84 Batch60 Loss: 0.0406\n",
      "Epoch84 Batch70 Loss: 0.0270\n",
      "train Loss: 0.0357\n",
      "Epoch 85/499\n",
      "----------\n",
      "Epoch85 Batch0 Loss: 0.0357\n",
      "shuffling the dataset\n",
      "Epoch85 Batch10 Loss: 0.0388\n",
      "Epoch85 Batch20 Loss: 0.0206\n",
      "Epoch85 Batch30 Loss: 0.0380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch85 Batch40 Loss: 0.0352\n",
      "Epoch85 Batch50 Loss: 0.0174\n",
      "Epoch85 Batch60 Loss: 0.0280\n",
      "Epoch85 Batch70 Loss: 0.0240\n",
      "train Loss: 0.0349\n",
      "Epoch 86/499\n",
      "----------\n",
      "Epoch86 Batch0 Loss: 0.0433\n",
      "shuffling the dataset\n",
      "Epoch86 Batch10 Loss: 0.0250\n",
      "Epoch86 Batch20 Loss: 0.0340\n",
      "Epoch86 Batch30 Loss: 0.0241\n",
      "Epoch86 Batch40 Loss: 0.0254\n",
      "Epoch86 Batch50 Loss: 0.0294\n",
      "Epoch86 Batch60 Loss: 0.0300\n",
      "Epoch86 Batch70 Loss: 0.0439\n",
      "train Loss: 0.0345\n",
      "Epoch 87/499\n",
      "----------\n",
      "Epoch87 Batch0 Loss: 0.0361\n",
      "shuffling the dataset\n",
      "Epoch87 Batch10 Loss: 0.0379\n",
      "Epoch87 Batch20 Loss: 0.0387\n",
      "Epoch87 Batch30 Loss: 0.0185\n",
      "Epoch87 Batch40 Loss: 0.0259\n",
      "Epoch87 Batch50 Loss: 0.0265\n",
      "Epoch87 Batch60 Loss: 0.0216\n",
      "Epoch87 Batch70 Loss: 0.0304\n",
      "train Loss: 0.0331\n",
      "Epoch 88/499\n",
      "----------\n",
      "Epoch88 Batch0 Loss: 0.0422\n",
      "shuffling the dataset\n",
      "Epoch88 Batch10 Loss: 0.0253\n",
      "Epoch88 Batch20 Loss: 0.0311\n",
      "Epoch88 Batch30 Loss: 0.0338\n",
      "Epoch88 Batch40 Loss: 0.0284\n",
      "Epoch88 Batch50 Loss: 0.0378\n",
      "Epoch88 Batch60 Loss: 0.0287\n",
      "Epoch88 Batch70 Loss: 0.0369\n",
      "train Loss: 0.0323\n",
      "Epoch 89/499\n",
      "----------\n",
      "Epoch89 Batch0 Loss: 0.0605\n",
      "shuffling the dataset\n",
      "Epoch89 Batch10 Loss: 0.0219\n",
      "Epoch89 Batch20 Loss: 0.0270\n",
      "Epoch89 Batch30 Loss: 0.0412\n",
      "Epoch89 Batch40 Loss: 0.0332\n",
      "Epoch89 Batch50 Loss: 0.0510\n",
      "Epoch89 Batch60 Loss: 0.0219\n",
      "Epoch89 Batch70 Loss: 0.0418\n",
      "train Loss: 0.0343\n",
      "Epoch 90/499\n",
      "----------\n",
      "Epoch90 Batch0 Loss: 0.0314\n",
      "shuffling the dataset\n",
      "Epoch90 Batch10 Loss: 0.0330\n",
      "Epoch90 Batch20 Loss: 0.0317\n",
      "Epoch90 Batch30 Loss: 0.0200\n",
      "Epoch90 Batch40 Loss: 0.0495\n",
      "Epoch90 Batch50 Loss: 0.0437\n",
      "Epoch90 Batch60 Loss: 0.0560\n",
      "Epoch90 Batch70 Loss: 0.0325\n",
      "train Loss: 0.0350\n",
      "Epoch 91/499\n",
      "----------\n",
      "Epoch91 Batch0 Loss: 0.0343\n",
      "shuffling the dataset\n",
      "Epoch91 Batch10 Loss: 0.0183\n",
      "Epoch91 Batch20 Loss: 0.0310\n",
      "Epoch91 Batch30 Loss: 0.0199\n",
      "Epoch91 Batch40 Loss: 0.0329\n",
      "Epoch91 Batch50 Loss: 0.0383\n",
      "Epoch91 Batch60 Loss: 0.0663\n",
      "Epoch91 Batch70 Loss: 0.0316\n",
      "train Loss: 0.0331\n",
      "Epoch 92/499\n",
      "----------\n",
      "Epoch92 Batch0 Loss: 0.0315\n",
      "shuffling the dataset\n",
      "Epoch92 Batch10 Loss: 0.0301\n",
      "Epoch92 Batch20 Loss: 0.0333\n",
      "Epoch92 Batch30 Loss: 0.0297\n",
      "Epoch92 Batch40 Loss: 0.0271\n",
      "Epoch92 Batch50 Loss: 0.0342\n",
      "Epoch92 Batch60 Loss: 0.0305\n",
      "Epoch92 Batch70 Loss: 0.0386\n",
      "train Loss: 0.0308\n",
      "Epoch 93/499\n",
      "----------\n",
      "Epoch93 Batch0 Loss: 0.0499\n",
      "shuffling the dataset\n",
      "Epoch93 Batch10 Loss: 0.0190\n",
      "Epoch93 Batch20 Loss: 0.0290\n",
      "Epoch93 Batch30 Loss: 0.0164\n",
      "Epoch93 Batch40 Loss: 0.0270\n",
      "Epoch93 Batch50 Loss: 0.0241\n",
      "Epoch93 Batch60 Loss: 0.0450\n",
      "Epoch93 Batch70 Loss: 0.0303\n",
      "train Loss: 0.0305\n",
      "Epoch 94/499\n",
      "----------\n",
      "Epoch94 Batch0 Loss: 0.0260\n",
      "shuffling the dataset\n",
      "Epoch94 Batch10 Loss: 0.0176\n",
      "Epoch94 Batch20 Loss: 0.0388\n",
      "Epoch94 Batch30 Loss: 0.0294\n",
      "Epoch94 Batch40 Loss: 0.0204\n",
      "Epoch94 Batch50 Loss: 0.0308\n",
      "Epoch94 Batch60 Loss: 0.0258\n",
      "Epoch94 Batch70 Loss: 0.0307\n",
      "train Loss: 0.0285\n",
      "Epoch 95/499\n",
      "----------\n",
      "Epoch95 Batch0 Loss: 0.0327\n",
      "Epoch95 Batch10 Loss: 0.0318\n",
      "shuffling the dataset\n",
      "Epoch95 Batch20 Loss: 0.0296\n",
      "Epoch95 Batch30 Loss: 0.0409\n",
      "Epoch95 Batch40 Loss: 0.0315\n",
      "Epoch95 Batch50 Loss: 0.0230\n",
      "Epoch95 Batch60 Loss: 0.0290\n",
      "Epoch95 Batch70 Loss: 0.0251\n",
      "train Loss: 0.0273\n",
      "Epoch 96/499\n",
      "----------\n",
      "Epoch96 Batch0 Loss: 0.0291\n",
      "Epoch96 Batch10 Loss: 0.0295\n",
      "shuffling the dataset\n",
      "Epoch96 Batch20 Loss: 0.0205\n",
      "Epoch96 Batch30 Loss: 0.0339\n",
      "Epoch96 Batch40 Loss: 0.0288\n",
      "Epoch96 Batch50 Loss: 0.0226\n",
      "Epoch96 Batch60 Loss: 0.0263\n",
      "Epoch96 Batch70 Loss: 0.0231\n",
      "train Loss: 0.0280\n",
      "Epoch 97/499\n",
      "----------\n",
      "Epoch97 Batch0 Loss: 0.0276\n",
      "Epoch97 Batch10 Loss: 0.0367\n",
      "shuffling the dataset\n",
      "Epoch97 Batch20 Loss: 0.0278\n",
      "Epoch97 Batch30 Loss: 0.0232\n",
      "Epoch97 Batch40 Loss: 0.0339\n",
      "Epoch97 Batch50 Loss: 0.0223\n",
      "Epoch97 Batch60 Loss: 0.0289\n",
      "Epoch97 Batch70 Loss: 0.0246\n",
      "train Loss: 0.0281\n",
      "Epoch 98/499\n",
      "----------\n",
      "Epoch98 Batch0 Loss: 0.0271\n",
      "Epoch98 Batch10 Loss: 0.0232\n",
      "shuffling the dataset\n",
      "Epoch98 Batch20 Loss: 0.0211\n",
      "Epoch98 Batch30 Loss: 0.0290\n",
      "Epoch98 Batch40 Loss: 0.0235\n",
      "Epoch98 Batch50 Loss: 0.0266\n",
      "Epoch98 Batch60 Loss: 0.0222\n",
      "Epoch98 Batch70 Loss: 0.0343\n",
      "train Loss: 0.0262\n",
      "Epoch 99/499\n",
      "----------\n",
      "Epoch99 Batch0 Loss: 0.0168\n",
      "Epoch99 Batch10 Loss: 0.0301\n",
      "shuffling the dataset\n",
      "Epoch99 Batch20 Loss: 0.0221\n",
      "Epoch99 Batch30 Loss: 0.0528\n",
      "Epoch99 Batch40 Loss: 0.0385\n",
      "Epoch99 Batch50 Loss: 0.0279\n",
      "Epoch99 Batch60 Loss: 0.0326\n",
      "Epoch99 Batch70 Loss: 0.0335\n",
      "train Loss: 0.0268\n",
      "Epoch 100/499\n",
      "----------\n",
      "Epoch100 Batch0 Loss: 0.0270\n",
      "Epoch100 Batch10 Loss: 0.0316\n",
      "shuffling the dataset\n",
      "Epoch100 Batch20 Loss: 0.0324\n",
      "Epoch100 Batch30 Loss: 0.0242\n",
      "Epoch100 Batch40 Loss: 0.0180\n",
      "Epoch100 Batch50 Loss: 0.0429\n",
      "Epoch100 Batch60 Loss: 0.0294\n",
      "Epoch100 Batch70 Loss: 0.0198\n",
      "train Loss: 0.0259\n",
      "Epoch 101/499\n",
      "----------\n",
      "Epoch101 Batch0 Loss: 0.0382\n",
      "Epoch101 Batch10 Loss: 0.0249\n",
      "shuffling the dataset\n",
      "Epoch101 Batch20 Loss: 0.0265\n",
      "Epoch101 Batch30 Loss: 0.0291\n",
      "Epoch101 Batch40 Loss: 0.0245\n",
      "Epoch101 Batch50 Loss: 0.0343\n",
      "Epoch101 Batch60 Loss: 0.0275\n",
      "Epoch101 Batch70 Loss: 0.0455\n",
      "train Loss: 0.0253\n",
      "Epoch 102/499\n",
      "----------\n",
      "Epoch102 Batch0 Loss: 0.0183\n",
      "Epoch102 Batch10 Loss: 0.0396\n",
      "shuffling the dataset\n",
      "Epoch102 Batch20 Loss: 0.0268\n",
      "Epoch102 Batch30 Loss: 0.0207\n",
      "Epoch102 Batch40 Loss: 0.0198\n",
      "Epoch102 Batch50 Loss: 0.0311\n",
      "Epoch102 Batch60 Loss: 0.0375\n",
      "Epoch102 Batch70 Loss: 0.0139\n",
      "train Loss: 0.0241\n",
      "Epoch 103/499\n",
      "----------\n",
      "Epoch103 Batch0 Loss: 0.0282\n",
      "Epoch103 Batch10 Loss: 0.0180\n",
      "shuffling the dataset\n",
      "Epoch103 Batch20 Loss: 0.0280\n",
      "Epoch103 Batch30 Loss: 0.0170\n",
      "Epoch103 Batch40 Loss: 0.0276\n",
      "Epoch103 Batch50 Loss: 0.0278\n",
      "Epoch103 Batch60 Loss: 0.0282\n",
      "Epoch103 Batch70 Loss: 0.0339\n",
      "train Loss: 0.0248\n",
      "Epoch 104/499\n",
      "----------\n",
      "Epoch104 Batch0 Loss: 0.0459\n",
      "Epoch104 Batch10 Loss: 0.0281\n",
      "shuffling the dataset\n",
      "Epoch104 Batch20 Loss: 0.0244\n",
      "Epoch104 Batch30 Loss: 0.0340\n",
      "Epoch104 Batch40 Loss: 0.0180\n",
      "Epoch104 Batch50 Loss: 0.0238\n",
      "Epoch104 Batch60 Loss: 0.0202\n",
      "Epoch104 Batch70 Loss: 0.0213\n",
      "train Loss: 0.0275\n",
      "Epoch 105/499\n",
      "----------\n",
      "Epoch105 Batch0 Loss: 0.0154\n",
      "Epoch105 Batch10 Loss: 0.0225\n",
      "shuffling the dataset\n",
      "Epoch105 Batch20 Loss: 0.0214\n",
      "Epoch105 Batch30 Loss: 0.0329\n",
      "Epoch105 Batch40 Loss: 0.0141\n",
      "Epoch105 Batch50 Loss: 0.0192\n",
      "Epoch105 Batch60 Loss: 0.0234\n",
      "Epoch105 Batch70 Loss: 0.0204\n",
      "train Loss: 0.0235\n",
      "Epoch 106/499\n",
      "----------\n",
      "Epoch106 Batch0 Loss: 0.0240\n",
      "Epoch106 Batch10 Loss: 0.0195\n",
      "shuffling the dataset\n",
      "Epoch106 Batch20 Loss: 0.0280\n",
      "Epoch106 Batch30 Loss: 0.0242\n",
      "Epoch106 Batch40 Loss: 0.0297\n",
      "Epoch106 Batch50 Loss: 0.0249\n",
      "Epoch106 Batch60 Loss: 0.0167\n",
      "Epoch106 Batch70 Loss: 0.0246\n",
      "train Loss: 0.0241\n",
      "Epoch 107/499\n",
      "----------\n",
      "Epoch107 Batch0 Loss: 0.0315\n",
      "Epoch107 Batch10 Loss: 0.0209\n",
      "Epoch107 Batch20 Loss: 0.0227\n",
      "shuffling the dataset\n",
      "Epoch107 Batch30 Loss: 0.0449\n",
      "Epoch107 Batch40 Loss: 0.0206\n",
      "Epoch107 Batch50 Loss: 0.0263\n",
      "Epoch107 Batch60 Loss: 0.0148\n",
      "Epoch107 Batch70 Loss: 0.0261\n",
      "train Loss: 0.0224\n",
      "Epoch 108/499\n",
      "----------\n",
      "Epoch108 Batch0 Loss: 0.0265\n",
      "Epoch108 Batch10 Loss: 0.0352\n",
      "Epoch108 Batch20 Loss: 0.0257\n",
      "shuffling the dataset\n",
      "Epoch108 Batch30 Loss: 0.0204\n",
      "Epoch108 Batch40 Loss: 0.0195\n",
      "Epoch108 Batch50 Loss: 0.0173\n",
      "Epoch108 Batch60 Loss: 0.0214\n",
      "Epoch108 Batch70 Loss: 0.0197\n",
      "train Loss: 0.0228\n",
      "Epoch 109/499\n",
      "----------\n",
      "Epoch109 Batch0 Loss: 0.0175\n",
      "Epoch109 Batch10 Loss: 0.0254\n",
      "Epoch109 Batch20 Loss: 0.0155\n",
      "shuffling the dataset\n",
      "Epoch109 Batch30 Loss: 0.0193\n",
      "Epoch109 Batch40 Loss: 0.0204\n",
      "Epoch109 Batch50 Loss: 0.0442\n",
      "Epoch109 Batch60 Loss: 0.0279\n",
      "Epoch109 Batch70 Loss: 0.0200\n",
      "train Loss: 0.0221\n",
      "Epoch 110/499\n",
      "----------\n",
      "Epoch110 Batch0 Loss: 0.0168\n",
      "Epoch110 Batch10 Loss: 0.0151\n",
      "Epoch110 Batch20 Loss: 0.0164\n",
      "shuffling the dataset\n",
      "Epoch110 Batch30 Loss: 0.0200\n",
      "Epoch110 Batch40 Loss: 0.0315\n",
      "Epoch110 Batch50 Loss: 0.0209\n",
      "Epoch110 Batch60 Loss: 0.0188\n",
      "Epoch110 Batch70 Loss: 0.0105\n",
      "train Loss: 0.0210\n",
      "Epoch 111/499\n",
      "----------\n",
      "Epoch111 Batch0 Loss: 0.0278\n",
      "Epoch111 Batch10 Loss: 0.0167\n",
      "Epoch111 Batch20 Loss: 0.0217\n",
      "shuffling the dataset\n",
      "Epoch111 Batch30 Loss: 0.0171\n",
      "Epoch111 Batch40 Loss: 0.0198\n",
      "Epoch111 Batch50 Loss: 0.0136\n",
      "Epoch111 Batch60 Loss: 0.0178\n",
      "Epoch111 Batch70 Loss: 0.0206\n",
      "train Loss: 0.0212\n",
      "Epoch 112/499\n",
      "----------\n",
      "Epoch112 Batch0 Loss: 0.0261\n",
      "Epoch112 Batch10 Loss: 0.0155\n",
      "Epoch112 Batch20 Loss: 0.0156\n",
      "shuffling the dataset\n",
      "Epoch112 Batch30 Loss: 0.0156\n",
      "Epoch112 Batch40 Loss: 0.0247\n",
      "Epoch112 Batch50 Loss: 0.0222\n",
      "Epoch112 Batch60 Loss: 0.0164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch112 Batch70 Loss: 0.0157\n",
      "train Loss: 0.0203\n",
      "Epoch 113/499\n",
      "----------\n",
      "Epoch113 Batch0 Loss: 0.0236\n",
      "Epoch113 Batch10 Loss: 0.0229\n",
      "Epoch113 Batch20 Loss: 0.0210\n",
      "shuffling the dataset\n",
      "Epoch113 Batch30 Loss: 0.0169\n",
      "Epoch113 Batch40 Loss: 0.0152\n",
      "Epoch113 Batch50 Loss: 0.0275\n",
      "Epoch113 Batch60 Loss: 0.0188\n",
      "Epoch113 Batch70 Loss: 0.0289\n",
      "train Loss: 0.0209\n",
      "Epoch 114/499\n",
      "----------\n",
      "Epoch114 Batch0 Loss: 0.0197\n",
      "Epoch114 Batch10 Loss: 0.0167\n",
      "Epoch114 Batch20 Loss: 0.0199\n",
      "shuffling the dataset\n",
      "Epoch114 Batch30 Loss: 0.0235\n",
      "Epoch114 Batch40 Loss: 0.0197\n",
      "Epoch114 Batch50 Loss: 0.0163\n",
      "Epoch114 Batch60 Loss: 0.0194\n",
      "Epoch114 Batch70 Loss: 0.0113\n",
      "train Loss: 0.0197\n",
      "Epoch 115/499\n",
      "----------\n",
      "Epoch115 Batch0 Loss: 0.0204\n",
      "Epoch115 Batch10 Loss: 0.0116\n",
      "Epoch115 Batch20 Loss: 0.0156\n",
      "shuffling the dataset\n",
      "Epoch115 Batch30 Loss: 0.0128\n",
      "Epoch115 Batch40 Loss: 0.0147\n",
      "Epoch115 Batch50 Loss: 0.0231\n",
      "Epoch115 Batch60 Loss: 0.0138\n",
      "Epoch115 Batch70 Loss: 0.0155\n",
      "train Loss: 0.0189\n",
      "Epoch 116/499\n",
      "----------\n",
      "Epoch116 Batch0 Loss: 0.0209\n",
      "Epoch116 Batch10 Loss: 0.0175\n",
      "Epoch116 Batch20 Loss: 0.0243\n",
      "shuffling the dataset\n",
      "Epoch116 Batch30 Loss: 0.0259\n",
      "Epoch116 Batch40 Loss: 0.0200\n",
      "Epoch116 Batch50 Loss: 0.0107\n",
      "Epoch116 Batch60 Loss: 0.0145\n",
      "Epoch116 Batch70 Loss: 0.0115\n",
      "train Loss: 0.0204\n",
      "Epoch 117/499\n",
      "----------\n",
      "Epoch117 Batch0 Loss: 0.0206\n",
      "Epoch117 Batch10 Loss: 0.0167\n",
      "Epoch117 Batch20 Loss: 0.0197\n",
      "shuffling the dataset\n",
      "Epoch117 Batch30 Loss: 0.0250\n",
      "Epoch117 Batch40 Loss: 0.0173\n",
      "Epoch117 Batch50 Loss: 0.0211\n",
      "Epoch117 Batch60 Loss: 0.0153\n",
      "Epoch117 Batch70 Loss: 0.0133\n",
      "train Loss: 0.0177\n",
      "Epoch 118/499\n",
      "----------\n",
      "Epoch118 Batch0 Loss: 0.0153\n",
      "Epoch118 Batch10 Loss: 0.0258\n",
      "Epoch118 Batch20 Loss: 0.0268\n",
      "Epoch118 Batch30 Loss: 0.0210\n",
      "shuffling the dataset\n",
      "Epoch118 Batch40 Loss: 0.0272\n",
      "Epoch118 Batch50 Loss: 0.0268\n",
      "Epoch118 Batch60 Loss: 0.0150\n",
      "Epoch118 Batch70 Loss: 0.0140\n",
      "train Loss: 0.0201\n",
      "Epoch 119/499\n",
      "----------\n",
      "Epoch119 Batch0 Loss: 0.0314\n",
      "Epoch119 Batch10 Loss: 0.0225\n",
      "Epoch119 Batch20 Loss: 0.0143\n",
      "Epoch119 Batch30 Loss: 0.0293\n",
      "shuffling the dataset\n",
      "Epoch119 Batch40 Loss: 0.0183\n",
      "Epoch119 Batch50 Loss: 0.0126\n",
      "Epoch119 Batch60 Loss: 0.0182\n",
      "Epoch119 Batch70 Loss: 0.0153\n",
      "train Loss: 0.0183\n",
      "Epoch 120/499\n",
      "----------\n",
      "Epoch120 Batch0 Loss: 0.0120\n",
      "Epoch120 Batch10 Loss: 0.0172\n",
      "Epoch120 Batch20 Loss: 0.0227\n",
      "Epoch120 Batch30 Loss: 0.0256\n",
      "shuffling the dataset\n",
      "Epoch120 Batch40 Loss: 0.0135\n",
      "Epoch120 Batch50 Loss: 0.0269\n",
      "Epoch120 Batch60 Loss: 0.0232\n",
      "Epoch120 Batch70 Loss: 0.0141\n",
      "train Loss: 0.0191\n",
      "Epoch 121/499\n",
      "----------\n",
      "Epoch121 Batch0 Loss: 0.0176\n",
      "Epoch121 Batch10 Loss: 0.0170\n",
      "Epoch121 Batch20 Loss: 0.0177\n",
      "Epoch121 Batch30 Loss: 0.0179\n",
      "shuffling the dataset\n",
      "Epoch121 Batch40 Loss: 0.0150\n",
      "Epoch121 Batch50 Loss: 0.0291\n",
      "Epoch121 Batch60 Loss: 0.0113\n",
      "Epoch121 Batch70 Loss: 0.0257\n",
      "train Loss: 0.0180\n",
      "Epoch 122/499\n",
      "----------\n",
      "Epoch122 Batch0 Loss: 0.0130\n",
      "Epoch122 Batch10 Loss: 0.0133\n",
      "Epoch122 Batch20 Loss: 0.0159\n",
      "Epoch122 Batch30 Loss: 0.0275\n",
      "shuffling the dataset\n",
      "Epoch122 Batch40 Loss: 0.0185\n",
      "Epoch122 Batch50 Loss: 0.0174\n",
      "Epoch122 Batch60 Loss: 0.0203\n",
      "Epoch122 Batch70 Loss: 0.0213\n",
      "train Loss: 0.0188\n",
      "Epoch 123/499\n",
      "----------\n",
      "Epoch123 Batch0 Loss: 0.0262\n",
      "Epoch123 Batch10 Loss: 0.0254\n",
      "Epoch123 Batch20 Loss: 0.0220\n",
      "Epoch123 Batch30 Loss: 0.0187\n",
      "shuffling the dataset\n",
      "Epoch123 Batch40 Loss: 0.0220\n",
      "Epoch123 Batch50 Loss: 0.0180\n",
      "Epoch123 Batch60 Loss: 0.0180\n",
      "Epoch123 Batch70 Loss: 0.0167\n",
      "train Loss: 0.0210\n",
      "Epoch 124/499\n",
      "----------\n",
      "Epoch124 Batch0 Loss: 0.0187\n",
      "Epoch124 Batch10 Loss: 0.0205\n",
      "Epoch124 Batch20 Loss: 0.0546\n",
      "Epoch124 Batch30 Loss: 0.0566\n",
      "shuffling the dataset\n",
      "Epoch124 Batch40 Loss: 0.0404\n",
      "Epoch124 Batch50 Loss: 0.0498\n",
      "Epoch124 Batch60 Loss: 0.0480\n",
      "Epoch124 Batch70 Loss: 0.0637\n",
      "train Loss: 0.0426\n",
      "Epoch 125/499\n",
      "----------\n",
      "Epoch125 Batch0 Loss: 0.0571\n",
      "Epoch125 Batch10 Loss: 0.0508\n",
      "Epoch125 Batch20 Loss: 0.0435\n",
      "Epoch125 Batch30 Loss: 0.0580\n",
      "shuffling the dataset\n",
      "Epoch125 Batch40 Loss: 0.0266\n",
      "Epoch125 Batch50 Loss: 0.0224\n",
      "Epoch125 Batch60 Loss: 0.0369\n",
      "Epoch125 Batch70 Loss: 0.0183\n",
      "train Loss: 0.0325\n",
      "Epoch 126/499\n",
      "----------\n",
      "Epoch126 Batch0 Loss: 0.0189\n",
      "Epoch126 Batch10 Loss: 0.0178\n",
      "Epoch126 Batch20 Loss: 0.0214\n",
      "Epoch126 Batch30 Loss: 0.0503\n",
      "shuffling the dataset\n",
      "Epoch126 Batch40 Loss: 0.0157\n",
      "Epoch126 Batch50 Loss: 0.0095\n",
      "Epoch126 Batch60 Loss: 0.0214\n",
      "Epoch126 Batch70 Loss: 0.0205\n",
      "train Loss: 0.0210\n",
      "Epoch 127/499\n",
      "----------\n",
      "Epoch127 Batch0 Loss: 0.0213\n",
      "Epoch127 Batch10 Loss: 0.0290\n",
      "Epoch127 Batch20 Loss: 0.0159\n",
      "Epoch127 Batch30 Loss: 0.0156\n",
      "shuffling the dataset\n",
      "Epoch127 Batch40 Loss: 0.0175\n",
      "Epoch127 Batch50 Loss: 0.0070\n",
      "Epoch127 Batch60 Loss: 0.0161\n",
      "Epoch127 Batch70 Loss: 0.0129\n",
      "train Loss: 0.0190\n",
      "Epoch 128/499\n",
      "----------\n",
      "Epoch128 Batch0 Loss: 0.0185\n",
      "Epoch128 Batch10 Loss: 0.0146\n",
      "Epoch128 Batch20 Loss: 0.0221\n",
      "Epoch128 Batch30 Loss: 0.0182\n",
      "shuffling the dataset\n",
      "Epoch128 Batch40 Loss: 0.0182\n",
      "Epoch128 Batch50 Loss: 0.0158\n",
      "Epoch128 Batch60 Loss: 0.0193\n",
      "Epoch128 Batch70 Loss: 0.0124\n",
      "train Loss: 0.0168\n",
      "Epoch 129/499\n",
      "----------\n",
      "Epoch129 Batch0 Loss: 0.0183\n",
      "Epoch129 Batch10 Loss: 0.0134\n",
      "Epoch129 Batch20 Loss: 0.0146\n",
      "Epoch129 Batch30 Loss: 0.0190\n",
      "shuffling the dataset\n",
      "Epoch129 Batch40 Loss: 0.0207\n",
      "Epoch129 Batch50 Loss: 0.0447\n",
      "Epoch129 Batch60 Loss: 0.0205\n",
      "Epoch129 Batch70 Loss: 0.0424\n",
      "train Loss: 0.0168\n",
      "Epoch 130/499\n",
      "----------\n",
      "Epoch130 Batch0 Loss: 0.0149\n",
      "Epoch130 Batch10 Loss: 0.0099\n",
      "Epoch130 Batch20 Loss: 0.0174\n",
      "Epoch130 Batch30 Loss: 0.0145\n",
      "Epoch130 Batch40 Loss: 0.0209\n",
      "shuffling the dataset\n",
      "Epoch130 Batch50 Loss: 0.0127\n",
      "Epoch130 Batch60 Loss: 0.0122\n",
      "Epoch130 Batch70 Loss: 0.0110\n",
      "train Loss: 0.0147\n",
      "Epoch 131/499\n",
      "----------\n",
      "Epoch131 Batch0 Loss: 0.0108\n",
      "Epoch131 Batch10 Loss: 0.0187\n",
      "Epoch131 Batch20 Loss: 0.0144\n",
      "Epoch131 Batch30 Loss: 0.0156\n",
      "Epoch131 Batch40 Loss: 0.0082\n",
      "shuffling the dataset\n",
      "Epoch131 Batch50 Loss: 0.0131\n",
      "Epoch131 Batch60 Loss: 0.0141\n",
      "Epoch131 Batch70 Loss: 0.0127\n",
      "train Loss: 0.0160\n",
      "Epoch 132/499\n",
      "----------\n",
      "Epoch132 Batch0 Loss: 0.0107\n",
      "Epoch132 Batch10 Loss: 0.0173\n",
      "Epoch132 Batch20 Loss: 0.0108\n",
      "Epoch132 Batch30 Loss: 0.0126\n",
      "Epoch132 Batch40 Loss: 0.0143\n",
      "shuffling the dataset\n",
      "Epoch132 Batch50 Loss: 0.0097\n",
      "Epoch132 Batch60 Loss: 0.0093\n",
      "Epoch132 Batch70 Loss: 0.0130\n",
      "train Loss: 0.0157\n",
      "Epoch 133/499\n",
      "----------\n",
      "Epoch133 Batch0 Loss: 0.0195\n",
      "Epoch133 Batch10 Loss: 0.0119\n",
      "Epoch133 Batch20 Loss: 0.0092\n",
      "Epoch133 Batch30 Loss: 0.0104\n",
      "Epoch133 Batch40 Loss: 0.0167\n",
      "shuffling the dataset\n",
      "Epoch133 Batch50 Loss: 0.0106\n",
      "Epoch133 Batch60 Loss: 0.0157\n",
      "Epoch133 Batch70 Loss: 0.0196\n",
      "train Loss: 0.0145\n",
      "Epoch 134/499\n",
      "----------\n",
      "Epoch134 Batch0 Loss: 0.0155\n",
      "Epoch134 Batch10 Loss: 0.0312\n",
      "Epoch134 Batch20 Loss: 0.0156\n",
      "Epoch134 Batch30 Loss: 0.0122\n",
      "Epoch134 Batch40 Loss: 0.0090\n",
      "shuffling the dataset\n",
      "Epoch134 Batch50 Loss: 0.0144\n",
      "Epoch134 Batch60 Loss: 0.0173\n",
      "Epoch134 Batch70 Loss: 0.0136\n",
      "train Loss: 0.0147\n",
      "Epoch 135/499\n",
      "----------\n",
      "Epoch135 Batch0 Loss: 0.0096\n",
      "Epoch135 Batch10 Loss: 0.0131\n",
      "Epoch135 Batch20 Loss: 0.0088\n",
      "Epoch135 Batch30 Loss: 0.0180\n",
      "Epoch135 Batch40 Loss: 0.0113\n",
      "shuffling the dataset\n",
      "Epoch135 Batch50 Loss: 0.0442\n",
      "Epoch135 Batch60 Loss: 0.0090\n",
      "Epoch135 Batch70 Loss: 0.0147\n",
      "train Loss: 0.0144\n",
      "Epoch 136/499\n",
      "----------\n",
      "Epoch136 Batch0 Loss: 0.0123\n",
      "Epoch136 Batch10 Loss: 0.0388\n",
      "Epoch136 Batch20 Loss: 0.0083\n",
      "Epoch136 Batch30 Loss: 0.0113\n",
      "Epoch136 Batch40 Loss: 0.0211\n",
      "shuffling the dataset\n",
      "Epoch136 Batch50 Loss: 0.0118\n",
      "Epoch136 Batch60 Loss: 0.0136\n",
      "Epoch136 Batch70 Loss: 0.0134\n",
      "train Loss: 0.0139\n",
      "Epoch 137/499\n",
      "----------\n",
      "Epoch137 Batch0 Loss: 0.0110\n",
      "Epoch137 Batch10 Loss: 0.0109\n",
      "Epoch137 Batch20 Loss: 0.0155\n",
      "Epoch137 Batch30 Loss: 0.0087\n",
      "Epoch137 Batch40 Loss: 0.0141\n",
      "shuffling the dataset\n",
      "Epoch137 Batch50 Loss: 0.0307\n",
      "Epoch137 Batch60 Loss: 0.0088\n",
      "Epoch137 Batch70 Loss: 0.0100\n",
      "train Loss: 0.0145\n",
      "Epoch 138/499\n",
      "----------\n",
      "Epoch138 Batch0 Loss: 0.0052\n",
      "Epoch138 Batch10 Loss: 0.0180\n",
      "Epoch138 Batch20 Loss: 0.0156\n",
      "Epoch138 Batch30 Loss: 0.0078\n",
      "Epoch138 Batch40 Loss: 0.0141\n",
      "shuffling the dataset\n",
      "Epoch138 Batch50 Loss: 0.0142\n",
      "Epoch138 Batch60 Loss: 0.0101\n",
      "Epoch138 Batch70 Loss: 0.0097\n",
      "train Loss: 0.0136\n",
      "Epoch 139/499\n",
      "----------\n",
      "Epoch139 Batch0 Loss: 0.0147\n",
      "Epoch139 Batch10 Loss: 0.0134\n",
      "Epoch139 Batch20 Loss: 0.0114\n",
      "Epoch139 Batch30 Loss: 0.0365\n",
      "Epoch139 Batch40 Loss: 0.0195\n",
      "shuffling the dataset\n",
      "Epoch139 Batch50 Loss: 0.0168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch139 Batch60 Loss: 0.0161\n",
      "Epoch139 Batch70 Loss: 0.0143\n",
      "train Loss: 0.0138\n",
      "Epoch 140/499\n",
      "----------\n",
      "Epoch140 Batch0 Loss: 0.0164\n",
      "Epoch140 Batch10 Loss: 0.0120\n",
      "Epoch140 Batch20 Loss: 0.0128\n",
      "Epoch140 Batch30 Loss: 0.0116\n",
      "Epoch140 Batch40 Loss: 0.0184\n",
      "shuffling the dataset\n",
      "Epoch140 Batch50 Loss: 0.0129\n",
      "Epoch140 Batch60 Loss: 0.0145\n",
      "Epoch140 Batch70 Loss: 0.0130\n",
      "train Loss: 0.0148\n",
      "Epoch 141/499\n",
      "----------\n",
      "Epoch141 Batch0 Loss: 0.0283\n",
      "Epoch141 Batch10 Loss: 0.0096\n",
      "Epoch141 Batch20 Loss: 0.0129\n",
      "Epoch141 Batch30 Loss: 0.0115\n",
      "Epoch141 Batch40 Loss: 0.0151\n",
      "Epoch141 Batch50 Loss: 0.0064\n",
      "shuffling the dataset\n",
      "Epoch141 Batch60 Loss: 0.0160\n",
      "Epoch141 Batch70 Loss: 0.0147\n",
      "train Loss: 0.0144\n",
      "Epoch 142/499\n",
      "----------\n",
      "Epoch142 Batch0 Loss: 0.0084\n",
      "Epoch142 Batch10 Loss: 0.0094\n",
      "Epoch142 Batch20 Loss: 0.0139\n",
      "Epoch142 Batch30 Loss: 0.0170\n",
      "Epoch142 Batch40 Loss: 0.0140\n",
      "Epoch142 Batch50 Loss: 0.0106\n",
      "shuffling the dataset\n",
      "Epoch142 Batch60 Loss: 0.0250\n",
      "Epoch142 Batch70 Loss: 0.0154\n",
      "train Loss: 0.0148\n",
      "Epoch 143/499\n",
      "----------\n",
      "Epoch143 Batch0 Loss: 0.0174\n",
      "Epoch143 Batch10 Loss: 0.0126\n",
      "Epoch143 Batch20 Loss: 0.0163\n",
      "Epoch143 Batch30 Loss: 0.0173\n",
      "Epoch143 Batch40 Loss: 0.0111\n",
      "Epoch143 Batch50 Loss: 0.0103\n",
      "shuffling the dataset\n",
      "Epoch143 Batch60 Loss: 0.0084\n",
      "Epoch143 Batch70 Loss: 0.0280\n",
      "train Loss: 0.0141\n",
      "Epoch 144/499\n",
      "----------\n",
      "Epoch144 Batch0 Loss: 0.0129\n",
      "Epoch144 Batch10 Loss: 0.0135\n",
      "Epoch144 Batch20 Loss: 0.0123\n",
      "Epoch144 Batch30 Loss: 0.0117\n",
      "Epoch144 Batch40 Loss: 0.0175\n",
      "Epoch144 Batch50 Loss: 0.0115\n",
      "shuffling the dataset\n",
      "Epoch144 Batch60 Loss: 0.0147\n",
      "Epoch144 Batch70 Loss: 0.0134\n",
      "train Loss: 0.0134\n",
      "Epoch 145/499\n",
      "----------\n",
      "Epoch145 Batch0 Loss: 0.0181\n",
      "Epoch145 Batch10 Loss: 0.0111\n",
      "Epoch145 Batch20 Loss: 0.0125\n",
      "Epoch145 Batch30 Loss: 0.0136\n",
      "Epoch145 Batch40 Loss: 0.0409\n",
      "Epoch145 Batch50 Loss: 0.0096\n",
      "shuffling the dataset\n",
      "Epoch145 Batch60 Loss: 0.0142\n",
      "Epoch145 Batch70 Loss: 0.0110\n",
      "train Loss: 0.0144\n",
      "Epoch 146/499\n",
      "----------\n",
      "Epoch146 Batch0 Loss: 0.0097\n",
      "Epoch146 Batch10 Loss: 0.0101\n",
      "Epoch146 Batch20 Loss: 0.0072\n",
      "Epoch146 Batch30 Loss: 0.0102\n",
      "Epoch146 Batch40 Loss: 0.0153\n",
      "Epoch146 Batch50 Loss: 0.0126\n",
      "shuffling the dataset\n",
      "Epoch146 Batch60 Loss: 0.0088\n",
      "Epoch146 Batch70 Loss: 0.0091\n",
      "train Loss: 0.0126\n",
      "Epoch 147/499\n",
      "----------\n",
      "Epoch147 Batch0 Loss: 0.0131\n",
      "Epoch147 Batch10 Loss: 0.0084\n",
      "Epoch147 Batch20 Loss: 0.0080\n",
      "Epoch147 Batch30 Loss: 0.0078\n",
      "Epoch147 Batch40 Loss: 0.0129\n",
      "Epoch147 Batch50 Loss: 0.0146\n",
      "shuffling the dataset\n",
      "Epoch147 Batch60 Loss: 0.0125\n",
      "Epoch147 Batch70 Loss: 0.0125\n",
      "train Loss: 0.0123\n",
      "Epoch 148/499\n",
      "----------\n",
      "Epoch148 Batch0 Loss: 0.0094\n",
      "Epoch148 Batch10 Loss: 0.0128\n",
      "Epoch148 Batch20 Loss: 0.0459\n",
      "Epoch148 Batch30 Loss: 0.0132\n",
      "Epoch148 Batch40 Loss: 0.0077\n",
      "Epoch148 Batch50 Loss: 0.0098\n",
      "shuffling the dataset\n",
      "Epoch148 Batch60 Loss: 0.0079\n",
      "Epoch148 Batch70 Loss: 0.0169\n",
      "train Loss: 0.0130\n",
      "Epoch 149/499\n",
      "----------\n",
      "Epoch149 Batch0 Loss: 0.0128\n",
      "Epoch149 Batch10 Loss: 0.0133\n",
      "Epoch149 Batch20 Loss: 0.0110\n",
      "Epoch149 Batch30 Loss: 0.0121\n",
      "Epoch149 Batch40 Loss: 0.0152\n",
      "Epoch149 Batch50 Loss: 0.0151\n",
      "shuffling the dataset\n",
      "Epoch149 Batch60 Loss: 0.0086\n",
      "Epoch149 Batch70 Loss: 0.0128\n",
      "train Loss: 0.0130\n",
      "Epoch 150/499\n",
      "----------\n",
      "Epoch150 Batch0 Loss: 0.0100\n",
      "Epoch150 Batch10 Loss: 0.0140\n",
      "Epoch150 Batch20 Loss: 0.0308\n",
      "Epoch150 Batch30 Loss: 0.0070\n",
      "Epoch150 Batch40 Loss: 0.0107\n",
      "Epoch150 Batch50 Loss: 0.0116\n",
      "shuffling the dataset\n",
      "Epoch150 Batch60 Loss: 0.0065\n",
      "Epoch150 Batch70 Loss: 0.0067\n",
      "train Loss: 0.0127\n",
      "Epoch 151/499\n",
      "----------\n",
      "Epoch151 Batch0 Loss: 0.0116\n",
      "Epoch151 Batch10 Loss: 0.0137\n",
      "Epoch151 Batch20 Loss: 0.0125\n",
      "Epoch151 Batch30 Loss: 0.0189\n",
      "Epoch151 Batch40 Loss: 0.0122\n",
      "Epoch151 Batch50 Loss: 0.0116\n",
      "shuffling the dataset\n",
      "Epoch151 Batch60 Loss: 0.0089\n",
      "Epoch151 Batch70 Loss: 0.0088\n",
      "train Loss: 0.0119\n",
      "Epoch 152/499\n",
      "----------\n",
      "Epoch152 Batch0 Loss: 0.0146\n",
      "Epoch152 Batch10 Loss: 0.0076\n",
      "Epoch152 Batch20 Loss: 0.0090\n",
      "Epoch152 Batch30 Loss: 0.0136\n",
      "Epoch152 Batch40 Loss: 0.0119\n",
      "Epoch152 Batch50 Loss: 0.0156\n",
      "Epoch152 Batch60 Loss: 0.0116\n",
      "shuffling the dataset\n",
      "Epoch152 Batch70 Loss: 0.0094\n",
      "train Loss: 0.0126\n",
      "Epoch 153/499\n",
      "----------\n",
      "Epoch153 Batch0 Loss: 0.0040\n",
      "Epoch153 Batch10 Loss: 0.0091\n",
      "Epoch153 Batch20 Loss: 0.0131\n",
      "Epoch153 Batch30 Loss: 0.0107\n",
      "Epoch153 Batch40 Loss: 0.0273\n",
      "Epoch153 Batch50 Loss: 0.0140\n",
      "Epoch153 Batch60 Loss: 0.0096\n",
      "shuffling the dataset\n",
      "Epoch153 Batch70 Loss: 0.0150\n",
      "train Loss: 0.0127\n",
      "Epoch 154/499\n",
      "----------\n",
      "Epoch154 Batch0 Loss: 0.0124\n",
      "Epoch154 Batch10 Loss: 0.0120\n",
      "Epoch154 Batch20 Loss: 0.0077\n",
      "Epoch154 Batch30 Loss: 0.0402\n",
      "Epoch154 Batch40 Loss: 0.0122\n",
      "Epoch154 Batch50 Loss: 0.0126\n",
      "Epoch154 Batch60 Loss: 0.0178\n",
      "shuffling the dataset\n",
      "Epoch154 Batch70 Loss: 0.0170\n",
      "train Loss: 0.0138\n",
      "Epoch 155/499\n",
      "----------\n",
      "Epoch155 Batch0 Loss: 0.0169\n",
      "Epoch155 Batch10 Loss: 0.0096\n",
      "Epoch155 Batch20 Loss: 0.0153\n",
      "Epoch155 Batch30 Loss: 0.0239\n",
      "Epoch155 Batch40 Loss: 0.0194\n",
      "Epoch155 Batch50 Loss: 0.0217\n",
      "Epoch155 Batch60 Loss: 0.0232\n",
      "shuffling the dataset\n",
      "Epoch155 Batch70 Loss: 0.0407\n",
      "train Loss: 0.0197\n",
      "Epoch 156/499\n",
      "----------\n",
      "Epoch156 Batch0 Loss: 0.0231\n",
      "Epoch156 Batch10 Loss: 0.0284\n",
      "Epoch156 Batch20 Loss: 0.0265\n",
      "Epoch156 Batch30 Loss: 0.0429\n",
      "Epoch156 Batch40 Loss: 0.0314\n",
      "Epoch156 Batch50 Loss: 0.0317\n",
      "Epoch156 Batch60 Loss: 0.0154\n",
      "shuffling the dataset\n",
      "Epoch156 Batch70 Loss: 0.0246\n",
      "train Loss: 0.0273\n",
      "Epoch 157/499\n",
      "----------\n",
      "Epoch157 Batch0 Loss: 0.0247\n",
      "Epoch157 Batch10 Loss: 0.0268\n",
      "Epoch157 Batch20 Loss: 0.0125\n",
      "Epoch157 Batch30 Loss: 0.0155\n",
      "Epoch157 Batch40 Loss: 0.0149\n",
      "Epoch157 Batch50 Loss: 0.0173\n",
      "Epoch157 Batch60 Loss: 0.0149\n",
      "shuffling the dataset\n",
      "Epoch157 Batch70 Loss: 0.0126\n",
      "train Loss: 0.0175\n",
      "Epoch 158/499\n",
      "----------\n",
      "Epoch158 Batch0 Loss: 0.0149\n",
      "Epoch158 Batch10 Loss: 0.0088\n",
      "Epoch158 Batch20 Loss: 0.0144\n",
      "Epoch158 Batch30 Loss: 0.0123\n",
      "Epoch158 Batch40 Loss: 0.0189\n",
      "Epoch158 Batch50 Loss: 0.0122\n",
      "Epoch158 Batch60 Loss: 0.0145\n",
      "shuffling the dataset\n",
      "Epoch158 Batch70 Loss: 0.0133\n",
      "train Loss: 0.0153\n",
      "Epoch 159/499\n",
      "----------\n",
      "Epoch159 Batch0 Loss: 0.0110\n",
      "Epoch159 Batch10 Loss: 0.0091\n",
      "Epoch159 Batch20 Loss: 0.0120\n",
      "Epoch159 Batch30 Loss: 0.0134\n",
      "Epoch159 Batch40 Loss: 0.0070\n",
      "Epoch159 Batch50 Loss: 0.0097\n",
      "Epoch159 Batch60 Loss: 0.0147\n",
      "shuffling the dataset\n",
      "Epoch159 Batch70 Loss: 0.0204\n",
      "train Loss: 0.0134\n",
      "Epoch 160/499\n",
      "----------\n",
      "Epoch160 Batch0 Loss: 0.0117\n",
      "Epoch160 Batch10 Loss: 0.0163\n",
      "Epoch160 Batch20 Loss: 0.0085\n",
      "Epoch160 Batch30 Loss: 0.0097\n",
      "Epoch160 Batch40 Loss: 0.0120\n",
      "Epoch160 Batch50 Loss: 0.0151\n",
      "Epoch160 Batch60 Loss: 0.0142\n",
      "shuffling the dataset\n",
      "Epoch160 Batch70 Loss: 0.0067\n",
      "train Loss: 0.0118\n",
      "Epoch 161/499\n",
      "----------\n",
      "Epoch161 Batch0 Loss: 0.0114\n",
      "Epoch161 Batch10 Loss: 0.0293\n",
      "Epoch161 Batch20 Loss: 0.0148\n",
      "Epoch161 Batch30 Loss: 0.0110\n",
      "Epoch161 Batch40 Loss: 0.0108\n",
      "Epoch161 Batch50 Loss: 0.0139\n",
      "Epoch161 Batch60 Loss: 0.0133\n",
      "shuffling the dataset\n",
      "Epoch161 Batch70 Loss: 0.0123\n",
      "train Loss: 0.0122\n",
      "Epoch 162/499\n",
      "----------\n",
      "Epoch162 Batch0 Loss: 0.0088\n",
      "Epoch162 Batch10 Loss: 0.0090\n",
      "Epoch162 Batch20 Loss: 0.0081\n",
      "Epoch162 Batch30 Loss: 0.0087\n",
      "Epoch162 Batch40 Loss: 0.0071\n",
      "Epoch162 Batch50 Loss: 0.0106\n",
      "Epoch162 Batch60 Loss: 0.0084\n",
      "shuffling the dataset\n",
      "Epoch162 Batch70 Loss: 0.0137\n",
      "train Loss: 0.0120\n",
      "Epoch 163/499\n",
      "----------\n",
      "Epoch163 Batch0 Loss: 0.0156\n",
      "Epoch163 Batch10 Loss: 0.0107\n",
      "Epoch163 Batch20 Loss: 0.0141\n",
      "Epoch163 Batch30 Loss: 0.0090\n",
      "Epoch163 Batch40 Loss: 0.0077\n",
      "Epoch163 Batch50 Loss: 0.0091\n",
      "Epoch163 Batch60 Loss: 0.0126\n",
      "shuffling the dataset\n",
      "Epoch163 Batch70 Loss: 0.0161\n",
      "train Loss: 0.0117\n",
      "Epoch 164/499\n",
      "----------\n",
      "Epoch164 Batch0 Loss: 0.0122\n",
      "Epoch164 Batch10 Loss: 0.0128\n",
      "Epoch164 Batch20 Loss: 0.0110\n",
      "Epoch164 Batch30 Loss: 0.0197\n",
      "Epoch164 Batch40 Loss: 0.0095\n",
      "Epoch164 Batch50 Loss: 0.0102\n",
      "Epoch164 Batch60 Loss: 0.0122\n",
      "Epoch164 Batch70 Loss: 0.0081\n",
      "train Loss: 0.0118\n",
      "Epoch 165/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch165 Batch0 Loss: 0.0079\n",
      "Epoch165 Batch10 Loss: 0.0103\n",
      "Epoch165 Batch20 Loss: 0.0393\n",
      "Epoch165 Batch30 Loss: 0.0089\n",
      "Epoch165 Batch40 Loss: 0.0103\n",
      "Epoch165 Batch50 Loss: 0.0067\n",
      "Epoch165 Batch60 Loss: 0.0069\n",
      "Epoch165 Batch70 Loss: 0.0060\n",
      "train Loss: 0.0109\n",
      "Epoch 166/499\n",
      "----------\n",
      "Epoch166 Batch0 Loss: 0.0117\n",
      "shuffling the dataset\n",
      "Epoch166 Batch10 Loss: 0.0093\n",
      "Epoch166 Batch20 Loss: 0.0097\n",
      "Epoch166 Batch30 Loss: 0.0124\n",
      "Epoch166 Batch40 Loss: 0.0067\n",
      "Epoch166 Batch50 Loss: 0.0135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch166 Batch60 Loss: 0.0108\n",
      "Epoch166 Batch70 Loss: 0.0057\n",
      "train Loss: 0.0107\n",
      "Epoch 167/499\n",
      "----------\n",
      "Epoch167 Batch0 Loss: 0.0091\n",
      "shuffling the dataset\n",
      "Epoch167 Batch10 Loss: 0.0090\n",
      "Epoch167 Batch20 Loss: 0.0145\n",
      "Epoch167 Batch30 Loss: 0.0102\n",
      "Epoch167 Batch40 Loss: 0.0111\n",
      "Epoch167 Batch50 Loss: 0.0096\n",
      "Epoch167 Batch60 Loss: 0.0156\n",
      "Epoch167 Batch70 Loss: 0.0085\n",
      "train Loss: 0.0105\n",
      "Epoch 168/499\n",
      "----------\n",
      "Epoch168 Batch0 Loss: 0.0120\n",
      "shuffling the dataset\n",
      "Epoch168 Batch10 Loss: 0.0109\n",
      "Epoch168 Batch20 Loss: 0.0424\n",
      "Epoch168 Batch30 Loss: 0.0088\n",
      "Epoch168 Batch40 Loss: 0.0110\n",
      "Epoch168 Batch50 Loss: 0.0143\n",
      "Epoch168 Batch60 Loss: 0.0080\n",
      "Epoch168 Batch70 Loss: 0.0086\n",
      "train Loss: 0.0106\n",
      "Epoch 169/499\n",
      "----------\n",
      "Epoch169 Batch0 Loss: 0.0119\n",
      "shuffling the dataset\n",
      "Epoch169 Batch10 Loss: 0.0355\n",
      "Epoch169 Batch20 Loss: 0.0110\n",
      "Epoch169 Batch30 Loss: 0.0077\n",
      "Epoch169 Batch40 Loss: 0.0071\n",
      "Epoch169 Batch50 Loss: 0.0055\n",
      "Epoch169 Batch60 Loss: 0.0095\n",
      "Epoch169 Batch70 Loss: 0.0124\n",
      "train Loss: 0.0101\n",
      "Epoch 170/499\n",
      "----------\n",
      "Epoch170 Batch0 Loss: 0.0090\n",
      "shuffling the dataset\n",
      "Epoch170 Batch10 Loss: 0.0113\n",
      "Epoch170 Batch20 Loss: 0.0364\n",
      "Epoch170 Batch30 Loss: 0.0056\n",
      "Epoch170 Batch40 Loss: 0.0095\n",
      "Epoch170 Batch50 Loss: 0.0044\n",
      "Epoch170 Batch60 Loss: 0.0071\n",
      "Epoch170 Batch70 Loss: 0.0054\n",
      "train Loss: 0.0101\n",
      "Epoch 171/499\n",
      "----------\n",
      "Epoch171 Batch0 Loss: 0.0116\n",
      "shuffling the dataset\n",
      "Epoch171 Batch10 Loss: 0.0096\n",
      "Epoch171 Batch20 Loss: 0.0058\n",
      "Epoch171 Batch30 Loss: 0.0067\n",
      "Epoch171 Batch40 Loss: 0.0071\n",
      "Epoch171 Batch50 Loss: 0.0060\n",
      "Epoch171 Batch60 Loss: 0.0160\n",
      "Epoch171 Batch70 Loss: 0.0102\n",
      "train Loss: 0.0104\n",
      "Epoch 172/499\n",
      "----------\n",
      "Epoch172 Batch0 Loss: 0.0063\n",
      "shuffling the dataset\n",
      "Epoch172 Batch10 Loss: 0.0100\n",
      "Epoch172 Batch20 Loss: 0.0117\n",
      "Epoch172 Batch30 Loss: 0.0398\n",
      "Epoch172 Batch40 Loss: 0.0117\n",
      "Epoch172 Batch50 Loss: 0.0076\n",
      "Epoch172 Batch60 Loss: 0.0122\n",
      "Epoch172 Batch70 Loss: 0.0120\n",
      "train Loss: 0.0102\n",
      "Epoch 173/499\n",
      "----------\n",
      "Epoch173 Batch0 Loss: 0.0154\n",
      "shuffling the dataset\n",
      "Epoch173 Batch10 Loss: 0.0075\n",
      "Epoch173 Batch20 Loss: 0.0073\n",
      "Epoch173 Batch30 Loss: 0.0079\n",
      "Epoch173 Batch40 Loss: 0.0076\n",
      "Epoch173 Batch50 Loss: 0.0062\n",
      "Epoch173 Batch60 Loss: 0.0080\n",
      "Epoch173 Batch70 Loss: 0.0108\n",
      "train Loss: 0.0100\n",
      "Epoch 174/499\n",
      "----------\n",
      "Epoch174 Batch0 Loss: 0.0058\n",
      "shuffling the dataset\n",
      "Epoch174 Batch10 Loss: 0.0071\n",
      "Epoch174 Batch20 Loss: 0.0060\n",
      "Epoch174 Batch30 Loss: 0.0075\n",
      "Epoch174 Batch40 Loss: 0.0066\n",
      "Epoch174 Batch50 Loss: 0.0086\n",
      "Epoch174 Batch60 Loss: 0.0111\n",
      "Epoch174 Batch70 Loss: 0.0107\n",
      "train Loss: 0.0098\n",
      "Epoch 175/499\n",
      "----------\n",
      "Epoch175 Batch0 Loss: 0.0195\n",
      "shuffling the dataset\n",
      "Epoch175 Batch10 Loss: 0.0112\n",
      "Epoch175 Batch20 Loss: 0.0089\n",
      "Epoch175 Batch30 Loss: 0.0102\n",
      "Epoch175 Batch40 Loss: 0.0077\n",
      "Epoch175 Batch50 Loss: 0.0156\n",
      "Epoch175 Batch60 Loss: 0.0094\n",
      "Epoch175 Batch70 Loss: 0.0092\n",
      "train Loss: 0.0108\n",
      "Epoch 176/499\n",
      "----------\n",
      "Epoch176 Batch0 Loss: 0.0098\n",
      "shuffling the dataset\n",
      "Epoch176 Batch10 Loss: 0.0081\n",
      "Epoch176 Batch20 Loss: 0.0118\n",
      "Epoch176 Batch30 Loss: 0.0070\n",
      "Epoch176 Batch40 Loss: 0.0087\n",
      "Epoch176 Batch50 Loss: 0.0071\n",
      "Epoch176 Batch60 Loss: 0.0137\n",
      "Epoch176 Batch70 Loss: 0.0100\n",
      "train Loss: 0.0094\n",
      "Epoch 177/499\n",
      "----------\n",
      "Epoch177 Batch0 Loss: 0.0070\n",
      "Epoch177 Batch10 Loss: 0.0067\n",
      "shuffling the dataset\n",
      "Epoch177 Batch20 Loss: 0.0094\n",
      "Epoch177 Batch30 Loss: 0.0130\n",
      "Epoch177 Batch40 Loss: 0.0075\n",
      "Epoch177 Batch50 Loss: 0.0071\n",
      "Epoch177 Batch60 Loss: 0.0335\n",
      "Epoch177 Batch70 Loss: 0.0125\n",
      "train Loss: 0.0106\n",
      "Epoch 178/499\n",
      "----------\n",
      "Epoch178 Batch0 Loss: 0.0107\n",
      "Epoch178 Batch10 Loss: 0.0368\n",
      "shuffling the dataset\n",
      "Epoch178 Batch20 Loss: 0.0079\n",
      "Epoch178 Batch30 Loss: 0.0063\n",
      "Epoch178 Batch40 Loss: 0.0114\n",
      "Epoch178 Batch50 Loss: 0.0061\n",
      "Epoch178 Batch60 Loss: 0.0097\n",
      "Epoch178 Batch70 Loss: 0.0075\n",
      "train Loss: 0.0106\n",
      "Epoch 179/499\n",
      "----------\n",
      "Epoch179 Batch0 Loss: 0.0071\n",
      "Epoch179 Batch10 Loss: 0.0125\n",
      "shuffling the dataset\n",
      "Epoch179 Batch20 Loss: 0.0152\n",
      "Epoch179 Batch30 Loss: 0.0099\n",
      "Epoch179 Batch40 Loss: 0.0084\n",
      "Epoch179 Batch50 Loss: 0.0089\n",
      "Epoch179 Batch60 Loss: 0.0133\n",
      "Epoch179 Batch70 Loss: 0.0066\n",
      "train Loss: 0.0102\n",
      "Epoch 180/499\n",
      "----------\n",
      "Epoch180 Batch0 Loss: 0.0102\n",
      "Epoch180 Batch10 Loss: 0.0051\n",
      "shuffling the dataset\n",
      "Epoch180 Batch20 Loss: 0.0083\n",
      "Epoch180 Batch30 Loss: 0.0055\n",
      "Epoch180 Batch40 Loss: 0.0121\n",
      "Epoch180 Batch50 Loss: 0.0091\n",
      "Epoch180 Batch60 Loss: 0.0086\n",
      "Epoch180 Batch70 Loss: 0.0094\n",
      "train Loss: 0.0099\n",
      "Epoch 181/499\n",
      "----------\n",
      "Epoch181 Batch0 Loss: 0.0099\n",
      "Epoch181 Batch10 Loss: 0.0124\n",
      "shuffling the dataset\n",
      "Epoch181 Batch20 Loss: 0.0362\n",
      "Epoch181 Batch30 Loss: 0.0065\n",
      "Epoch181 Batch40 Loss: 0.0146\n",
      "Epoch181 Batch50 Loss: 0.0121\n",
      "Epoch181 Batch60 Loss: 0.0110\n",
      "Epoch181 Batch70 Loss: 0.0052\n",
      "train Loss: 0.0105\n",
      "Epoch 182/499\n",
      "----------\n",
      "Epoch182 Batch0 Loss: 0.0090\n",
      "Epoch182 Batch10 Loss: 0.0099\n",
      "shuffling the dataset\n",
      "Epoch182 Batch20 Loss: 0.0127\n",
      "Epoch182 Batch30 Loss: 0.0070\n",
      "Epoch182 Batch40 Loss: 0.0090\n",
      "Epoch182 Batch50 Loss: 0.0060\n",
      "Epoch182 Batch60 Loss: 0.0093\n",
      "Epoch182 Batch70 Loss: 0.0081\n",
      "train Loss: 0.0094\n",
      "Epoch 183/499\n",
      "----------\n",
      "Epoch183 Batch0 Loss: 0.0082\n",
      "Epoch183 Batch10 Loss: 0.0129\n",
      "shuffling the dataset\n",
      "Epoch183 Batch20 Loss: 0.0053\n",
      "Epoch183 Batch30 Loss: 0.0079\n",
      "Epoch183 Batch40 Loss: 0.0073\n",
      "Epoch183 Batch50 Loss: 0.0138\n",
      "Epoch183 Batch60 Loss: 0.0114\n",
      "Epoch183 Batch70 Loss: 0.0106\n",
      "train Loss: 0.0099\n",
      "Epoch 184/499\n",
      "----------\n",
      "Epoch184 Batch0 Loss: 0.0106\n",
      "Epoch184 Batch10 Loss: 0.0104\n",
      "shuffling the dataset\n",
      "Epoch184 Batch20 Loss: 0.0106\n",
      "Epoch184 Batch30 Loss: 0.0132\n",
      "Epoch184 Batch40 Loss: 0.0084\n",
      "Epoch184 Batch50 Loss: 0.0063\n",
      "Epoch184 Batch60 Loss: 0.0096\n",
      "Epoch184 Batch70 Loss: 0.0076\n",
      "train Loss: 0.0105\n",
      "Epoch 185/499\n",
      "----------\n",
      "Epoch185 Batch0 Loss: 0.0075\n",
      "Epoch185 Batch10 Loss: 0.0073\n",
      "shuffling the dataset\n",
      "Epoch185 Batch20 Loss: 0.0071\n",
      "Epoch185 Batch30 Loss: 0.0092\n",
      "Epoch185 Batch40 Loss: 0.0079\n",
      "Epoch185 Batch50 Loss: 0.0082\n",
      "Epoch185 Batch60 Loss: 0.0106\n",
      "Epoch185 Batch70 Loss: 0.0348\n",
      "train Loss: 0.0099\n",
      "Epoch 186/499\n",
      "----------\n",
      "Epoch186 Batch0 Loss: 0.0074\n",
      "Epoch186 Batch10 Loss: 0.0069\n",
      "shuffling the dataset\n",
      "Epoch186 Batch20 Loss: 0.0107\n",
      "Epoch186 Batch30 Loss: 0.0107\n",
      "Epoch186 Batch40 Loss: 0.0074\n",
      "Epoch186 Batch50 Loss: 0.0052\n",
      "Epoch186 Batch60 Loss: 0.0089\n",
      "Epoch186 Batch70 Loss: 0.0066\n",
      "train Loss: 0.0088\n",
      "Epoch 187/499\n",
      "----------\n",
      "Epoch187 Batch0 Loss: 0.0077\n",
      "Epoch187 Batch10 Loss: 0.0112\n",
      "shuffling the dataset\n",
      "Epoch187 Batch20 Loss: 0.0106\n",
      "Epoch187 Batch30 Loss: 0.0053\n",
      "Epoch187 Batch40 Loss: 0.0046\n",
      "Epoch187 Batch50 Loss: 0.0096\n",
      "Epoch187 Batch60 Loss: 0.0076\n",
      "Epoch187 Batch70 Loss: 0.0058\n",
      "train Loss: 0.0094\n",
      "Epoch 188/499\n",
      "----------\n",
      "Epoch188 Batch0 Loss: 0.0070\n",
      "Epoch188 Batch10 Loss: 0.0058\n",
      "shuffling the dataset\n",
      "Epoch188 Batch20 Loss: 0.0080\n",
      "Epoch188 Batch30 Loss: 0.0083\n",
      "Epoch188 Batch40 Loss: 0.0105\n",
      "Epoch188 Batch50 Loss: 0.0077\n",
      "Epoch188 Batch60 Loss: 0.0068\n",
      "Epoch188 Batch70 Loss: 0.0077\n",
      "train Loss: 0.0091\n",
      "Epoch 189/499\n",
      "----------\n",
      "Epoch189 Batch0 Loss: 0.0377\n",
      "Epoch189 Batch10 Loss: 0.0172\n",
      "Epoch189 Batch20 Loss: 0.0094\n",
      "shuffling the dataset\n",
      "Epoch189 Batch30 Loss: 0.0118\n",
      "Epoch189 Batch40 Loss: 0.0297\n",
      "Epoch189 Batch50 Loss: 0.0470\n",
      "Epoch189 Batch60 Loss: 0.0240\n",
      "Epoch189 Batch70 Loss: 0.0379\n",
      "train Loss: 0.0213\n",
      "Epoch 190/499\n",
      "----------\n",
      "Epoch190 Batch0 Loss: 0.0471\n",
      "Epoch190 Batch10 Loss: 0.0572\n",
      "Epoch190 Batch20 Loss: 0.0399\n",
      "shuffling the dataset\n",
      "Epoch190 Batch30 Loss: 0.0310\n",
      "Epoch190 Batch40 Loss: 0.0264\n",
      "Epoch190 Batch50 Loss: 0.0249\n",
      "Epoch190 Batch60 Loss: 0.0593\n",
      "Epoch190 Batch70 Loss: 0.0214\n",
      "train Loss: 0.0384\n",
      "Epoch 191/499\n",
      "----------\n",
      "Epoch191 Batch0 Loss: 0.0175\n",
      "Epoch191 Batch10 Loss: 0.0239\n",
      "Epoch191 Batch20 Loss: 0.0204\n",
      "shuffling the dataset\n",
      "Epoch191 Batch30 Loss: 0.0169\n",
      "Epoch191 Batch40 Loss: 0.0246\n",
      "Epoch191 Batch50 Loss: 0.0258\n",
      "Epoch191 Batch60 Loss: 0.0142\n",
      "Epoch191 Batch70 Loss: 0.0177\n",
      "train Loss: 0.0204\n",
      "Epoch 192/499\n",
      "----------\n",
      "Epoch192 Batch0 Loss: 0.0129\n",
      "Epoch192 Batch10 Loss: 0.0258\n",
      "Epoch192 Batch20 Loss: 0.0119\n",
      "shuffling the dataset\n",
      "Epoch192 Batch30 Loss: 0.0162\n",
      "Epoch192 Batch40 Loss: 0.0111\n",
      "Epoch192 Batch50 Loss: 0.0095\n",
      "Epoch192 Batch60 Loss: 0.0156\n",
      "Epoch192 Batch70 Loss: 0.0089\n",
      "train Loss: 0.0137\n",
      "Epoch 193/499\n",
      "----------\n",
      "Epoch193 Batch0 Loss: 0.0379\n",
      "Epoch193 Batch10 Loss: 0.0123\n",
      "Epoch193 Batch20 Loss: 0.0080\n",
      "shuffling the dataset\n",
      "Epoch193 Batch30 Loss: 0.0095\n",
      "Epoch193 Batch40 Loss: 0.0109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch193 Batch50 Loss: 0.0096\n",
      "Epoch193 Batch60 Loss: 0.0127\n",
      "Epoch193 Batch70 Loss: 0.0060\n",
      "train Loss: 0.0115\n",
      "Epoch 194/499\n",
      "----------\n",
      "Epoch194 Batch0 Loss: 0.0092\n",
      "Epoch194 Batch10 Loss: 0.0049\n",
      "Epoch194 Batch20 Loss: 0.0075\n",
      "shuffling the dataset\n",
      "Epoch194 Batch30 Loss: 0.0064\n",
      "Epoch194 Batch40 Loss: 0.0068\n",
      "Epoch194 Batch50 Loss: 0.0092\n",
      "Epoch194 Batch60 Loss: 0.0081\n",
      "Epoch194 Batch70 Loss: 0.0100\n",
      "train Loss: 0.0095\n",
      "Epoch 195/499\n",
      "----------\n",
      "Epoch195 Batch0 Loss: 0.0092\n",
      "Epoch195 Batch10 Loss: 0.0092\n",
      "Epoch195 Batch20 Loss: 0.0088\n",
      "shuffling the dataset\n",
      "Epoch195 Batch30 Loss: 0.0054\n",
      "Epoch195 Batch40 Loss: 0.0064\n",
      "Epoch195 Batch50 Loss: 0.0093\n",
      "Epoch195 Batch60 Loss: 0.0086\n",
      "Epoch195 Batch70 Loss: 0.0097\n",
      "train Loss: 0.0097\n",
      "Epoch 196/499\n",
      "----------\n",
      "Epoch196 Batch0 Loss: 0.0073\n",
      "Epoch196 Batch10 Loss: 0.0089\n",
      "Epoch196 Batch20 Loss: 0.0052\n",
      "shuffling the dataset\n",
      "Epoch196 Batch30 Loss: 0.0079\n",
      "Epoch196 Batch40 Loss: 0.0384\n",
      "Epoch196 Batch50 Loss: 0.0050\n",
      "Epoch196 Batch60 Loss: 0.0059\n",
      "Epoch196 Batch70 Loss: 0.0069\n",
      "train Loss: 0.0075\n",
      "Epoch 197/499\n",
      "----------\n",
      "Epoch197 Batch0 Loss: 0.0094\n",
      "Epoch197 Batch10 Loss: 0.0105\n",
      "Epoch197 Batch20 Loss: 0.0090\n",
      "shuffling the dataset\n",
      "Epoch197 Batch30 Loss: 0.0065\n",
      "Epoch197 Batch40 Loss: 0.0076\n",
      "Epoch197 Batch50 Loss: 0.0058\n",
      "Epoch197 Batch60 Loss: 0.0059\n",
      "Epoch197 Batch70 Loss: 0.0038\n",
      "train Loss: 0.0082\n",
      "Epoch 198/499\n",
      "----------\n",
      "Epoch198 Batch0 Loss: 0.0058\n",
      "Epoch198 Batch10 Loss: 0.0065\n",
      "Epoch198 Batch20 Loss: 0.0058\n",
      "shuffling the dataset\n",
      "Epoch198 Batch30 Loss: 0.0048\n",
      "Epoch198 Batch40 Loss: 0.0078\n",
      "Epoch198 Batch50 Loss: 0.0072\n",
      "Epoch198 Batch60 Loss: 0.0062\n",
      "Epoch198 Batch70 Loss: 0.0058\n",
      "train Loss: 0.0087\n",
      "Epoch 199/499\n",
      "----------\n",
      "Epoch199 Batch0 Loss: 0.0048\n",
      "Epoch199 Batch10 Loss: 0.0049\n",
      "Epoch199 Batch20 Loss: 0.0071\n",
      "shuffling the dataset\n",
      "Epoch199 Batch30 Loss: 0.0065\n",
      "Epoch199 Batch40 Loss: 0.0100\n",
      "Epoch199 Batch50 Loss: 0.0056\n",
      "Epoch199 Batch60 Loss: 0.0058\n",
      "Epoch199 Batch70 Loss: 0.0100\n",
      "train Loss: 0.0080\n",
      "Epoch 200/499\n",
      "----------\n",
      "Epoch200 Batch0 Loss: 0.0063\n",
      "Epoch200 Batch10 Loss: 0.0057\n",
      "Epoch200 Batch20 Loss: 0.0059\n",
      "Epoch200 Batch30 Loss: 0.0065\n",
      "shuffling the dataset\n",
      "Epoch200 Batch40 Loss: 0.0049\n",
      "Epoch200 Batch50 Loss: 0.0057\n",
      "Epoch200 Batch60 Loss: 0.0064\n",
      "Epoch200 Batch70 Loss: 0.0071\n",
      "train Loss: 0.0072\n",
      "Epoch 201/499\n",
      "----------\n",
      "Epoch201 Batch0 Loss: 0.0062\n",
      "Epoch201 Batch10 Loss: 0.0080\n",
      "Epoch201 Batch20 Loss: 0.0066\n",
      "Epoch201 Batch30 Loss: 0.0139\n",
      "shuffling the dataset\n",
      "Epoch201 Batch40 Loss: 0.0094\n",
      "Epoch201 Batch50 Loss: 0.0050\n",
      "Epoch201 Batch60 Loss: 0.0046\n",
      "Epoch201 Batch70 Loss: 0.0070\n",
      "train Loss: 0.0081\n",
      "Epoch 202/499\n",
      "----------\n",
      "Epoch202 Batch0 Loss: 0.0085\n",
      "Epoch202 Batch10 Loss: 0.0077\n",
      "Epoch202 Batch20 Loss: 0.0103\n",
      "Epoch202 Batch30 Loss: 0.0052\n",
      "shuffling the dataset\n",
      "Epoch202 Batch40 Loss: 0.0053\n",
      "Epoch202 Batch50 Loss: 0.0080\n",
      "Epoch202 Batch60 Loss: 0.0077\n",
      "Epoch202 Batch70 Loss: 0.0031\n",
      "train Loss: 0.0076\n",
      "Epoch 203/499\n",
      "----------\n",
      "Epoch203 Batch0 Loss: 0.0087\n",
      "Epoch203 Batch10 Loss: 0.0047\n",
      "Epoch203 Batch20 Loss: 0.0061\n",
      "Epoch203 Batch30 Loss: 0.0276\n",
      "shuffling the dataset\n",
      "Epoch203 Batch40 Loss: 0.0072\n",
      "Epoch203 Batch50 Loss: 0.0051\n",
      "Epoch203 Batch60 Loss: 0.0066\n",
      "Epoch203 Batch70 Loss: 0.0061\n",
      "train Loss: 0.0081\n",
      "Epoch 204/499\n",
      "----------\n",
      "Epoch204 Batch0 Loss: 0.0062\n",
      "Epoch204 Batch10 Loss: 0.0059\n",
      "Epoch204 Batch20 Loss: 0.0040\n",
      "Epoch204 Batch30 Loss: 0.0077\n",
      "shuffling the dataset\n",
      "Epoch204 Batch40 Loss: 0.0063\n",
      "Epoch204 Batch50 Loss: 0.0076\n",
      "Epoch204 Batch60 Loss: 0.0062\n",
      "Epoch204 Batch70 Loss: 0.0062\n",
      "train Loss: 0.0070\n",
      "Epoch 205/499\n",
      "----------\n",
      "Epoch205 Batch0 Loss: 0.0055\n",
      "Epoch205 Batch10 Loss: 0.0348\n",
      "Epoch205 Batch20 Loss: 0.0139\n",
      "Epoch205 Batch30 Loss: 0.0055\n",
      "shuffling the dataset\n",
      "Epoch205 Batch40 Loss: 0.0051\n",
      "Epoch205 Batch50 Loss: 0.0066\n",
      "Epoch205 Batch60 Loss: 0.0058\n",
      "Epoch205 Batch70 Loss: 0.0084\n",
      "train Loss: 0.0084\n",
      "Epoch 206/499\n",
      "----------\n",
      "Epoch206 Batch0 Loss: 0.0052\n",
      "Epoch206 Batch10 Loss: 0.0045\n",
      "Epoch206 Batch20 Loss: 0.0052\n",
      "Epoch206 Batch30 Loss: 0.0086\n",
      "shuffling the dataset\n",
      "Epoch206 Batch40 Loss: 0.0061\n",
      "Epoch206 Batch50 Loss: 0.0035\n",
      "Epoch206 Batch60 Loss: 0.0090\n",
      "Epoch206 Batch70 Loss: 0.0105\n",
      "train Loss: 0.0088\n",
      "Epoch 207/499\n",
      "----------\n",
      "Epoch207 Batch0 Loss: 0.0077\n",
      "Epoch207 Batch10 Loss: 0.0083\n",
      "Epoch207 Batch20 Loss: 0.0094\n",
      "Epoch207 Batch30 Loss: 0.0088\n",
      "shuffling the dataset\n",
      "Epoch207 Batch40 Loss: 0.0057\n",
      "Epoch207 Batch50 Loss: 0.0070\n",
      "Epoch207 Batch60 Loss: 0.0055\n",
      "Epoch207 Batch70 Loss: 0.0100\n",
      "train Loss: 0.0081\n",
      "Epoch 208/499\n",
      "----------\n",
      "Epoch208 Batch0 Loss: 0.0058\n",
      "Epoch208 Batch10 Loss: 0.0062\n",
      "Epoch208 Batch20 Loss: 0.0064\n",
      "Epoch208 Batch30 Loss: 0.0158\n",
      "shuffling the dataset\n",
      "Epoch208 Batch40 Loss: 0.0152\n",
      "Epoch208 Batch50 Loss: 0.0101\n",
      "Epoch208 Batch60 Loss: 0.0053\n",
      "Epoch208 Batch70 Loss: 0.0072\n",
      "train Loss: 0.0085\n",
      "Epoch 209/499\n",
      "----------\n",
      "Epoch209 Batch0 Loss: 0.0096\n",
      "Epoch209 Batch10 Loss: 0.0037\n",
      "Epoch209 Batch20 Loss: 0.0077\n",
      "Epoch209 Batch30 Loss: 0.0069\n",
      "shuffling the dataset\n",
      "Epoch209 Batch40 Loss: 0.0048\n",
      "Epoch209 Batch50 Loss: 0.0053\n",
      "Epoch209 Batch60 Loss: 0.0056\n",
      "Epoch209 Batch70 Loss: 0.0044\n",
      "train Loss: 0.0074\n",
      "Epoch 210/499\n",
      "----------\n",
      "Epoch210 Batch0 Loss: 0.0066\n",
      "Epoch210 Batch10 Loss: 0.0056\n",
      "Epoch210 Batch20 Loss: 0.0072\n",
      "Epoch210 Batch30 Loss: 0.0070\n",
      "shuffling the dataset\n",
      "Epoch210 Batch40 Loss: 0.0076\n",
      "Epoch210 Batch50 Loss: 0.0064\n",
      "Epoch210 Batch60 Loss: 0.0060\n",
      "Epoch210 Batch70 Loss: 0.0089\n",
      "train Loss: 0.0066\n",
      "Epoch 211/499\n",
      "----------\n",
      "Epoch211 Batch0 Loss: 0.0066\n",
      "Epoch211 Batch10 Loss: 0.0064\n",
      "Epoch211 Batch20 Loss: 0.0051\n",
      "Epoch211 Batch30 Loss: 0.0066\n",
      "shuffling the dataset\n",
      "Epoch211 Batch40 Loss: 0.0073\n",
      "Epoch211 Batch50 Loss: 0.0361\n",
      "Epoch211 Batch60 Loss: 0.0049\n",
      "Epoch211 Batch70 Loss: 0.0084\n",
      "train Loss: 0.0080\n",
      "Epoch 212/499\n",
      "----------\n",
      "Epoch212 Batch0 Loss: 0.0075\n",
      "Epoch212 Batch10 Loss: 0.0064\n",
      "Epoch212 Batch20 Loss: 0.0083\n",
      "Epoch212 Batch30 Loss: 0.0049\n",
      "Epoch212 Batch40 Loss: 0.0061\n",
      "shuffling the dataset\n",
      "Epoch212 Batch50 Loss: 0.0084\n",
      "Epoch212 Batch60 Loss: 0.0043\n",
      "Epoch212 Batch70 Loss: 0.0068\n",
      "train Loss: 0.0081\n",
      "Epoch 213/499\n",
      "----------\n",
      "Epoch213 Batch0 Loss: 0.0059\n",
      "Epoch213 Batch10 Loss: 0.0115\n",
      "Epoch213 Batch20 Loss: 0.0070\n",
      "Epoch213 Batch30 Loss: 0.0084\n",
      "Epoch213 Batch40 Loss: 0.0060\n",
      "shuffling the dataset\n",
      "Epoch213 Batch50 Loss: 0.0084\n",
      "Epoch213 Batch60 Loss: 0.0038\n",
      "Epoch213 Batch70 Loss: 0.0067\n",
      "train Loss: 0.0072\n",
      "Epoch 214/499\n",
      "----------\n",
      "Epoch214 Batch0 Loss: 0.0042\n",
      "Epoch214 Batch10 Loss: 0.0144\n",
      "Epoch214 Batch20 Loss: 0.0059\n",
      "Epoch214 Batch30 Loss: 0.0051\n",
      "Epoch214 Batch40 Loss: 0.0099\n",
      "shuffling the dataset\n",
      "Epoch214 Batch50 Loss: 0.0080\n",
      "Epoch214 Batch60 Loss: 0.0068\n",
      "Epoch214 Batch70 Loss: 0.0063\n",
      "train Loss: 0.0075\n",
      "Epoch 215/499\n",
      "----------\n",
      "Epoch215 Batch0 Loss: 0.0096\n",
      "Epoch215 Batch10 Loss: 0.0059\n",
      "Epoch215 Batch20 Loss: 0.0093\n",
      "Epoch215 Batch30 Loss: 0.0069\n",
      "Epoch215 Batch40 Loss: 0.0059\n",
      "shuffling the dataset\n",
      "Epoch215 Batch50 Loss: 0.0070\n",
      "Epoch215 Batch60 Loss: 0.0110\n",
      "Epoch215 Batch70 Loss: 0.0067\n",
      "train Loss: 0.0081\n",
      "Epoch 216/499\n",
      "----------\n",
      "Epoch216 Batch0 Loss: 0.0038\n",
      "Epoch216 Batch10 Loss: 0.0065\n",
      "Epoch216 Batch20 Loss: 0.0111\n",
      "Epoch216 Batch30 Loss: 0.0049\n",
      "Epoch216 Batch40 Loss: 0.0072\n",
      "shuffling the dataset\n",
      "Epoch216 Batch50 Loss: 0.0047\n",
      "Epoch216 Batch60 Loss: 0.0068\n",
      "Epoch216 Batch70 Loss: 0.0062\n",
      "train Loss: 0.0078\n",
      "Epoch 217/499\n",
      "----------\n",
      "Epoch217 Batch0 Loss: 0.0056\n",
      "Epoch217 Batch10 Loss: 0.0093\n",
      "Epoch217 Batch20 Loss: 0.0088\n",
      "Epoch217 Batch30 Loss: 0.0099\n",
      "Epoch217 Batch40 Loss: 0.0077\n",
      "shuffling the dataset\n",
      "Epoch217 Batch50 Loss: 0.0074\n",
      "Epoch217 Batch60 Loss: 0.0066\n",
      "Epoch217 Batch70 Loss: 0.0070\n",
      "train Loss: 0.0076\n",
      "Epoch 218/499\n",
      "----------\n",
      "Epoch218 Batch0 Loss: 0.0361\n",
      "Epoch218 Batch10 Loss: 0.0075\n",
      "Epoch218 Batch20 Loss: 0.0068\n",
      "Epoch218 Batch30 Loss: 0.0068\n",
      "Epoch218 Batch40 Loss: 0.0077\n",
      "shuffling the dataset\n",
      "Epoch218 Batch50 Loss: 0.0064\n",
      "Epoch218 Batch60 Loss: 0.0066\n",
      "Epoch218 Batch70 Loss: 0.0050\n",
      "train Loss: 0.0073\n",
      "Epoch 219/499\n",
      "----------\n",
      "Epoch219 Batch0 Loss: 0.0041\n",
      "Epoch219 Batch10 Loss: 0.0058\n",
      "Epoch219 Batch20 Loss: 0.0054\n",
      "Epoch219 Batch30 Loss: 0.0041\n",
      "Epoch219 Batch40 Loss: 0.0060\n",
      "shuffling the dataset\n",
      "Epoch219 Batch50 Loss: 0.0085\n",
      "Epoch219 Batch60 Loss: 0.0098\n",
      "Epoch219 Batch70 Loss: 0.0044\n",
      "train Loss: 0.0082\n",
      "Epoch 220/499\n",
      "----------\n",
      "Epoch220 Batch0 Loss: 0.0069\n",
      "Epoch220 Batch10 Loss: 0.0055\n",
      "Epoch220 Batch20 Loss: 0.0105\n",
      "Epoch220 Batch30 Loss: 0.0059\n",
      "Epoch220 Batch40 Loss: 0.0086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling the dataset\n",
      "Epoch220 Batch50 Loss: 0.0036\n",
      "Epoch220 Batch60 Loss: 0.0080\n",
      "Epoch220 Batch70 Loss: 0.0083\n",
      "train Loss: 0.0078\n",
      "Epoch 221/499\n",
      "----------\n",
      "Epoch221 Batch0 Loss: 0.0051\n",
      "Epoch221 Batch10 Loss: 0.0062\n",
      "Epoch221 Batch20 Loss: 0.0061\n",
      "Epoch221 Batch30 Loss: 0.0063\n",
      "Epoch221 Batch40 Loss: 0.0060\n",
      "shuffling the dataset\n",
      "Epoch221 Batch50 Loss: 0.0069\n",
      "Epoch221 Batch60 Loss: 0.0079\n",
      "Epoch221 Batch70 Loss: 0.0067\n",
      "train Loss: 0.0070\n",
      "Epoch 222/499\n",
      "----------\n",
      "Epoch222 Batch0 Loss: 0.0053\n",
      "Epoch222 Batch10 Loss: 0.0090\n",
      "Epoch222 Batch20 Loss: 0.0065\n",
      "Epoch222 Batch30 Loss: 0.0057\n",
      "Epoch222 Batch40 Loss: 0.0043\n",
      "shuffling the dataset\n",
      "Epoch222 Batch50 Loss: 0.0142\n",
      "Epoch222 Batch60 Loss: 0.0043\n",
      "Epoch222 Batch70 Loss: 0.0085\n",
      "train Loss: 0.0081\n",
      "Epoch 223/499\n",
      "----------\n",
      "Epoch223 Batch0 Loss: 0.0050\n",
      "Epoch223 Batch10 Loss: 0.0060\n",
      "Epoch223 Batch20 Loss: 0.0088\n",
      "Epoch223 Batch30 Loss: 0.0108\n",
      "Epoch223 Batch40 Loss: 0.0145\n",
      "Epoch223 Batch50 Loss: 0.0033\n",
      "shuffling the dataset\n",
      "Epoch223 Batch60 Loss: 0.0168\n",
      "Epoch223 Batch70 Loss: 0.0093\n",
      "train Loss: 0.0075\n",
      "Epoch 224/499\n",
      "----------\n",
      "Epoch224 Batch0 Loss: 0.0076\n",
      "Epoch224 Batch10 Loss: 0.0080\n",
      "Epoch224 Batch20 Loss: 0.0061\n",
      "Epoch224 Batch30 Loss: 0.0068\n",
      "Epoch224 Batch40 Loss: 0.0109\n",
      "Epoch224 Batch50 Loss: 0.0091\n",
      "shuffling the dataset\n",
      "Epoch224 Batch60 Loss: 0.0095\n",
      "Epoch224 Batch70 Loss: 0.0062\n",
      "train Loss: 0.0086\n",
      "Epoch 225/499\n",
      "----------\n",
      "Epoch225 Batch0 Loss: 0.0072\n",
      "Epoch225 Batch10 Loss: 0.0093\n",
      "Epoch225 Batch20 Loss: 0.0069\n",
      "Epoch225 Batch30 Loss: 0.0055\n",
      "Epoch225 Batch40 Loss: 0.0094\n",
      "Epoch225 Batch50 Loss: 0.0087\n",
      "shuffling the dataset\n",
      "Epoch225 Batch60 Loss: 0.0056\n",
      "Epoch225 Batch70 Loss: 0.0072\n",
      "train Loss: 0.0080\n",
      "Epoch 226/499\n",
      "----------\n",
      "Epoch226 Batch0 Loss: 0.0081\n",
      "Epoch226 Batch10 Loss: 0.0063\n",
      "Epoch226 Batch20 Loss: 0.0040\n",
      "Epoch226 Batch30 Loss: 0.0094\n",
      "Epoch226 Batch40 Loss: 0.0076\n",
      "Epoch226 Batch50 Loss: 0.0069\n",
      "shuffling the dataset\n",
      "Epoch226 Batch60 Loss: 0.0100\n",
      "Epoch226 Batch70 Loss: 0.0048\n",
      "train Loss: 0.0077\n",
      "Epoch 227/499\n",
      "----------\n",
      "Epoch227 Batch0 Loss: 0.0085\n",
      "Epoch227 Batch10 Loss: 0.0072\n",
      "Epoch227 Batch20 Loss: 0.0071\n",
      "Epoch227 Batch30 Loss: 0.0079\n"
     ]
    }
   ],
   "source": [
    "###################### Train Model #############################\n",
    "# Calculate total iter_num\n",
    "total_iter_num = 0\n",
    "\n",
    "for epoch in range(opt.num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, opt.num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Iterate over data.\n",
    "    for i in range(int(dataloader.size()/opt.batchSize)):\n",
    "        total_iter_num += 1\n",
    "        \n",
    "        # Get data\n",
    "        inputs, labels =  dataloader.get_batch()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #ToDo: get labels into correct format\n",
    "        \n",
    "        ## Create Graph ##\n",
    "        if graph_created == False:\n",
    "            graph_created = True\n",
    "            writer.add_graph(model, inputs, verbose=False)\n",
    "        \n",
    "        # Forward + Backward Prop\n",
    "        optimizer.zero_grad()\n",
    "        torch.set_grad_enabled(True)\n",
    "        normal_vectors = model(inputs)\n",
    "        normal_vectors_norm = nn.functional.normalize(normal_vectors, p=2, dim=1)\n",
    "        \n",
    "        loss = loss_fn(normal_vectors_norm, labels, reduction='elementwise_mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar('loss', loss.item(), total_iter_num)\n",
    "        \n",
    "        if (i % 10 == 0):\n",
    "            print('Epoch{} Batch{} Loss: {:.4f}'.format(epoch, i, loss.item()))\n",
    "\n",
    "    epoch_loss = running_loss / (dataloader.size()/opt.batchSize)\n",
    "    writer.add_scalar('epoch_loss', epoch_loss, epoch)\n",
    "    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    directory = opt.logs_path+'/checkpoints/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    if (epoch % 5 == 0):\n",
    "        filename = opt.logs_path + '/checkpoints/checkpoint-epoch_{}.pth'.format(epoch,i)\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        \n",
    "\n",
    "# Save final Checkpoint\n",
    "filename = opt.logs_path + '/checkpoints/checkpoint.pth'\n",
    "torch.save(model.state_dict(), filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
