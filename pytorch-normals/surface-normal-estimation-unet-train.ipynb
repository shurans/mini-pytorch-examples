{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from data_loader import Dataset,Options\n",
    "import models.unet_normals as unet\n",
    "from tensorboardX import SummaryWriter\n",
    "# import OpenEXR, Imath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Options\n",
    "Set the various parameters:\n",
    "- dataroot: The folder where the training data is stored\n",
    "- file_list: List of filenames of images for training\n",
    "- batchSize: Batch size for model\n",
    "- shuffle: If true, will shuffle the dataset\n",
    "- phase: If 'train', then it's in training mode.\n",
    "- num_epochs: Number of epochs to train the model for\n",
    "- imsize: Dimensions of the image (square)\n",
    "- num_classes: Num of classes in the output\n",
    "- gpu: Which GPU device to use\n",
    "- logs_path: The path where the log files (tensorboard) will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPT():\n",
    "    def __init__(self):\n",
    "        self.dataroot = './data/'\n",
    "        self.file_list = './data/datalist'\n",
    "        self.batchSize = 32\n",
    "        self.shuffle = True\n",
    "        self.phase = 'train'\n",
    "        self.num_epochs = 500\n",
    "        self.imsize = 224\n",
    "        self.num_classes = int(3)\n",
    "        self.gpu = '0'\n",
    "        self.logs_path = 'logs/exp11-3'\n",
    "        self.use_pretrained = True\n",
    "\n",
    "opt = OPT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling the dataset\n"
     ]
    }
   ],
   "source": [
    "###################### Options #############################\n",
    "phase = opt.phase\n",
    "device = torch.device(\"cuda:\"+ opt.gpu if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###################### TensorBoardX #############################\n",
    "if os.path.exists(opt.logs_path):\n",
    "    raise Exception('The folder \\\"{}\\\" already exists! Define a new log path or delete old contents.'.format(opt.logs_path))\n",
    "    \n",
    "writer = SummaryWriter(opt.logs_path, comment='create-graph')\n",
    "graph_created = False\n",
    "\n",
    "###################### DataLoader #############################\n",
    "dataloader = Dataset(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "We use a UNet model. The last few layers of this model are modified to return a 3 channel image, containing the x,y,z values of surface normal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### ModelBuilder #############################\n",
    "model = unet.Unet(num_classes=opt.num_classes)\n",
    "\n",
    "# Load weights from checkpoint\n",
    "if (opt.use_pretrained == True):\n",
    "    checkpoint_path = 'logs/exp11-2/checkpoints/checkpoint-epoch_1300.pth'\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "###################### Setup Optimazation #############################\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "###################### Loss fuction #############################\n",
    "'''\n",
    "@input: The 2 vectors whose cosine loss is to be calculated\n",
    "The dimensions of the matrices are expected to be (batchSize, 3, imsize, imsize). \n",
    "\n",
    "@return: \n",
    "elementwise_mean: will return the sum of all losses divided by num of elements\n",
    "none: The loss will be calculated to be of size (batchSize, imsize, imsize) containing cosine loss of each pixel\n",
    "'''\n",
    "def loss_fn(input_vec, target_vec, reduction='elementwise_mean'):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    loss_val = 1.0 - cos(input_vec, target_vec)\n",
    "    if (reduction=='elementwise_mean'):\n",
    "        return torch.mean(loss_val)\n",
    "    elif (reduction=='none'):\n",
    "        return loss_val\n",
    "    else:\n",
    "        raise Exception('Warning! The reduction is invalid. Please use \\'elementwise_mean\\' or \\'none\\''.format())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1305/1999\n",
      "----------\n",
      "Epoch1305 Batch0 Loss: 0.1084\n",
      "Epoch1305 Batch10 Loss: 27.5719\n",
      "Epoch1305 Batch20 Loss: 17.2170\n",
      "Epoch1305 Batch30 Loss: 20.7190\n",
      "Epoch1305 Batch40 Loss: 17.4696\n",
      "Epoch1305 Batch50 Loss: 23.6811\n",
      "Epoch1305 Batch60 Loss: 24.2594\n",
      "Epoch1305 Batch70 Loss: 19.0002\n",
      "shuffling the dataset\n",
      "train Loss: 22.7854\n",
      "Epoch 1306/1999\n",
      "----------\n",
      "Epoch1306 Batch0 Loss: 19.3001\n",
      "Epoch1306 Batch10 Loss: 23.9740\n",
      "Epoch1306 Batch20 Loss: 22.1439\n",
      "Epoch1306 Batch30 Loss: 25.1439\n",
      "Epoch1306 Batch40 Loss: 22.3646\n",
      "Epoch1306 Batch50 Loss: 23.6209\n",
      "Epoch1306 Batch60 Loss: 20.5597\n",
      "Epoch1306 Batch70 Loss: 22.4340\n",
      "shuffling the dataset\n",
      "train Loss: 22.3486\n",
      "Epoch 1307/1999\n",
      "----------\n",
      "Epoch1307 Batch0 Loss: 22.2476\n",
      "Epoch1307 Batch10 Loss: 22.1259\n",
      "Epoch1307 Batch20 Loss: 20.6062\n",
      "Epoch1307 Batch30 Loss: 19.8032\n",
      "Epoch1307 Batch40 Loss: 21.0468\n",
      "Epoch1307 Batch50 Loss: 19.9323\n",
      "Epoch1307 Batch60 Loss: 21.5210\n",
      "Epoch1307 Batch70 Loss: 22.5190\n",
      "shuffling the dataset\n",
      "train Loss: 22.1677\n",
      "Epoch 1308/1999\n",
      "----------\n",
      "Epoch1308 Batch0 Loss: 19.9423\n",
      "Epoch1308 Batch10 Loss: 24.1838\n",
      "Epoch1308 Batch20 Loss: 19.8751\n",
      "Epoch1308 Batch30 Loss: 19.0340\n",
      "Epoch1308 Batch40 Loss: 19.4223\n",
      "Epoch1308 Batch50 Loss: 20.9213\n",
      "Epoch1308 Batch60 Loss: 17.5025\n",
      "Epoch1308 Batch70 Loss: 24.4450\n",
      "shuffling the dataset\n",
      "train Loss: 21.7215\n",
      "Epoch 1309/1999\n",
      "----------\n",
      "Epoch1309 Batch0 Loss: 20.9929\n",
      "Epoch1309 Batch10 Loss: 19.2904\n",
      "Epoch1309 Batch20 Loss: 18.2881\n",
      "Epoch1309 Batch30 Loss: 23.4520\n",
      "Epoch1309 Batch40 Loss: 18.2390\n",
      "Epoch1309 Batch50 Loss: 20.3065\n",
      "Epoch1309 Batch60 Loss: 24.1080\n",
      "Epoch1309 Batch70 Loss: 24.3839\n",
      "shuffling the dataset\n",
      "train Loss: 21.1021\n",
      "Epoch 1310/1999\n",
      "----------\n",
      "Epoch1310 Batch0 Loss: 15.6554\n",
      "Epoch1310 Batch10 Loss: 21.7987\n",
      "Epoch1310 Batch20 Loss: 20.7444\n",
      "Epoch1310 Batch30 Loss: 23.6471\n",
      "Epoch1310 Batch40 Loss: 23.1698\n",
      "Epoch1310 Batch50 Loss: 19.9545\n",
      "Epoch1310 Batch60 Loss: 19.8808\n",
      "Epoch1310 Batch70 Loss: 21.8287\n",
      "shuffling the dataset\n",
      "train Loss: 20.8937\n",
      "Epoch 1311/1999\n",
      "----------\n",
      "Epoch1311 Batch0 Loss: 21.7513\n",
      "Epoch1311 Batch10 Loss: 20.1245\n",
      "Epoch1311 Batch20 Loss: 17.3303\n",
      "Epoch1311 Batch30 Loss: 20.1447\n",
      "Epoch1311 Batch40 Loss: 18.1907\n",
      "Epoch1311 Batch50 Loss: 18.5043\n",
      "Epoch1311 Batch60 Loss: 22.6985\n",
      "Epoch1311 Batch70 Loss: 23.4894\n",
      "shuffling the dataset\n",
      "train Loss: 20.3710\n",
      "Epoch 1312/1999\n",
      "----------\n",
      "Epoch1312 Batch0 Loss: 22.2698\n",
      "Epoch1312 Batch10 Loss: 23.4330\n",
      "Epoch1312 Batch20 Loss: 20.7412\n",
      "Epoch1312 Batch30 Loss: 17.8518\n",
      "Epoch1312 Batch40 Loss: 17.0932\n",
      "Epoch1312 Batch50 Loss: 16.6278\n",
      "Epoch1312 Batch60 Loss: 19.2068\n",
      "Epoch1312 Batch70 Loss: 20.4137\n",
      "shuffling the dataset\n",
      "train Loss: 19.9034\n",
      "Epoch 1313/1999\n",
      "----------\n",
      "Epoch1313 Batch0 Loss: 20.2663\n",
      "Epoch1313 Batch10 Loss: 20.1104\n",
      "Epoch1313 Batch20 Loss: 21.3130\n",
      "Epoch1313 Batch30 Loss: 21.1466\n",
      "Epoch1313 Batch40 Loss: 19.6811\n",
      "Epoch1313 Batch50 Loss: 24.4796\n",
      "Epoch1313 Batch60 Loss: 16.8505\n",
      "Epoch1313 Batch70 Loss: 20.2124\n",
      "shuffling the dataset\n",
      "train Loss: 20.1775\n",
      "Epoch 1314/1999\n",
      "----------\n",
      "Epoch1314 Batch0 Loss: 17.6398\n",
      "Epoch1314 Batch10 Loss: 16.2468\n",
      "Epoch1314 Batch20 Loss: 19.3064\n",
      "Epoch1314 Batch30 Loss: 20.4450\n",
      "Epoch1314 Batch40 Loss: 21.6502\n",
      "Epoch1314 Batch50 Loss: 17.6935\n",
      "Epoch1314 Batch60 Loss: 18.8460\n",
      "Epoch1314 Batch70 Loss: 19.7529\n",
      "shuffling the dataset\n",
      "train Loss: 19.1924\n",
      "Epoch 1315/1999\n",
      "----------\n",
      "Epoch1315 Batch0 Loss: 19.2028\n",
      "Epoch1315 Batch10 Loss: 16.9625\n",
      "Epoch1315 Batch20 Loss: 19.1497\n",
      "Epoch1315 Batch30 Loss: 20.1975\n",
      "Epoch1315 Batch40 Loss: 18.0165\n",
      "Epoch1315 Batch50 Loss: 19.5557\n",
      "Epoch1315 Batch60 Loss: 20.6539\n",
      "Epoch1315 Batch70 Loss: 25.5562\n",
      "shuffling the dataset\n",
      "train Loss: 18.9123\n",
      "Epoch 1316/1999\n",
      "----------\n",
      "Epoch1316 Batch0 Loss: 20.6195\n",
      "Epoch1316 Batch10 Loss: 16.0286\n",
      "Epoch1316 Batch20 Loss: 23.6159\n",
      "Epoch1316 Batch30 Loss: 20.1596\n",
      "Epoch1316 Batch40 Loss: 20.3002\n",
      "Epoch1316 Batch50 Loss: 17.4189\n",
      "Epoch1316 Batch60 Loss: 22.1018\n",
      "Epoch1316 Batch70 Loss: 15.2591\n",
      "shuffling the dataset\n",
      "train Loss: 18.6616\n",
      "Epoch 1317/1999\n",
      "----------\n",
      "Epoch1317 Batch0 Loss: 19.2785\n",
      "Epoch1317 Batch10 Loss: 15.9497\n",
      "Epoch1317 Batch20 Loss: 21.6734\n",
      "Epoch1317 Batch30 Loss: 17.0586\n",
      "Epoch1317 Batch40 Loss: 14.0715\n",
      "Epoch1317 Batch50 Loss: 18.5498\n",
      "Epoch1317 Batch60 Loss: 14.7634\n",
      "Epoch1317 Batch70 Loss: 16.3097\n",
      "shuffling the dataset\n",
      "train Loss: 18.1995\n",
      "Epoch 1318/1999\n",
      "----------\n",
      "Epoch1318 Batch0 Loss: 15.6631\n",
      "Epoch1318 Batch10 Loss: 12.9599\n",
      "Epoch1318 Batch20 Loss: 18.4398\n",
      "Epoch1318 Batch30 Loss: 15.8546\n",
      "Epoch1318 Batch40 Loss: 17.7023\n",
      "Epoch1318 Batch50 Loss: 18.7428\n",
      "Epoch1318 Batch60 Loss: 14.3092\n",
      "Epoch1318 Batch70 Loss: 17.5589\n",
      "shuffling the dataset\n",
      "train Loss: 17.4059\n",
      "Epoch 1319/1999\n",
      "----------\n",
      "Epoch1319 Batch0 Loss: 17.5644\n",
      "Epoch1319 Batch10 Loss: 17.5455\n",
      "Epoch1319 Batch20 Loss: 17.9090\n",
      "Epoch1319 Batch30 Loss: 17.7780\n",
      "Epoch1319 Batch40 Loss: 17.7608\n",
      "Epoch1319 Batch50 Loss: 18.9671\n",
      "Epoch1319 Batch60 Loss: 18.6823\n",
      "Epoch1319 Batch70 Loss: 16.0328\n",
      "shuffling the dataset\n",
      "train Loss: 16.9341\n",
      "Epoch 1320/1999\n",
      "----------\n",
      "Epoch1320 Batch0 Loss: 16.6240\n",
      "Epoch1320 Batch10 Loss: 13.5992\n",
      "Epoch1320 Batch20 Loss: 16.4710\n",
      "Epoch1320 Batch30 Loss: 16.6783\n",
      "Epoch1320 Batch40 Loss: 13.4920\n",
      "Epoch1320 Batch50 Loss: 16.4400\n",
      "Epoch1320 Batch60 Loss: 16.7793\n",
      "Epoch1320 Batch70 Loss: 17.2249\n",
      "shuffling the dataset\n",
      "train Loss: 16.6061\n",
      "Epoch 1321/1999\n",
      "----------\n",
      "Epoch1321 Batch0 Loss: 19.6069\n",
      "Epoch1321 Batch10 Loss: 14.5444\n",
      "Epoch1321 Batch20 Loss: 14.3082\n",
      "Epoch1321 Batch30 Loss: 16.4689\n",
      "Epoch1321 Batch40 Loss: 15.2786\n",
      "Epoch1321 Batch50 Loss: 17.1810\n",
      "Epoch1321 Batch60 Loss: 18.1077\n",
      "Epoch1321 Batch70 Loss: 13.1763\n",
      "shuffling the dataset\n",
      "train Loss: 16.0940\n",
      "Epoch 1322/1999\n",
      "----------\n",
      "Epoch1322 Batch0 Loss: 13.0636\n",
      "Epoch1322 Batch10 Loss: 21.1797\n",
      "Epoch1322 Batch20 Loss: 15.7677\n",
      "Epoch1322 Batch30 Loss: 13.0615\n",
      "Epoch1322 Batch40 Loss: 14.6678\n",
      "Epoch1322 Batch50 Loss: 12.2154\n",
      "Epoch1322 Batch60 Loss: 15.3648\n",
      "Epoch1322 Batch70 Loss: 16.0696\n",
      "shuffling the dataset\n",
      "train Loss: 15.6659\n",
      "Epoch 1323/1999\n",
      "----------\n",
      "Epoch1323 Batch0 Loss: 12.2912\n",
      "Epoch1323 Batch10 Loss: 13.9471\n",
      "Epoch1323 Batch20 Loss: 13.4040\n",
      "Epoch1323 Batch30 Loss: 15.0264\n",
      "Epoch1323 Batch40 Loss: 15.0207\n",
      "Epoch1323 Batch50 Loss: 15.4786\n",
      "Epoch1323 Batch60 Loss: 13.2754\n",
      "Epoch1323 Batch70 Loss: 13.6079\n",
      "shuffling the dataset\n",
      "train Loss: 15.1332\n",
      "Epoch 1324/1999\n",
      "----------\n",
      "Epoch1324 Batch0 Loss: 15.2138\n",
      "Epoch1324 Batch10 Loss: 14.6864\n",
      "Epoch1324 Batch20 Loss: 14.4032\n",
      "Epoch1324 Batch30 Loss: 13.7081\n",
      "Epoch1324 Batch40 Loss: 11.3040\n",
      "Epoch1324 Batch50 Loss: 11.8943\n",
      "Epoch1324 Batch60 Loss: 15.0391\n",
      "Epoch1324 Batch70 Loss: 13.7439\n",
      "shuffling the dataset\n",
      "train Loss: 14.9245\n",
      "Epoch 1325/1999\n",
      "----------\n",
      "Epoch1325 Batch0 Loss: 16.5800\n",
      "Epoch1325 Batch10 Loss: 18.4685\n",
      "Epoch1325 Batch20 Loss: 16.5004\n",
      "Epoch1325 Batch30 Loss: 14.7185\n",
      "Epoch1325 Batch40 Loss: 11.8721\n",
      "Epoch1325 Batch50 Loss: 14.8491\n",
      "Epoch1325 Batch60 Loss: 16.6277\n",
      "Epoch1325 Batch70 Loss: 12.5152\n",
      "shuffling the dataset\n",
      "train Loss: 14.2801\n",
      "Epoch 1326/1999\n",
      "----------\n",
      "Epoch1326 Batch0 Loss: 15.2709\n",
      "Epoch1326 Batch10 Loss: 17.4055\n",
      "Epoch1326 Batch20 Loss: 13.1837\n",
      "Epoch1326 Batch30 Loss: 14.9151\n",
      "Epoch1326 Batch40 Loss: 15.4960\n",
      "Epoch1326 Batch50 Loss: 15.3107\n",
      "Epoch1326 Batch60 Loss: 18.7217\n",
      "Epoch1326 Batch70 Loss: 11.2976\n",
      "shuffling the dataset\n",
      "train Loss: 13.8898\n",
      "Epoch 1327/1999\n",
      "----------\n",
      "Epoch1327 Batch0 Loss: 12.8923\n",
      "Epoch1327 Batch10 Loss: 12.1934\n",
      "Epoch1327 Batch20 Loss: 11.0208\n",
      "Epoch1327 Batch30 Loss: 11.9635\n",
      "Epoch1327 Batch40 Loss: 12.6433\n",
      "Epoch1327 Batch50 Loss: 12.0282\n",
      "Epoch1327 Batch60 Loss: 14.9336\n",
      "Epoch1327 Batch70 Loss: 16.2645\n",
      "shuffling the dataset\n",
      "train Loss: 13.5917\n",
      "Epoch 1328/1999\n",
      "----------\n",
      "Epoch1328 Batch0 Loss: 12.9510\n",
      "Epoch1328 Batch10 Loss: 13.7652\n",
      "Epoch1328 Batch20 Loss: 13.0621\n",
      "Epoch1328 Batch30 Loss: 11.9249\n",
      "Epoch1328 Batch40 Loss: 13.6258\n",
      "Epoch1328 Batch50 Loss: 14.6215\n",
      "Epoch1328 Batch60 Loss: 14.0934\n",
      "Epoch1328 Batch70 Loss: 13.5201\n",
      "shuffling the dataset\n",
      "train Loss: 13.3605\n",
      "Epoch 1329/1999\n",
      "----------\n",
      "Epoch1329 Batch0 Loss: 12.4012\n",
      "Epoch1329 Batch10 Loss: 18.0296\n",
      "Epoch1329 Batch20 Loss: 13.4913\n",
      "Epoch1329 Batch30 Loss: 14.5115\n",
      "Epoch1329 Batch40 Loss: 10.4776\n",
      "Epoch1329 Batch50 Loss: 16.8190\n",
      "Epoch1329 Batch60 Loss: 11.5027\n",
      "Epoch1329 Batch70 Loss: 13.3221\n",
      "shuffling the dataset\n",
      "train Loss: 12.6573\n",
      "Epoch 1330/1999\n",
      "----------\n",
      "Epoch1330 Batch0 Loss: 13.2377\n",
      "Epoch1330 Batch10 Loss: 11.0708\n",
      "Epoch1330 Batch20 Loss: 11.8718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1330 Batch30 Loss: 10.3037\n",
      "Epoch1330 Batch40 Loss: 9.1361\n",
      "Epoch1330 Batch50 Loss: 14.2845\n",
      "Epoch1330 Batch60 Loss: 14.4471\n",
      "Epoch1330 Batch70 Loss: 12.3994\n",
      "shuffling the dataset\n",
      "train Loss: 11.9802\n",
      "Epoch 1331/1999\n",
      "----------\n",
      "Epoch1331 Batch0 Loss: 11.7412\n",
      "Epoch1331 Batch10 Loss: 12.6361\n",
      "Epoch1331 Batch20 Loss: 9.5441\n",
      "Epoch1331 Batch30 Loss: 9.4450\n",
      "Epoch1331 Batch40 Loss: 10.5854\n",
      "Epoch1331 Batch50 Loss: 14.6807\n",
      "Epoch1331 Batch60 Loss: 11.4438\n",
      "Epoch1331 Batch70 Loss: 10.3526\n",
      "shuffling the dataset\n",
      "train Loss: 11.5274\n",
      "Epoch 1332/1999\n",
      "----------\n",
      "Epoch1332 Batch0 Loss: 10.5276\n",
      "Epoch1332 Batch10 Loss: 11.6453\n",
      "Epoch1332 Batch20 Loss: 9.9441\n",
      "Epoch1332 Batch30 Loss: 10.0518\n",
      "Epoch1332 Batch40 Loss: 10.9913\n",
      "Epoch1332 Batch50 Loss: 11.8808\n",
      "Epoch1332 Batch60 Loss: 11.2748\n",
      "Epoch1332 Batch70 Loss: 10.6298\n",
      "shuffling the dataset\n",
      "train Loss: 10.9759\n",
      "Epoch 1333/1999\n",
      "----------\n",
      "Epoch1333 Batch0 Loss: 10.6528\n",
      "Epoch1333 Batch10 Loss: 10.4520\n",
      "Epoch1333 Batch20 Loss: 11.2228\n",
      "Epoch1333 Batch30 Loss: 10.8621\n",
      "Epoch1333 Batch40 Loss: 10.1693\n",
      "Epoch1333 Batch50 Loss: 13.2189\n",
      "Epoch1333 Batch60 Loss: 11.9585\n",
      "Epoch1333 Batch70 Loss: 7.2998\n",
      "shuffling the dataset\n",
      "train Loss: 10.7304\n",
      "Epoch 1334/1999\n",
      "----------\n",
      "Epoch1334 Batch0 Loss: 13.0501\n",
      "Epoch1334 Batch10 Loss: 10.6011\n",
      "Epoch1334 Batch20 Loss: 9.7757\n",
      "Epoch1334 Batch30 Loss: 10.2040\n",
      "Epoch1334 Batch40 Loss: 9.4606\n",
      "Epoch1334 Batch50 Loss: 10.5357\n",
      "Epoch1334 Batch60 Loss: 10.4632\n",
      "Epoch1334 Batch70 Loss: 10.6088\n",
      "shuffling the dataset\n",
      "train Loss: 10.1946\n",
      "Epoch 1335/1999\n",
      "----------\n",
      "Epoch1335 Batch0 Loss: 9.4872\n",
      "Epoch1335 Batch10 Loss: 9.4203\n",
      "Epoch1335 Batch20 Loss: 8.7124\n",
      "Epoch1335 Batch30 Loss: 7.5432\n",
      "Epoch1335 Batch40 Loss: 10.0865\n",
      "Epoch1335 Batch50 Loss: 10.1499\n",
      "Epoch1335 Batch60 Loss: 8.7652\n",
      "Epoch1335 Batch70 Loss: 10.5210\n",
      "shuffling the dataset\n",
      "train Loss: 9.5377\n",
      "Epoch 1336/1999\n",
      "----------\n",
      "Epoch1336 Batch0 Loss: 10.1249\n",
      "Epoch1336 Batch10 Loss: 8.5898\n",
      "Epoch1336 Batch20 Loss: 7.3428\n",
      "Epoch1336 Batch30 Loss: 8.3012\n",
      "Epoch1336 Batch40 Loss: 8.5335\n",
      "Epoch1336 Batch50 Loss: 8.2193\n",
      "Epoch1336 Batch60 Loss: 10.7891\n",
      "Epoch1336 Batch70 Loss: 8.8184\n",
      "shuffling the dataset\n",
      "train Loss: 8.9125\n",
      "Epoch 1337/1999\n",
      "----------\n",
      "Epoch1337 Batch0 Loss: 7.2907\n",
      "Epoch1337 Batch10 Loss: 8.6075\n",
      "Epoch1337 Batch20 Loss: 8.3013\n",
      "Epoch1337 Batch30 Loss: 9.0083\n",
      "Epoch1337 Batch40 Loss: 8.9968\n",
      "Epoch1337 Batch50 Loss: 8.1085\n",
      "Epoch1337 Batch60 Loss: 7.3206\n",
      "Epoch1337 Batch70 Loss: 7.2038\n",
      "shuffling the dataset\n",
      "train Loss: 8.7020\n",
      "Epoch 1338/1999\n",
      "----------\n",
      "Epoch1338 Batch0 Loss: 9.7166\n",
      "Epoch1338 Batch10 Loss: 7.7542\n",
      "Epoch1338 Batch20 Loss: 8.8113\n",
      "Epoch1338 Batch30 Loss: 7.6110\n",
      "Epoch1338 Batch40 Loss: 7.9682\n",
      "Epoch1338 Batch50 Loss: 7.1210\n",
      "Epoch1338 Batch60 Loss: 6.7954\n",
      "Epoch1338 Batch70 Loss: 7.0953\n",
      "shuffling the dataset\n",
      "train Loss: 7.9558\n",
      "Epoch 1339/1999\n",
      "----------\n",
      "Epoch1339 Batch0 Loss: 7.4355\n",
      "Epoch1339 Batch10 Loss: 6.0399\n",
      "Epoch1339 Batch20 Loss: 5.6598\n",
      "Epoch1339 Batch30 Loss: 7.0413\n",
      "Epoch1339 Batch40 Loss: 8.8380\n",
      "Epoch1339 Batch50 Loss: 7.7574\n",
      "Epoch1339 Batch60 Loss: 8.9645\n",
      "Epoch1339 Batch70 Loss: 7.2829\n",
      "shuffling the dataset\n",
      "train Loss: 7.4209\n",
      "Epoch 1340/1999\n",
      "----------\n",
      "Epoch1340 Batch0 Loss: 8.0311\n",
      "Epoch1340 Batch10 Loss: 6.5569\n",
      "Epoch1340 Batch20 Loss: 6.6004\n",
      "Epoch1340 Batch30 Loss: 7.1908\n",
      "Epoch1340 Batch40 Loss: 5.2583\n",
      "Epoch1340 Batch50 Loss: 6.5944\n",
      "Epoch1340 Batch60 Loss: 6.6593\n",
      "Epoch1340 Batch70 Loss: 6.9250\n",
      "shuffling the dataset\n",
      "train Loss: 6.9327\n",
      "Epoch 1341/1999\n",
      "----------\n",
      "Epoch1341 Batch0 Loss: 6.0242\n",
      "Epoch1341 Batch10 Loss: 5.8736\n",
      "Epoch1341 Batch20 Loss: 5.2288\n",
      "Epoch1341 Batch30 Loss: 6.4393\n",
      "Epoch1341 Batch40 Loss: 7.2825\n",
      "Epoch1341 Batch50 Loss: 5.0649\n",
      "Epoch1341 Batch60 Loss: 6.1942\n",
      "Epoch1341 Batch70 Loss: 5.5597\n",
      "shuffling the dataset\n",
      "train Loss: 6.5335\n",
      "Epoch 1342/1999\n",
      "----------\n",
      "Epoch1342 Batch0 Loss: 6.2304\n",
      "Epoch1342 Batch10 Loss: 5.0106\n",
      "Epoch1342 Batch20 Loss: 7.2944\n",
      "Epoch1342 Batch30 Loss: 5.1417\n",
      "Epoch1342 Batch40 Loss: 5.9263\n",
      "Epoch1342 Batch50 Loss: 5.9367\n",
      "Epoch1342 Batch60 Loss: 5.5146\n",
      "Epoch1342 Batch70 Loss: 7.5576\n",
      "shuffling the dataset\n",
      "train Loss: 6.1422\n",
      "Epoch 1343/1999\n",
      "----------\n",
      "Epoch1343 Batch0 Loss: 5.9343\n",
      "Epoch1343 Batch10 Loss: 5.2226\n",
      "Epoch1343 Batch20 Loss: 6.4699\n",
      "Epoch1343 Batch30 Loss: 6.5301\n",
      "Epoch1343 Batch40 Loss: 6.0628\n",
      "Epoch1343 Batch50 Loss: 5.2218\n",
      "Epoch1343 Batch60 Loss: 5.0861\n",
      "Epoch1343 Batch70 Loss: 6.0873\n",
      "shuffling the dataset\n",
      "train Loss: 5.8018\n",
      "Epoch 1344/1999\n",
      "----------\n",
      "Epoch1344 Batch0 Loss: 5.3378\n",
      "Epoch1344 Batch10 Loss: 5.7696\n",
      "Epoch1344 Batch20 Loss: 5.9222\n",
      "Epoch1344 Batch30 Loss: 5.0952\n",
      "Epoch1344 Batch40 Loss: 4.6053\n",
      "Epoch1344 Batch50 Loss: 8.2228\n",
      "Epoch1344 Batch60 Loss: 6.2904\n",
      "Epoch1344 Batch70 Loss: 4.9930\n",
      "shuffling the dataset\n",
      "train Loss: 5.4055\n",
      "Epoch 1345/1999\n",
      "----------\n",
      "Epoch1345 Batch0 Loss: 4.8691\n",
      "Epoch1345 Batch10 Loss: 5.1315\n",
      "Epoch1345 Batch20 Loss: 5.3727\n",
      "Epoch1345 Batch30 Loss: 4.6797\n",
      "Epoch1345 Batch40 Loss: 4.4523\n",
      "Epoch1345 Batch50 Loss: 5.1195\n",
      "Epoch1345 Batch60 Loss: 5.7993\n",
      "Epoch1345 Batch70 Loss: 4.4931\n",
      "shuffling the dataset\n",
      "train Loss: 5.1093\n",
      "Epoch 1346/1999\n",
      "----------\n",
      "Epoch1346 Batch0 Loss: 5.4488\n",
      "Epoch1346 Batch10 Loss: 4.0593\n",
      "Epoch1346 Batch20 Loss: 5.2644\n",
      "Epoch1346 Batch30 Loss: 4.9357\n",
      "Epoch1346 Batch40 Loss: 5.0296\n",
      "Epoch1346 Batch50 Loss: 4.5764\n",
      "Epoch1346 Batch60 Loss: 4.4477\n",
      "Epoch1346 Batch70 Loss: 4.2240\n",
      "shuffling the dataset\n",
      "train Loss: 4.8056\n",
      "Epoch 1347/1999\n",
      "----------\n",
      "Epoch1347 Batch0 Loss: 3.9848\n",
      "Epoch1347 Batch10 Loss: 4.7775\n",
      "Epoch1347 Batch20 Loss: 4.7109\n",
      "Epoch1347 Batch30 Loss: 5.1287\n",
      "Epoch1347 Batch40 Loss: 4.9048\n",
      "Epoch1347 Batch50 Loss: 3.0633\n",
      "Epoch1347 Batch60 Loss: 3.6338\n",
      "Epoch1347 Batch70 Loss: 4.0908\n",
      "shuffling the dataset\n",
      "train Loss: 4.5939\n",
      "Epoch 1348/1999\n",
      "----------\n",
      "Epoch1348 Batch0 Loss: 3.6241\n",
      "Epoch1348 Batch10 Loss: 4.9581\n",
      "Epoch1348 Batch20 Loss: 3.2929\n",
      "Epoch1348 Batch30 Loss: 4.6711\n",
      "Epoch1348 Batch40 Loss: 3.2835\n",
      "Epoch1348 Batch50 Loss: 4.3848\n",
      "Epoch1348 Batch60 Loss: 4.8705\n",
      "Epoch1348 Batch70 Loss: 4.9229\n",
      "shuffling the dataset\n",
      "train Loss: 4.3895\n",
      "Epoch 1349/1999\n",
      "----------\n",
      "Epoch1349 Batch0 Loss: 2.8167\n",
      "Epoch1349 Batch10 Loss: 4.3318\n",
      "Epoch1349 Batch20 Loss: 3.9848\n",
      "Epoch1349 Batch30 Loss: 3.7101\n",
      "Epoch1349 Batch40 Loss: 4.3525\n",
      "Epoch1349 Batch50 Loss: 4.3786\n",
      "Epoch1349 Batch60 Loss: 4.5335\n",
      "Epoch1349 Batch70 Loss: 3.4341\n",
      "shuffling the dataset\n",
      "train Loss: 4.3939\n",
      "Epoch 1350/1999\n",
      "----------\n",
      "Epoch1350 Batch0 Loss: 4.6615\n",
      "Epoch1350 Batch10 Loss: 5.1522\n",
      "Epoch1350 Batch20 Loss: 5.4072\n",
      "Epoch1350 Batch30 Loss: 3.8981\n",
      "Epoch1350 Batch40 Loss: 3.9633\n",
      "Epoch1350 Batch50 Loss: 4.5798\n",
      "Epoch1350 Batch60 Loss: 3.5995\n",
      "Epoch1350 Batch70 Loss: 3.5636\n",
      "shuffling the dataset\n",
      "train Loss: 4.2243\n",
      "Epoch 1351/1999\n",
      "----------\n",
      "Epoch1351 Batch0 Loss: 3.9718\n",
      "Epoch1351 Batch10 Loss: 3.8053\n",
      "Epoch1351 Batch20 Loss: 3.2648\n",
      "Epoch1351 Batch30 Loss: 5.0612\n",
      "Epoch1351 Batch40 Loss: 2.9697\n",
      "Epoch1351 Batch50 Loss: 3.2831\n",
      "Epoch1351 Batch60 Loss: 3.7225\n",
      "Epoch1351 Batch70 Loss: 3.3388\n",
      "shuffling the dataset\n",
      "train Loss: 3.9625\n",
      "Epoch 1352/1999\n",
      "----------\n",
      "Epoch1352 Batch0 Loss: 3.5045\n",
      "Epoch1352 Batch10 Loss: 3.4502\n",
      "Epoch1352 Batch20 Loss: 3.4730\n",
      "Epoch1352 Batch30 Loss: 3.7566\n",
      "Epoch1352 Batch40 Loss: 3.6010\n",
      "Epoch1352 Batch50 Loss: 4.0763\n",
      "Epoch1352 Batch60 Loss: 6.5025\n",
      "Epoch1352 Batch70 Loss: 3.9292\n",
      "shuffling the dataset\n",
      "train Loss: 3.7552\n",
      "Epoch 1353/1999\n",
      "----------\n",
      "Epoch1353 Batch0 Loss: 3.3434\n",
      "Epoch1353 Batch10 Loss: 3.5396\n",
      "Epoch1353 Batch20 Loss: 3.9580\n",
      "Epoch1353 Batch30 Loss: 3.7113\n",
      "Epoch1353 Batch40 Loss: 4.1670\n",
      "Epoch1353 Batch50 Loss: 3.0886\n",
      "Epoch1353 Batch60 Loss: 2.7796\n",
      "Epoch1353 Batch70 Loss: 2.8486\n",
      "shuffling the dataset\n",
      "train Loss: 3.5884\n",
      "Epoch 1354/1999\n",
      "----------\n",
      "Epoch1354 Batch0 Loss: 3.9438\n",
      "Epoch1354 Batch10 Loss: 3.1502\n",
      "Epoch1354 Batch20 Loss: 2.9503\n",
      "Epoch1354 Batch30 Loss: 3.7169\n",
      "Epoch1354 Batch40 Loss: 3.0794\n",
      "Epoch1354 Batch50 Loss: 3.2003\n",
      "Epoch1354 Batch60 Loss: 3.7325\n",
      "Epoch1354 Batch70 Loss: 5.3032\n",
      "shuffling the dataset\n",
      "train Loss: 3.4694\n",
      "Epoch 1355/1999\n",
      "----------\n",
      "Epoch1355 Batch0 Loss: 3.0489\n",
      "Epoch1355 Batch10 Loss: 3.5482\n",
      "Epoch1355 Batch20 Loss: 3.7524\n",
      "Epoch1355 Batch30 Loss: 4.6445\n",
      "Epoch1355 Batch40 Loss: 3.0146\n",
      "Epoch1355 Batch50 Loss: 4.0740\n",
      "Epoch1355 Batch60 Loss: 3.0341\n",
      "Epoch1355 Batch70 Loss: 2.7361\n",
      "shuffling the dataset\n",
      "train Loss: 3.3930\n",
      "Epoch 1356/1999\n",
      "----------\n",
      "Epoch1356 Batch0 Loss: 3.4412\n",
      "Epoch1356 Batch10 Loss: 2.9032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1356 Batch20 Loss: 3.6835\n",
      "Epoch1356 Batch30 Loss: 2.7145\n",
      "Epoch1356 Batch40 Loss: 3.6454\n",
      "Epoch1356 Batch50 Loss: 3.6761\n",
      "Epoch1356 Batch60 Loss: 3.2053\n",
      "Epoch1356 Batch70 Loss: 2.9093\n",
      "shuffling the dataset\n",
      "train Loss: 3.2530\n",
      "Epoch 1357/1999\n",
      "----------\n",
      "Epoch1357 Batch0 Loss: 3.1587\n",
      "Epoch1357 Batch10 Loss: 2.4689\n",
      "Epoch1357 Batch20 Loss: 2.8471\n",
      "Epoch1357 Batch30 Loss: 3.1814\n",
      "Epoch1357 Batch40 Loss: 3.4468\n",
      "Epoch1357 Batch50 Loss: 2.5087\n",
      "Epoch1357 Batch60 Loss: 3.1368\n",
      "Epoch1357 Batch70 Loss: 3.7118\n",
      "shuffling the dataset\n",
      "train Loss: 3.1777\n",
      "Epoch 1358/1999\n",
      "----------\n",
      "Epoch1358 Batch0 Loss: 2.4847\n",
      "Epoch1358 Batch10 Loss: 3.2685\n",
      "Epoch1358 Batch20 Loss: 2.7303\n",
      "Epoch1358 Batch30 Loss: 3.1612\n",
      "Epoch1358 Batch40 Loss: 2.4471\n",
      "Epoch1358 Batch50 Loss: 2.8813\n",
      "Epoch1358 Batch60 Loss: 3.4253\n",
      "Epoch1358 Batch70 Loss: 2.9333\n",
      "shuffling the dataset\n",
      "train Loss: 3.0883\n",
      "Epoch 1359/1999\n",
      "----------\n",
      "Epoch1359 Batch0 Loss: 3.0121\n",
      "Epoch1359 Batch10 Loss: 3.4219\n",
      "Epoch1359 Batch20 Loss: 3.1088\n",
      "Epoch1359 Batch30 Loss: 2.4444\n",
      "Epoch1359 Batch40 Loss: 2.7745\n",
      "Epoch1359 Batch50 Loss: 2.1314\n",
      "Epoch1359 Batch60 Loss: 3.0536\n",
      "Epoch1359 Batch70 Loss: 3.6899\n",
      "shuffling the dataset\n",
      "train Loss: 3.0082\n",
      "Epoch 1360/1999\n",
      "----------\n",
      "Epoch1360 Batch0 Loss: 2.8357\n",
      "Epoch1360 Batch10 Loss: 2.9529\n",
      "Epoch1360 Batch20 Loss: 2.9726\n",
      "Epoch1360 Batch30 Loss: 2.6138\n",
      "Epoch1360 Batch40 Loss: 2.5541\n",
      "Epoch1360 Batch50 Loss: 3.3227\n",
      "Epoch1360 Batch60 Loss: 3.4118\n",
      "Epoch1360 Batch70 Loss: 2.9348\n",
      "shuffling the dataset\n",
      "train Loss: 2.9499\n",
      "Epoch 1361/1999\n",
      "----------\n",
      "Epoch1361 Batch0 Loss: 2.4224\n",
      "Epoch1361 Batch10 Loss: 5.8148\n",
      "Epoch1361 Batch20 Loss: 2.4930\n",
      "Epoch1361 Batch30 Loss: 3.6963\n",
      "Epoch1361 Batch40 Loss: 3.5745\n",
      "Epoch1361 Batch50 Loss: 2.1585\n",
      "Epoch1361 Batch60 Loss: 2.2687\n",
      "Epoch1361 Batch70 Loss: 2.6992\n",
      "shuffling the dataset\n",
      "train Loss: 2.8771\n",
      "Epoch 1362/1999\n",
      "----------\n",
      "Epoch1362 Batch0 Loss: 2.3050\n",
      "Epoch1362 Batch10 Loss: 3.1108\n",
      "Epoch1362 Batch20 Loss: 2.5941\n",
      "Epoch1362 Batch30 Loss: 2.4254\n",
      "Epoch1362 Batch40 Loss: 2.4539\n",
      "Epoch1362 Batch50 Loss: 3.2585\n",
      "Epoch1362 Batch60 Loss: 2.7256\n",
      "Epoch1362 Batch70 Loss: 1.9809\n",
      "shuffling the dataset\n",
      "train Loss: 2.8346\n",
      "Epoch 1363/1999\n",
      "----------\n",
      "Epoch1363 Batch0 Loss: 2.0928\n",
      "Epoch1363 Batch10 Loss: 3.3134\n",
      "Epoch1363 Batch20 Loss: 4.6535\n",
      "Epoch1363 Batch30 Loss: 2.4278\n",
      "Epoch1363 Batch40 Loss: 3.1782\n",
      "Epoch1363 Batch50 Loss: 2.5937\n",
      "Epoch1363 Batch60 Loss: 2.5397\n",
      "Epoch1363 Batch70 Loss: 2.5874\n",
      "shuffling the dataset\n",
      "train Loss: 2.7548\n",
      "Epoch 1364/1999\n",
      "----------\n",
      "Epoch1364 Batch0 Loss: 2.6784\n",
      "Epoch1364 Batch10 Loss: 2.2076\n",
      "Epoch1364 Batch20 Loss: 2.4381\n",
      "Epoch1364 Batch30 Loss: 2.1567\n",
      "Epoch1364 Batch40 Loss: 2.1750\n",
      "Epoch1364 Batch50 Loss: 2.2842\n",
      "Epoch1364 Batch60 Loss: 2.0845\n",
      "Epoch1364 Batch70 Loss: 3.2187\n",
      "shuffling the dataset\n",
      "train Loss: 2.6874\n",
      "Epoch 1365/1999\n",
      "----------\n",
      "Epoch1365 Batch0 Loss: 2.6170\n",
      "Epoch1365 Batch10 Loss: 2.7845\n",
      "Epoch1365 Batch20 Loss: 3.2526\n",
      "Epoch1365 Batch30 Loss: 3.3525\n",
      "Epoch1365 Batch40 Loss: 2.2160\n",
      "Epoch1365 Batch50 Loss: 3.0069\n",
      "Epoch1365 Batch60 Loss: 2.3769\n",
      "Epoch1365 Batch70 Loss: 2.3889\n",
      "shuffling the dataset\n",
      "train Loss: 2.7217\n",
      "Epoch 1366/1999\n",
      "----------\n",
      "Epoch1366 Batch0 Loss: 2.4208\n",
      "Epoch1366 Batch10 Loss: 2.2951\n",
      "Epoch1366 Batch20 Loss: 3.0399\n",
      "Epoch1366 Batch30 Loss: 2.2842\n",
      "Epoch1366 Batch40 Loss: 2.2993\n",
      "Epoch1366 Batch50 Loss: 2.4043\n",
      "Epoch1366 Batch60 Loss: 5.7658\n",
      "Epoch1366 Batch70 Loss: 2.6292\n",
      "shuffling the dataset\n",
      "train Loss: 2.6479\n",
      "Epoch 1367/1999\n",
      "----------\n",
      "Epoch1367 Batch0 Loss: 2.3969\n",
      "Epoch1367 Batch10 Loss: 2.4852\n",
      "Epoch1367 Batch20 Loss: 2.6219\n",
      "Epoch1367 Batch30 Loss: 2.1745\n",
      "Epoch1367 Batch40 Loss: 2.7607\n",
      "Epoch1367 Batch50 Loss: 3.0078\n",
      "Epoch1367 Batch60 Loss: 3.1959\n",
      "Epoch1367 Batch70 Loss: 2.8409\n",
      "shuffling the dataset\n",
      "train Loss: 2.5637\n",
      "Epoch 1368/1999\n",
      "----------\n",
      "Epoch1368 Batch0 Loss: 2.3627\n",
      "Epoch1368 Batch10 Loss: 2.3376\n",
      "Epoch1368 Batch20 Loss: 2.4799\n",
      "Epoch1368 Batch30 Loss: 1.9580\n",
      "Epoch1368 Batch40 Loss: 2.0755\n",
      "Epoch1368 Batch50 Loss: 2.7399\n",
      "Epoch1368 Batch60 Loss: 2.2395\n",
      "Epoch1368 Batch70 Loss: 2.6202\n",
      "shuffling the dataset\n",
      "train Loss: 2.4969\n",
      "Epoch 1369/1999\n",
      "----------\n",
      "Epoch1369 Batch0 Loss: 1.9516\n",
      "Epoch1369 Batch10 Loss: 2.6144\n",
      "Epoch1369 Batch20 Loss: 2.2839\n",
      "Epoch1369 Batch30 Loss: 2.5781\n",
      "Epoch1369 Batch40 Loss: 2.8353\n",
      "Epoch1369 Batch50 Loss: 2.3555\n",
      "Epoch1369 Batch60 Loss: 2.0055\n",
      "Epoch1369 Batch70 Loss: 2.7238\n",
      "shuffling the dataset\n",
      "train Loss: 2.4190\n",
      "Epoch 1370/1999\n",
      "----------\n",
      "Epoch1370 Batch0 Loss: 2.2948\n",
      "Epoch1370 Batch10 Loss: 2.3955\n",
      "Epoch1370 Batch20 Loss: 2.5098\n",
      "Epoch1370 Batch30 Loss: 1.7891\n",
      "Epoch1370 Batch40 Loss: 1.7973\n",
      "Epoch1370 Batch50 Loss: 2.4701\n",
      "Epoch1370 Batch60 Loss: 1.6818\n",
      "Epoch1370 Batch70 Loss: 2.5108\n",
      "shuffling the dataset\n",
      "train Loss: 2.3863\n",
      "Epoch 1371/1999\n",
      "----------\n",
      "Epoch1371 Batch0 Loss: 2.3844\n",
      "Epoch1371 Batch10 Loss: 2.2069\n",
      "Epoch1371 Batch20 Loss: 2.7251\n",
      "Epoch1371 Batch30 Loss: 3.3323\n",
      "Epoch1371 Batch40 Loss: 2.8823\n",
      "Epoch1371 Batch50 Loss: 3.1791\n",
      "Epoch1371 Batch60 Loss: 3.0907\n",
      "Epoch1371 Batch70 Loss: 3.5972\n",
      "shuffling the dataset\n",
      "train Loss: 2.8253\n",
      "Epoch 1372/1999\n",
      "----------\n",
      "Epoch1372 Batch0 Loss: 3.2324\n",
      "Epoch1372 Batch10 Loss: 4.5554\n",
      "Epoch1372 Batch20 Loss: 10.4855\n",
      "Epoch1372 Batch30 Loss: 11.7134\n",
      "Epoch1372 Batch40 Loss: 9.3910\n",
      "Epoch1372 Batch50 Loss: 14.4515\n",
      "Epoch1372 Batch60 Loss: 12.4420\n",
      "Epoch1372 Batch70 Loss: 9.6045\n",
      "shuffling the dataset\n",
      "train Loss: 9.6021\n",
      "Epoch 1373/1999\n",
      "----------\n",
      "Epoch1373 Batch0 Loss: 9.3971\n",
      "Epoch1373 Batch10 Loss: 7.9314\n",
      "Epoch1373 Batch20 Loss: 7.8646\n",
      "Epoch1373 Batch30 Loss: 6.8109\n",
      "Epoch1373 Batch40 Loss: 6.3161\n",
      "Epoch1373 Batch50 Loss: 4.6059\n",
      "Epoch1373 Batch60 Loss: 6.8970\n",
      "Epoch1373 Batch70 Loss: 7.6407\n",
      "shuffling the dataset\n",
      "train Loss: 6.8891\n",
      "Epoch 1374/1999\n",
      "----------\n",
      "Epoch1374 Batch0 Loss: 5.8170\n",
      "Epoch1374 Batch10 Loss: 5.7929\n",
      "Epoch1374 Batch20 Loss: 5.5775\n",
      "Epoch1374 Batch30 Loss: 7.7247\n",
      "Epoch1374 Batch40 Loss: 4.3495\n",
      "Epoch1374 Batch50 Loss: 4.5930\n",
      "Epoch1374 Batch60 Loss: 5.0898\n",
      "Epoch1374 Batch70 Loss: 4.8329\n",
      "shuffling the dataset\n",
      "train Loss: 5.0261\n",
      "Epoch 1375/1999\n",
      "----------\n",
      "Epoch1375 Batch0 Loss: 4.2460\n",
      "Epoch1375 Batch10 Loss: 3.0709\n",
      "Epoch1375 Batch20 Loss: 2.6576\n",
      "Epoch1375 Batch30 Loss: 3.0713\n",
      "Epoch1375 Batch40 Loss: 3.2028\n",
      "Epoch1375 Batch50 Loss: 3.0551\n",
      "Epoch1375 Batch60 Loss: 3.6176\n",
      "Epoch1375 Batch70 Loss: 3.5954\n",
      "shuffling the dataset\n",
      "train Loss: 3.2401\n",
      "Epoch 1376/1999\n",
      "----------\n",
      "Epoch1376 Batch0 Loss: 2.7380\n",
      "Epoch1376 Batch10 Loss: 2.7576\n",
      "Epoch1376 Batch20 Loss: 2.9665\n",
      "Epoch1376 Batch30 Loss: 2.3105\n",
      "Epoch1376 Batch40 Loss: 2.5220\n",
      "Epoch1376 Batch50 Loss: 3.6557\n",
      "Epoch1376 Batch60 Loss: 2.4822\n",
      "Epoch1376 Batch70 Loss: 2.7316\n",
      "shuffling the dataset\n",
      "train Loss: 2.6333\n",
      "Epoch 1377/1999\n",
      "----------\n",
      "Epoch1377 Batch0 Loss: 2.3872\n",
      "Epoch1377 Batch10 Loss: 1.9148\n",
      "Epoch1377 Batch20 Loss: 2.3998\n",
      "Epoch1377 Batch30 Loss: 2.0545\n",
      "Epoch1377 Batch40 Loss: 1.5697\n",
      "Epoch1377 Batch50 Loss: 2.3684\n",
      "Epoch1377 Batch60 Loss: 2.1135\n",
      "Epoch1377 Batch70 Loss: 2.7741\n",
      "shuffling the dataset\n",
      "train Loss: 2.3311\n",
      "Epoch 1378/1999\n",
      "----------\n",
      "Epoch1378 Batch0 Loss: 2.0383\n",
      "Epoch1378 Batch10 Loss: 2.4012\n",
      "Epoch1378 Batch20 Loss: 1.8471\n",
      "Epoch1378 Batch30 Loss: 2.1917\n",
      "Epoch1378 Batch40 Loss: 2.1504\n",
      "Epoch1378 Batch50 Loss: 2.0716\n",
      "Epoch1378 Batch60 Loss: 1.7635\n",
      "Epoch1378 Batch70 Loss: 2.0864\n",
      "shuffling the dataset\n",
      "train Loss: 2.1702\n",
      "Epoch 1379/1999\n",
      "----------\n",
      "Epoch1379 Batch0 Loss: 1.3760\n",
      "Epoch1379 Batch10 Loss: 1.8971\n",
      "Epoch1379 Batch20 Loss: 1.5858\n",
      "Epoch1379 Batch30 Loss: 2.2784\n",
      "Epoch1379 Batch40 Loss: 1.9740\n",
      "Epoch1379 Batch50 Loss: 2.0427\n",
      "Epoch1379 Batch60 Loss: 1.8317\n",
      "Epoch1379 Batch70 Loss: 2.4397\n",
      "shuffling the dataset\n",
      "train Loss: 2.0743\n",
      "Epoch 1380/1999\n",
      "----------\n",
      "Epoch1380 Batch0 Loss: 1.8587\n",
      "Epoch1380 Batch10 Loss: 2.0691\n",
      "Epoch1380 Batch20 Loss: 2.0466\n",
      "Epoch1380 Batch30 Loss: 1.8247\n",
      "Epoch1380 Batch40 Loss: 2.1559\n",
      "Epoch1380 Batch50 Loss: 2.4078\n",
      "Epoch1380 Batch60 Loss: 2.1443\n",
      "Epoch1380 Batch70 Loss: 1.8119\n",
      "shuffling the dataset\n",
      "train Loss: 2.0099\n",
      "Epoch 1381/1999\n",
      "----------\n",
      "Epoch1381 Batch0 Loss: 1.3791\n",
      "Epoch1381 Batch10 Loss: 2.0941\n",
      "Epoch1381 Batch20 Loss: 1.6585\n",
      "Epoch1381 Batch30 Loss: 2.5677\n",
      "Epoch1381 Batch40 Loss: 1.2739\n",
      "Epoch1381 Batch50 Loss: 2.0001\n",
      "Epoch1381 Batch60 Loss: 2.0055\n",
      "Epoch1381 Batch70 Loss: 1.6422\n",
      "shuffling the dataset\n",
      "train Loss: 1.9675\n",
      "Epoch 1382/1999\n",
      "----------\n",
      "Epoch1382 Batch0 Loss: 2.0636\n",
      "Epoch1382 Batch10 Loss: 1.9528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1382 Batch20 Loss: 1.6044\n",
      "Epoch1382 Batch30 Loss: 1.6405\n",
      "Epoch1382 Batch40 Loss: 1.7521\n",
      "Epoch1382 Batch50 Loss: 1.7262\n",
      "Epoch1382 Batch60 Loss: 1.4808\n",
      "Epoch1382 Batch70 Loss: 1.5363\n",
      "shuffling the dataset\n",
      "train Loss: 1.9421\n",
      "Epoch 1383/1999\n",
      "----------\n",
      "Epoch1383 Batch0 Loss: 2.1242\n",
      "Epoch1383 Batch10 Loss: 1.6787\n",
      "Epoch1383 Batch20 Loss: 1.8124\n",
      "Epoch1383 Batch30 Loss: 1.9514\n",
      "Epoch1383 Batch40 Loss: 1.8510\n",
      "Epoch1383 Batch50 Loss: 2.8162\n",
      "Epoch1383 Batch60 Loss: 1.6658\n",
      "Epoch1383 Batch70 Loss: 1.7797\n",
      "shuffling the dataset\n",
      "train Loss: 1.9228\n",
      "Epoch 1384/1999\n",
      "----------\n",
      "Epoch1384 Batch0 Loss: 2.1446\n",
      "Epoch1384 Batch10 Loss: 1.2827\n",
      "Epoch1384 Batch20 Loss: 1.9388\n",
      "Epoch1384 Batch30 Loss: 2.0104\n",
      "Epoch1384 Batch40 Loss: 2.2169\n",
      "Epoch1384 Batch50 Loss: 2.0473\n",
      "Epoch1384 Batch60 Loss: 1.6936\n",
      "Epoch1384 Batch70 Loss: 1.5134\n",
      "shuffling the dataset\n",
      "train Loss: 1.9024\n",
      "Epoch 1385/1999\n",
      "----------\n",
      "Epoch1385 Batch0 Loss: 1.1956\n",
      "Epoch1385 Batch10 Loss: 2.0939\n",
      "Epoch1385 Batch20 Loss: 1.7422\n",
      "Epoch1385 Batch30 Loss: 1.6863\n",
      "Epoch1385 Batch40 Loss: 1.8165\n",
      "Epoch1385 Batch50 Loss: 1.7137\n",
      "Epoch1385 Batch60 Loss: 2.0231\n",
      "Epoch1385 Batch70 Loss: 1.9055\n",
      "shuffling the dataset\n",
      "train Loss: 1.8858\n",
      "Epoch 1386/1999\n",
      "----------\n",
      "Epoch1386 Batch0 Loss: 1.5353\n",
      "Epoch1386 Batch10 Loss: 1.4133\n",
      "Epoch1386 Batch20 Loss: 1.2562\n",
      "Epoch1386 Batch30 Loss: 1.4355\n",
      "Epoch1386 Batch40 Loss: 2.2505\n",
      "Epoch1386 Batch50 Loss: 1.8432\n",
      "Epoch1386 Batch60 Loss: 1.9530\n",
      "Epoch1386 Batch70 Loss: 1.7910\n",
      "shuffling the dataset\n",
      "train Loss: 1.8621\n",
      "Epoch 1387/1999\n",
      "----------\n",
      "Epoch1387 Batch0 Loss: 1.2919\n",
      "Epoch1387 Batch10 Loss: 1.6750\n",
      "Epoch1387 Batch20 Loss: 1.9510\n",
      "Epoch1387 Batch30 Loss: 1.4428\n",
      "Epoch1387 Batch40 Loss: 4.5599\n",
      "Epoch1387 Batch50 Loss: 1.2401\n",
      "Epoch1387 Batch60 Loss: 1.6049\n",
      "Epoch1387 Batch70 Loss: 1.7819\n",
      "shuffling the dataset\n",
      "train Loss: 1.8410\n",
      "Epoch 1388/1999\n",
      "----------\n",
      "Epoch1388 Batch0 Loss: 1.3641\n",
      "Epoch1388 Batch10 Loss: 1.6956\n",
      "Epoch1388 Batch20 Loss: 1.6008\n",
      "Epoch1388 Batch30 Loss: 1.9292\n",
      "Epoch1388 Batch40 Loss: 1.3010\n",
      "Epoch1388 Batch50 Loss: 2.2469\n",
      "Epoch1388 Batch60 Loss: 1.6517\n",
      "Epoch1388 Batch70 Loss: 1.4573\n",
      "shuffling the dataset\n",
      "train Loss: 1.8170\n",
      "Epoch 1389/1999\n",
      "----------\n",
      "Epoch1389 Batch0 Loss: 3.9099\n",
      "Epoch1389 Batch10 Loss: 2.2855\n",
      "Epoch1389 Batch20 Loss: 1.6254\n",
      "Epoch1389 Batch30 Loss: 1.7119\n",
      "Epoch1389 Batch40 Loss: 1.0490\n",
      "Epoch1389 Batch50 Loss: 1.6845\n",
      "Epoch1389 Batch60 Loss: 1.4094\n",
      "Epoch1389 Batch70 Loss: 1.9818\n",
      "shuffling the dataset\n",
      "train Loss: 1.7885\n",
      "Epoch 1390/1999\n",
      "----------\n",
      "Epoch1390 Batch0 Loss: 2.4279\n",
      "Epoch1390 Batch10 Loss: 1.6353\n",
      "Epoch1390 Batch20 Loss: 1.6658\n",
      "Epoch1390 Batch30 Loss: 1.7027\n",
      "Epoch1390 Batch40 Loss: 1.6072\n",
      "Epoch1390 Batch50 Loss: 1.8660\n",
      "Epoch1390 Batch60 Loss: 1.7075\n",
      "Epoch1390 Batch70 Loss: 1.7051\n",
      "shuffling the dataset\n",
      "train Loss: 1.7812\n",
      "Epoch 1391/1999\n",
      "----------\n",
      "Epoch1391 Batch0 Loss: 1.9109\n",
      "Epoch1391 Batch10 Loss: 1.8061\n",
      "Epoch1391 Batch20 Loss: 1.8476\n",
      "Epoch1391 Batch30 Loss: 1.7222\n",
      "Epoch1391 Batch40 Loss: 2.6199\n",
      "Epoch1391 Batch50 Loss: 1.5345\n",
      "Epoch1391 Batch60 Loss: 1.8658\n",
      "Epoch1391 Batch70 Loss: 1.8338\n",
      "shuffling the dataset\n",
      "train Loss: 1.7976\n",
      "Epoch 1392/1999\n",
      "----------\n",
      "Epoch1392 Batch0 Loss: 1.6075\n",
      "Epoch1392 Batch10 Loss: 1.4421\n",
      "Epoch1392 Batch20 Loss: 1.5677\n",
      "Epoch1392 Batch30 Loss: 1.7382\n",
      "Epoch1392 Batch40 Loss: 1.4796\n",
      "Epoch1392 Batch50 Loss: 2.3606\n",
      "Epoch1392 Batch60 Loss: 1.5017\n",
      "Epoch1392 Batch70 Loss: 1.8159\n",
      "shuffling the dataset\n",
      "train Loss: 1.8013\n",
      "Epoch 1393/1999\n",
      "----------\n",
      "Epoch1393 Batch0 Loss: 1.5098\n",
      "Epoch1393 Batch10 Loss: 1.9940\n",
      "Epoch1393 Batch20 Loss: 1.7887\n",
      "Epoch1393 Batch30 Loss: 1.2363\n",
      "Epoch1393 Batch40 Loss: 2.1154\n",
      "Epoch1393 Batch50 Loss: 1.7878\n",
      "Epoch1393 Batch60 Loss: 1.4210\n",
      "Epoch1393 Batch70 Loss: 1.7411\n",
      "shuffling the dataset\n",
      "train Loss: 1.7690\n",
      "Epoch 1394/1999\n",
      "----------\n",
      "Epoch1394 Batch0 Loss: 1.4404\n",
      "Epoch1394 Batch10 Loss: 1.8568\n",
      "Epoch1394 Batch20 Loss: 1.8615\n",
      "Epoch1394 Batch30 Loss: 1.7286\n",
      "Epoch1394 Batch40 Loss: 1.8061\n",
      "Epoch1394 Batch50 Loss: 1.5414\n",
      "Epoch1394 Batch60 Loss: 1.2683\n",
      "Epoch1394 Batch70 Loss: 1.5000\n",
      "shuffling the dataset\n",
      "train Loss: 1.7365\n",
      "Epoch 1395/1999\n",
      "----------\n",
      "Epoch1395 Batch0 Loss: 2.0261\n",
      "Epoch1395 Batch10 Loss: 1.7051\n",
      "Epoch1395 Batch20 Loss: 1.4608\n",
      "Epoch1395 Batch30 Loss: 1.5999\n",
      "Epoch1395 Batch40 Loss: 1.7032\n",
      "Epoch1395 Batch50 Loss: 1.7279\n",
      "Epoch1395 Batch60 Loss: 1.3484\n",
      "Epoch1395 Batch70 Loss: 1.6560\n",
      "shuffling the dataset\n",
      "train Loss: 1.7511\n",
      "Epoch 1396/1999\n",
      "----------\n",
      "Epoch1396 Batch0 Loss: 1.8263\n",
      "Epoch1396 Batch10 Loss: 1.5504\n",
      "Epoch1396 Batch20 Loss: 1.3848\n",
      "Epoch1396 Batch30 Loss: 1.9379\n",
      "Epoch1396 Batch40 Loss: 2.2793\n",
      "Epoch1396 Batch50 Loss: 1.6033\n",
      "Epoch1396 Batch60 Loss: 1.6706\n",
      "Epoch1396 Batch70 Loss: 1.8880\n",
      "shuffling the dataset\n",
      "train Loss: 1.7368\n",
      "Epoch 1397/1999\n",
      "----------\n",
      "Epoch1397 Batch0 Loss: 1.6898\n",
      "Epoch1397 Batch10 Loss: 1.4899\n",
      "Epoch1397 Batch20 Loss: 1.9571\n",
      "Epoch1397 Batch30 Loss: 1.8173\n",
      "Epoch1397 Batch40 Loss: 1.4713\n",
      "Epoch1397 Batch50 Loss: 1.7085\n",
      "Epoch1397 Batch60 Loss: 2.3641\n",
      "Epoch1397 Batch70 Loss: 1.7540\n",
      "shuffling the dataset\n",
      "train Loss: 1.7360\n",
      "Epoch 1398/1999\n",
      "----------\n",
      "Epoch1398 Batch0 Loss: 1.5458\n",
      "Epoch1398 Batch10 Loss: 1.7211\n",
      "Epoch1398 Batch20 Loss: 1.7584\n",
      "Epoch1398 Batch30 Loss: 1.1741\n",
      "Epoch1398 Batch40 Loss: 1.6361\n",
      "Epoch1398 Batch50 Loss: 1.6512\n",
      "Epoch1398 Batch60 Loss: 1.8305\n",
      "Epoch1398 Batch70 Loss: 1.4930\n",
      "shuffling the dataset\n",
      "train Loss: 1.6965\n",
      "Epoch 1399/1999\n",
      "----------\n",
      "Epoch1399 Batch0 Loss: 2.0882\n",
      "Epoch1399 Batch10 Loss: 1.3098\n",
      "Epoch1399 Batch20 Loss: 1.4383\n",
      "Epoch1399 Batch30 Loss: 1.7254\n",
      "Epoch1399 Batch40 Loss: 1.6125\n",
      "Epoch1399 Batch50 Loss: 1.6806\n",
      "Epoch1399 Batch60 Loss: 1.6115\n",
      "Epoch1399 Batch70 Loss: 1.3302\n",
      "shuffling the dataset\n",
      "train Loss: 1.6567\n",
      "Epoch 1400/1999\n",
      "----------\n",
      "Epoch1400 Batch0 Loss: 2.7162\n",
      "Epoch1400 Batch10 Loss: 1.5011\n",
      "Epoch1400 Batch20 Loss: 1.9715\n",
      "Epoch1400 Batch30 Loss: 1.9192\n",
      "Epoch1400 Batch40 Loss: 1.6485\n",
      "Epoch1400 Batch50 Loss: 1.0917\n",
      "Epoch1400 Batch60 Loss: 1.3161\n",
      "Epoch1400 Batch70 Loss: 1.1182\n",
      "shuffling the dataset\n",
      "train Loss: 1.6201\n",
      "Epoch 1401/1999\n",
      "----------\n",
      "Epoch1401 Batch0 Loss: 1.5178\n",
      "Epoch1401 Batch10 Loss: 1.1628\n",
      "Epoch1401 Batch20 Loss: 1.6420\n",
      "Epoch1401 Batch30 Loss: 1.6512\n",
      "Epoch1401 Batch40 Loss: 1.5841\n",
      "Epoch1401 Batch50 Loss: 1.3964\n",
      "Epoch1401 Batch60 Loss: 1.6750\n",
      "Epoch1401 Batch70 Loss: 1.4758\n",
      "shuffling the dataset\n",
      "train Loss: 1.6015\n",
      "Epoch 1402/1999\n",
      "----------\n",
      "Epoch1402 Batch0 Loss: 1.7425\n",
      "Epoch1402 Batch10 Loss: 1.6983\n",
      "Epoch1402 Batch20 Loss: 1.3519\n",
      "Epoch1402 Batch30 Loss: 1.6043\n",
      "Epoch1402 Batch40 Loss: 1.6963\n",
      "Epoch1402 Batch50 Loss: 1.6295\n",
      "Epoch1402 Batch60 Loss: 1.4261\n",
      "Epoch1402 Batch70 Loss: 1.7059\n",
      "shuffling the dataset\n",
      "train Loss: 1.5829\n",
      "Epoch 1403/1999\n",
      "----------\n",
      "Epoch1403 Batch0 Loss: 1.2928\n",
      "Epoch1403 Batch10 Loss: 0.9900\n",
      "Epoch1403 Batch20 Loss: 1.6400\n",
      "Epoch1403 Batch30 Loss: 1.8679\n",
      "Epoch1403 Batch40 Loss: 1.4200\n",
      "Epoch1403 Batch50 Loss: 1.3181\n",
      "Epoch1403 Batch60 Loss: 1.6369\n",
      "Epoch1403 Batch70 Loss: 3.4024\n",
      "shuffling the dataset\n",
      "train Loss: 1.6009\n",
      "Epoch 1404/1999\n",
      "----------\n",
      "Epoch1404 Batch0 Loss: 1.4619\n",
      "Epoch1404 Batch10 Loss: 4.3897\n",
      "Epoch1404 Batch20 Loss: 1.4191\n",
      "Epoch1404 Batch30 Loss: 1.7414\n",
      "Epoch1404 Batch40 Loss: 1.1613\n",
      "Epoch1404 Batch50 Loss: 1.4846\n",
      "Epoch1404 Batch60 Loss: 1.3182\n",
      "Epoch1404 Batch70 Loss: 1.4798\n",
      "shuffling the dataset\n",
      "train Loss: 1.5787\n",
      "Epoch 1405/1999\n",
      "----------\n",
      "Epoch1405 Batch0 Loss: 1.8136\n",
      "Epoch1405 Batch10 Loss: 1.2065\n",
      "Epoch1405 Batch20 Loss: 1.7823\n",
      "Epoch1405 Batch30 Loss: 1.0902\n",
      "Epoch1405 Batch40 Loss: 1.0842\n",
      "Epoch1405 Batch50 Loss: 1.4196\n",
      "Epoch1405 Batch60 Loss: 1.3688\n",
      "Epoch1405 Batch70 Loss: 1.7818\n",
      "shuffling the dataset\n",
      "train Loss: 1.5690\n",
      "Epoch 1406/1999\n",
      "----------\n",
      "Epoch1406 Batch0 Loss: 1.4160\n",
      "Epoch1406 Batch10 Loss: 1.5399\n",
      "Epoch1406 Batch20 Loss: 0.9341\n",
      "Epoch1406 Batch30 Loss: 1.3549\n",
      "Epoch1406 Batch40 Loss: 1.8780\n",
      "Epoch1406 Batch50 Loss: 1.4869\n",
      "Epoch1406 Batch60 Loss: 1.3063\n",
      "Epoch1406 Batch70 Loss: 1.4210\n",
      "shuffling the dataset\n",
      "train Loss: 1.5570\n",
      "Epoch 1407/1999\n",
      "----------\n",
      "Epoch1407 Batch0 Loss: 1.7503\n",
      "Epoch1407 Batch10 Loss: 1.3329\n",
      "Epoch1407 Batch20 Loss: 1.8554\n",
      "Epoch1407 Batch30 Loss: 1.0200\n",
      "Epoch1407 Batch40 Loss: 1.0402\n",
      "Epoch1407 Batch50 Loss: 1.5477\n",
      "Epoch1407 Batch60 Loss: 1.6875\n",
      "Epoch1407 Batch70 Loss: 2.0769\n",
      "shuffling the dataset\n",
      "train Loss: 1.6034\n",
      "Epoch 1408/1999\n",
      "----------\n",
      "Epoch1408 Batch0 Loss: 2.0088\n",
      "Epoch1408 Batch10 Loss: 1.4698\n",
      "Epoch1408 Batch20 Loss: 1.5160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1408 Batch30 Loss: 1.9673\n",
      "Epoch1408 Batch40 Loss: 1.5470\n",
      "Epoch1408 Batch50 Loss: 2.1826\n",
      "Epoch1408 Batch60 Loss: 1.4797\n",
      "Epoch1408 Batch70 Loss: 1.4733\n",
      "shuffling the dataset\n",
      "train Loss: 1.6739\n",
      "Epoch 1409/1999\n",
      "----------\n",
      "Epoch1409 Batch0 Loss: 1.4037\n",
      "Epoch1409 Batch10 Loss: 1.8244\n",
      "Epoch1409 Batch20 Loss: 1.5540\n",
      "Epoch1409 Batch30 Loss: 1.5608\n",
      "Epoch1409 Batch40 Loss: 1.4899\n",
      "Epoch1409 Batch50 Loss: 1.4051\n",
      "Epoch1409 Batch60 Loss: 1.3704\n",
      "Epoch1409 Batch70 Loss: 1.6496\n",
      "shuffling the dataset\n",
      "train Loss: 1.6327\n",
      "Epoch 1410/1999\n",
      "----------\n",
      "Epoch1410 Batch0 Loss: 1.5083\n",
      "Epoch1410 Batch10 Loss: 1.3867\n",
      "Epoch1410 Batch20 Loss: 1.1525\n",
      "Epoch1410 Batch30 Loss: 1.5076\n",
      "Epoch1410 Batch40 Loss: 1.2809\n",
      "Epoch1410 Batch50 Loss: 1.6872\n",
      "Epoch1410 Batch60 Loss: 1.4802\n",
      "Epoch1410 Batch70 Loss: 1.9504\n",
      "shuffling the dataset\n",
      "train Loss: 1.5576\n",
      "Epoch 1411/1999\n",
      "----------\n",
      "Epoch1411 Batch0 Loss: 1.3208\n",
      "Epoch1411 Batch10 Loss: 1.5008\n",
      "Epoch1411 Batch20 Loss: 1.5114\n",
      "Epoch1411 Batch30 Loss: 1.2505\n",
      "Epoch1411 Batch40 Loss: 2.0918\n",
      "Epoch1411 Batch50 Loss: 1.3331\n",
      "Epoch1411 Batch60 Loss: 1.3621\n",
      "Epoch1411 Batch70 Loss: 1.5178\n",
      "shuffling the dataset\n",
      "train Loss: 1.4950\n",
      "Epoch 1412/1999\n",
      "----------\n",
      "Epoch1412 Batch0 Loss: 1.3271\n",
      "Epoch1412 Batch10 Loss: 1.5329\n",
      "Epoch1412 Batch20 Loss: 1.6141\n",
      "Epoch1412 Batch30 Loss: 1.4375\n",
      "Epoch1412 Batch40 Loss: 1.5206\n",
      "Epoch1412 Batch50 Loss: 2.2052\n",
      "Epoch1412 Batch60 Loss: 1.4990\n",
      "Epoch1412 Batch70 Loss: 1.6614\n",
      "shuffling the dataset\n",
      "train Loss: 1.4479\n",
      "Epoch 1413/1999\n",
      "----------\n",
      "Epoch1413 Batch0 Loss: 1.3970\n",
      "Epoch1413 Batch10 Loss: 1.3745\n",
      "Epoch1413 Batch20 Loss: 1.3529\n",
      "Epoch1413 Batch30 Loss: 1.3787\n",
      "Epoch1413 Batch40 Loss: 1.2023\n",
      "Epoch1413 Batch50 Loss: 1.3411\n",
      "Epoch1413 Batch60 Loss: 1.4452\n",
      "Epoch1413 Batch70 Loss: 1.7839\n",
      "shuffling the dataset\n",
      "train Loss: 1.4234\n",
      "Epoch 1414/1999\n",
      "----------\n",
      "Epoch1414 Batch0 Loss: 1.2377\n",
      "Epoch1414 Batch10 Loss: 1.0896\n",
      "Epoch1414 Batch20 Loss: 1.0542\n",
      "Epoch1414 Batch30 Loss: 1.2799\n",
      "Epoch1414 Batch40 Loss: 1.2691\n",
      "Epoch1414 Batch50 Loss: 1.5042\n",
      "Epoch1414 Batch60 Loss: 1.2294\n",
      "Epoch1414 Batch70 Loss: 1.3526\n",
      "shuffling the dataset\n",
      "train Loss: 1.3928\n",
      "Epoch 1415/1999\n",
      "----------\n",
      "Epoch1415 Batch0 Loss: 1.2897\n",
      "Epoch1415 Batch10 Loss: 1.0550\n",
      "Epoch1415 Batch20 Loss: 1.1100\n",
      "Epoch1415 Batch30 Loss: 1.3510\n",
      "Epoch1415 Batch40 Loss: 1.3715\n",
      "Epoch1415 Batch50 Loss: 1.1344\n",
      "Epoch1415 Batch60 Loss: 1.2375\n",
      "Epoch1415 Batch70 Loss: 0.9210\n",
      "shuffling the dataset\n",
      "train Loss: 1.3996\n",
      "Epoch 1416/1999\n",
      "----------\n",
      "Epoch1416 Batch0 Loss: 1.2623\n",
      "Epoch1416 Batch10 Loss: 1.1526\n",
      "Epoch1416 Batch20 Loss: 1.1155\n",
      "Epoch1416 Batch30 Loss: 1.2664\n",
      "Epoch1416 Batch40 Loss: 1.1016\n",
      "Epoch1416 Batch50 Loss: 0.9302\n",
      "Epoch1416 Batch60 Loss: 1.5076\n",
      "Epoch1416 Batch70 Loss: 1.4921\n",
      "shuffling the dataset\n",
      "train Loss: 1.4030\n",
      "Epoch 1417/1999\n",
      "----------\n",
      "Epoch1417 Batch0 Loss: 1.4073\n",
      "Epoch1417 Batch10 Loss: 1.3471\n",
      "Epoch1417 Batch20 Loss: 1.0826\n",
      "Epoch1417 Batch30 Loss: 1.1899\n",
      "Epoch1417 Batch40 Loss: 1.8649\n",
      "Epoch1417 Batch50 Loss: 1.4586\n",
      "Epoch1417 Batch60 Loss: 1.1669\n",
      "Epoch1417 Batch70 Loss: 1.3813\n",
      "shuffling the dataset\n",
      "train Loss: 1.3794\n",
      "Epoch 1418/1999\n",
      "----------\n",
      "Epoch1418 Batch0 Loss: 1.5556\n",
      "Epoch1418 Batch10 Loss: 1.4243\n",
      "Epoch1418 Batch20 Loss: 1.2418\n",
      "Epoch1418 Batch30 Loss: 1.4924\n",
      "Epoch1418 Batch40 Loss: 1.0622\n",
      "Epoch1418 Batch50 Loss: 1.5109\n",
      "Epoch1418 Batch60 Loss: 1.2798\n",
      "Epoch1418 Batch70 Loss: 1.1373\n",
      "shuffling the dataset\n",
      "train Loss: 1.3780\n",
      "Epoch 1419/1999\n",
      "----------\n",
      "Epoch1419 Batch0 Loss: 1.0963\n",
      "Epoch1419 Batch10 Loss: 1.3877\n",
      "Epoch1419 Batch20 Loss: 1.0358\n",
      "Epoch1419 Batch30 Loss: 0.8579\n",
      "Epoch1419 Batch40 Loss: 1.3419\n",
      "Epoch1419 Batch50 Loss: 1.1280\n",
      "Epoch1419 Batch60 Loss: 1.4687\n",
      "Epoch1419 Batch70 Loss: 1.2806\n",
      "shuffling the dataset\n",
      "train Loss: 1.3627\n",
      "Epoch 1420/1999\n",
      "----------\n",
      "Epoch1420 Batch0 Loss: 1.2671\n",
      "Epoch1420 Batch10 Loss: 1.2516\n",
      "Epoch1420 Batch20 Loss: 1.0975\n",
      "Epoch1420 Batch30 Loss: 1.4205\n",
      "Epoch1420 Batch40 Loss: 1.1232\n",
      "Epoch1420 Batch50 Loss: 1.0909\n",
      "Epoch1420 Batch60 Loss: 1.7432\n",
      "Epoch1420 Batch70 Loss: 1.0179\n",
      "shuffling the dataset\n",
      "train Loss: 1.3497\n",
      "Epoch 1421/1999\n",
      "----------\n",
      "Epoch1421 Batch0 Loss: 1.5850\n",
      "Epoch1421 Batch10 Loss: 1.1177\n",
      "Epoch1421 Batch20 Loss: 1.2413\n",
      "Epoch1421 Batch30 Loss: 0.9963\n",
      "Epoch1421 Batch40 Loss: 1.0731\n",
      "Epoch1421 Batch50 Loss: 1.3343\n",
      "Epoch1421 Batch60 Loss: 1.3807\n",
      "Epoch1421 Batch70 Loss: 1.1900\n",
      "shuffling the dataset\n",
      "train Loss: 1.3411\n",
      "Epoch 1422/1999\n",
      "----------\n",
      "Epoch1422 Batch0 Loss: 1.2054\n",
      "Epoch1422 Batch10 Loss: 4.5044\n",
      "Epoch1422 Batch20 Loss: 1.1188\n",
      "Epoch1422 Batch30 Loss: 1.4124\n",
      "Epoch1422 Batch40 Loss: 1.3361\n",
      "Epoch1422 Batch50 Loss: 1.0281\n",
      "Epoch1422 Batch60 Loss: 1.3742\n",
      "Epoch1422 Batch70 Loss: 2.1163\n",
      "shuffling the dataset\n",
      "train Loss: 1.3297\n",
      "Epoch 1423/1999\n",
      "----------\n",
      "Epoch1423 Batch0 Loss: 1.3628\n",
      "Epoch1423 Batch10 Loss: 1.3050\n",
      "Epoch1423 Batch20 Loss: 0.8747\n",
      "Epoch1423 Batch30 Loss: 1.0325\n",
      "Epoch1423 Batch40 Loss: 1.1352\n",
      "Epoch1423 Batch50 Loss: 1.1670\n",
      "Epoch1423 Batch60 Loss: 1.1412\n",
      "Epoch1423 Batch70 Loss: 0.9914\n",
      "shuffling the dataset\n",
      "train Loss: 1.3109\n",
      "Epoch 1424/1999\n",
      "----------\n",
      "Epoch1424 Batch0 Loss: 0.8098\n",
      "Epoch1424 Batch10 Loss: 1.3334\n",
      "Epoch1424 Batch20 Loss: 1.6114\n",
      "Epoch1424 Batch30 Loss: 1.4317\n",
      "Epoch1424 Batch40 Loss: 1.2902\n",
      "Epoch1424 Batch50 Loss: 1.1720\n",
      "Epoch1424 Batch60 Loss: 1.1870\n",
      "Epoch1424 Batch70 Loss: 1.0195\n",
      "shuffling the dataset\n",
      "train Loss: 1.3144\n",
      "Epoch 1425/1999\n",
      "----------\n",
      "Epoch1425 Batch0 Loss: 1.7020\n",
      "Epoch1425 Batch10 Loss: 1.3057\n",
      "Epoch1425 Batch20 Loss: 1.3151\n",
      "Epoch1425 Batch30 Loss: 1.0167\n",
      "Epoch1425 Batch40 Loss: 1.2924\n",
      "Epoch1425 Batch50 Loss: 1.6230\n",
      "Epoch1425 Batch60 Loss: 1.4032\n",
      "Epoch1425 Batch70 Loss: 0.9348\n",
      "shuffling the dataset\n",
      "train Loss: 1.2989\n",
      "Epoch 1426/1999\n",
      "----------\n",
      "Epoch1426 Batch0 Loss: 1.1784\n",
      "Epoch1426 Batch10 Loss: 1.3143\n",
      "Epoch1426 Batch20 Loss: 1.1556\n",
      "Epoch1426 Batch30 Loss: 1.5194\n",
      "Epoch1426 Batch40 Loss: 0.9013\n",
      "Epoch1426 Batch50 Loss: 0.9969\n",
      "Epoch1426 Batch60 Loss: 1.2068\n",
      "Epoch1426 Batch70 Loss: 1.7976\n",
      "shuffling the dataset\n",
      "train Loss: 1.2800\n",
      "Epoch 1427/1999\n",
      "----------\n",
      "Epoch1427 Batch0 Loss: 1.1587\n",
      "Epoch1427 Batch10 Loss: 1.0917\n",
      "Epoch1427 Batch20 Loss: 1.3053\n",
      "Epoch1427 Batch30 Loss: 1.1897\n",
      "Epoch1427 Batch40 Loss: 1.1665\n",
      "Epoch1427 Batch50 Loss: 1.5286\n",
      "Epoch1427 Batch60 Loss: 1.5009\n",
      "Epoch1427 Batch70 Loss: 1.4274\n",
      "shuffling the dataset\n",
      "train Loss: 1.2754\n",
      "Epoch 1428/1999\n",
      "----------\n",
      "Epoch1428 Batch0 Loss: 1.4598\n",
      "Epoch1428 Batch10 Loss: 1.3705\n",
      "Epoch1428 Batch20 Loss: 3.9752\n",
      "Epoch1428 Batch30 Loss: 1.6418\n",
      "Epoch1428 Batch40 Loss: 1.3829\n",
      "Epoch1428 Batch50 Loss: 1.0213\n",
      "Epoch1428 Batch60 Loss: 1.0586\n",
      "Epoch1428 Batch70 Loss: 0.9355\n",
      "shuffling the dataset\n",
      "train Loss: 1.2681\n",
      "Epoch 1429/1999\n",
      "----------\n",
      "Epoch1429 Batch0 Loss: 0.7368\n",
      "Epoch1429 Batch10 Loss: 0.8483\n",
      "Epoch1429 Batch20 Loss: 1.0891\n",
      "Epoch1429 Batch30 Loss: 1.0360\n",
      "Epoch1429 Batch40 Loss: 0.9547\n",
      "Epoch1429 Batch50 Loss: 1.3531\n",
      "Epoch1429 Batch60 Loss: 1.3067\n",
      "Epoch1429 Batch70 Loss: 1.2340\n",
      "shuffling the dataset\n",
      "train Loss: 1.2503\n",
      "Epoch 1430/1999\n",
      "----------\n",
      "Epoch1430 Batch0 Loss: 0.9629\n",
      "Epoch1430 Batch10 Loss: 1.1560\n",
      "Epoch1430 Batch20 Loss: 1.0798\n",
      "Epoch1430 Batch30 Loss: 1.0066\n",
      "Epoch1430 Batch40 Loss: 1.1050\n",
      "Epoch1430 Batch50 Loss: 1.0888\n",
      "Epoch1430 Batch60 Loss: 1.7637\n",
      "Epoch1430 Batch70 Loss: 1.1543\n",
      "shuffling the dataset\n",
      "train Loss: 1.2309\n",
      "Epoch 1431/1999\n",
      "----------\n",
      "Epoch1431 Batch0 Loss: 1.2113\n",
      "Epoch1431 Batch10 Loss: 1.1000\n",
      "Epoch1431 Batch20 Loss: 1.3267\n",
      "Epoch1431 Batch30 Loss: 1.0712\n",
      "Epoch1431 Batch40 Loss: 1.3952\n",
      "Epoch1431 Batch50 Loss: 1.6659\n",
      "Epoch1431 Batch60 Loss: 1.2391\n",
      "Epoch1431 Batch70 Loss: 1.2844\n",
      "shuffling the dataset\n",
      "train Loss: 1.2396\n",
      "Epoch 1432/1999\n",
      "----------\n",
      "Epoch1432 Batch0 Loss: 1.1513\n",
      "Epoch1432 Batch10 Loss: 1.4362\n",
      "Epoch1432 Batch20 Loss: 1.1984\n",
      "Epoch1432 Batch30 Loss: 0.7272\n",
      "Epoch1432 Batch40 Loss: 0.8902\n",
      "Epoch1432 Batch50 Loss: 1.0466\n",
      "Epoch1432 Batch60 Loss: 1.0457\n",
      "Epoch1432 Batch70 Loss: 1.2556\n",
      "shuffling the dataset\n",
      "train Loss: 1.2620\n",
      "Epoch 1433/1999\n",
      "----------\n",
      "Epoch1433 Batch0 Loss: 1.1237\n",
      "Epoch1433 Batch10 Loss: 1.0249\n",
      "Epoch1433 Batch20 Loss: 1.2380\n",
      "Epoch1433 Batch30 Loss: 1.4144\n",
      "Epoch1433 Batch40 Loss: 1.2376\n",
      "Epoch1433 Batch50 Loss: 3.1930\n",
      "Epoch1433 Batch60 Loss: 10.5514\n",
      "Epoch1433 Batch70 Loss: 10.4646\n",
      "shuffling the dataset\n",
      "train Loss: 4.0319\n",
      "Epoch 1434/1999\n",
      "----------\n",
      "Epoch1434 Batch0 Loss: 9.9680\n",
      "Epoch1434 Batch10 Loss: 10.4171\n",
      "Epoch1434 Batch20 Loss: 8.1233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1434 Batch30 Loss: 7.0893\n",
      "Epoch1434 Batch40 Loss: 6.0969\n",
      "Epoch1434 Batch50 Loss: 5.8757\n",
      "Epoch1434 Batch60 Loss: 5.7600\n",
      "Epoch1434 Batch70 Loss: 6.6441\n",
      "shuffling the dataset\n",
      "train Loss: 8.2445\n",
      "Epoch 1435/1999\n",
      "----------\n",
      "Epoch1435 Batch0 Loss: 5.8791\n",
      "Epoch1435 Batch10 Loss: 4.4160\n",
      "Epoch1435 Batch20 Loss: 6.1179\n",
      "Epoch1435 Batch30 Loss: 3.9452\n",
      "Epoch1435 Batch40 Loss: 3.0999\n",
      "Epoch1435 Batch50 Loss: 3.2854\n",
      "Epoch1435 Batch60 Loss: 3.1575\n",
      "Epoch1435 Batch70 Loss: 4.0002\n",
      "shuffling the dataset\n",
      "train Loss: 3.9182\n",
      "Epoch 1436/1999\n",
      "----------\n",
      "Epoch1436 Batch0 Loss: 2.3439\n",
      "Epoch1436 Batch10 Loss: 2.8768\n",
      "Epoch1436 Batch20 Loss: 2.2728\n",
      "Epoch1436 Batch30 Loss: 1.8285\n",
      "Epoch1436 Batch40 Loss: 2.0075\n",
      "Epoch1436 Batch50 Loss: 2.1179\n",
      "Epoch1436 Batch60 Loss: 3.8784\n",
      "Epoch1436 Batch70 Loss: 1.8743\n",
      "shuffling the dataset\n",
      "train Loss: 2.1525\n",
      "Epoch 1437/1999\n",
      "----------\n",
      "Epoch1437 Batch0 Loss: 1.7420\n",
      "Epoch1437 Batch10 Loss: 1.6396\n",
      "Epoch1437 Batch20 Loss: 3.8707\n",
      "Epoch1437 Batch30 Loss: 1.6958\n",
      "Epoch1437 Batch40 Loss: 1.2862\n",
      "Epoch1437 Batch50 Loss: 1.6278\n",
      "Epoch1437 Batch60 Loss: 1.4906\n",
      "Epoch1437 Batch70 Loss: 1.6893\n",
      "shuffling the dataset\n",
      "train Loss: 1.5366\n",
      "Epoch 1438/1999\n",
      "----------\n",
      "Epoch1438 Batch0 Loss: 1.4000\n",
      "Epoch1438 Batch10 Loss: 1.1833\n",
      "Epoch1438 Batch20 Loss: 1.2580\n",
      "Epoch1438 Batch30 Loss: 0.9705\n",
      "Epoch1438 Batch40 Loss: 1.0551\n",
      "Epoch1438 Batch50 Loss: 1.1590\n",
      "Epoch1438 Batch60 Loss: 1.7660\n",
      "Epoch1438 Batch70 Loss: 1.2910\n",
      "shuffling the dataset\n",
      "train Loss: 1.2962\n",
      "Epoch 1439/1999\n",
      "----------\n",
      "Epoch1439 Batch0 Loss: 0.8127\n",
      "Epoch1439 Batch10 Loss: 1.0148\n",
      "Epoch1439 Batch20 Loss: 0.8817\n",
      "Epoch1439 Batch30 Loss: 1.2254\n",
      "Epoch1439 Batch40 Loss: 1.1663\n",
      "Epoch1439 Batch50 Loss: 1.1096\n",
      "Epoch1439 Batch60 Loss: 1.2760\n",
      "Epoch1439 Batch70 Loss: 1.0748\n",
      "shuffling the dataset\n",
      "train Loss: 1.1564\n",
      "Epoch 1440/1999\n",
      "----------\n",
      "Epoch1440 Batch0 Loss: 1.1101\n",
      "Epoch1440 Batch10 Loss: 0.9563\n",
      "Epoch1440 Batch20 Loss: 1.0570\n",
      "Epoch1440 Batch30 Loss: 0.8438\n",
      "Epoch1440 Batch40 Loss: 0.8717\n",
      "Epoch1440 Batch50 Loss: 3.3573\n",
      "Epoch1440 Batch60 Loss: 1.1500\n",
      "Epoch1440 Batch70 Loss: 0.7217\n",
      "shuffling the dataset\n",
      "train Loss: 1.0731\n",
      "Epoch 1441/1999\n",
      "----------\n",
      "Epoch1441 Batch0 Loss: 0.7447\n",
      "Epoch1441 Batch10 Loss: 1.0587\n",
      "Epoch1441 Batch20 Loss: 0.7300\n",
      "Epoch1441 Batch30 Loss: 1.1036\n",
      "Epoch1441 Batch40 Loss: 1.0250\n",
      "Epoch1441 Batch50 Loss: 0.8581\n",
      "Epoch1441 Batch60 Loss: 0.8516\n",
      "Epoch1441 Batch70 Loss: 1.0875\n",
      "shuffling the dataset\n",
      "train Loss: 1.0246\n",
      "Epoch 1442/1999\n",
      "----------\n",
      "Epoch1442 Batch0 Loss: 0.6951\n",
      "Epoch1442 Batch10 Loss: 0.8754\n",
      "Epoch1442 Batch20 Loss: 1.0114\n",
      "Epoch1442 Batch30 Loss: 1.0106\n",
      "Epoch1442 Batch40 Loss: 1.1714\n",
      "Epoch1442 Batch50 Loss: 0.6504\n",
      "Epoch1442 Batch60 Loss: 0.9197\n",
      "Epoch1442 Batch70 Loss: 1.2296\n",
      "shuffling the dataset\n",
      "train Loss: 0.9986\n",
      "Epoch 1443/1999\n",
      "----------\n",
      "Epoch1443 Batch0 Loss: 0.8308\n",
      "Epoch1443 Batch10 Loss: 1.8841\n",
      "Epoch1443 Batch20 Loss: 1.0358\n",
      "Epoch1443 Batch30 Loss: 0.9715\n",
      "Epoch1443 Batch40 Loss: 0.7897\n",
      "Epoch1443 Batch50 Loss: 0.7336\n",
      "Epoch1443 Batch60 Loss: 0.7327\n",
      "Epoch1443 Batch70 Loss: 0.6642\n",
      "shuffling the dataset\n",
      "train Loss: 0.9804\n",
      "Epoch 1444/1999\n",
      "----------\n",
      "Epoch1444 Batch0 Loss: 0.8564\n",
      "Epoch1444 Batch10 Loss: 0.8143\n",
      "Epoch1444 Batch20 Loss: 1.0412\n",
      "Epoch1444 Batch30 Loss: 0.9804\n",
      "Epoch1444 Batch40 Loss: 0.7481\n",
      "Epoch1444 Batch50 Loss: 1.0241\n",
      "Epoch1444 Batch60 Loss: 0.9787\n",
      "Epoch1444 Batch70 Loss: 1.0795\n",
      "shuffling the dataset\n",
      "train Loss: 0.9723\n",
      "Epoch 1445/1999\n",
      "----------\n",
      "Epoch1445 Batch0 Loss: 0.5988\n",
      "Epoch1445 Batch10 Loss: 0.6617\n",
      "Epoch1445 Batch20 Loss: 0.8089\n",
      "Epoch1445 Batch30 Loss: 0.9981\n",
      "Epoch1445 Batch40 Loss: 0.9308\n",
      "Epoch1445 Batch50 Loss: 0.7934\n",
      "Epoch1445 Batch60 Loss: 1.6563\n",
      "Epoch1445 Batch70 Loss: 0.7039\n",
      "shuffling the dataset\n",
      "train Loss: 0.9658\n",
      "Epoch 1446/1999\n",
      "----------\n",
      "Epoch1446 Batch0 Loss: 0.9489\n",
      "Epoch1446 Batch10 Loss: 1.6735\n",
      "Epoch1446 Batch20 Loss: 0.7832\n",
      "Epoch1446 Batch30 Loss: 0.8541\n",
      "Epoch1446 Batch40 Loss: 0.7856\n",
      "Epoch1446 Batch50 Loss: 0.8293\n",
      "Epoch1446 Batch60 Loss: 0.7668\n",
      "Epoch1446 Batch70 Loss: 1.1219\n",
      "shuffling the dataset\n",
      "train Loss: 0.9706\n",
      "Epoch 1447/1999\n",
      "----------\n",
      "Epoch1447 Batch0 Loss: 1.1113\n",
      "Epoch1447 Batch10 Loss: 1.0049\n",
      "Epoch1447 Batch20 Loss: 0.7763\n",
      "Epoch1447 Batch30 Loss: 0.7556\n",
      "Epoch1447 Batch40 Loss: 0.6512\n",
      "Epoch1447 Batch50 Loss: 0.8266\n",
      "Epoch1447 Batch60 Loss: 0.7742\n",
      "Epoch1447 Batch70 Loss: 0.8255\n",
      "shuffling the dataset\n",
      "train Loss: 0.9585\n",
      "Epoch 1448/1999\n",
      "----------\n",
      "Epoch1448 Batch0 Loss: 1.0956\n",
      "Epoch1448 Batch10 Loss: 0.9092\n",
      "Epoch1448 Batch20 Loss: 0.9122\n",
      "Epoch1448 Batch30 Loss: 0.8301\n",
      "Epoch1448 Batch40 Loss: 0.6698\n",
      "Epoch1448 Batch50 Loss: 0.9022\n",
      "Epoch1448 Batch60 Loss: 0.7366\n",
      "Epoch1448 Batch70 Loss: 0.8380\n",
      "shuffling the dataset\n",
      "train Loss: 0.9526\n",
      "Epoch 1449/1999\n",
      "----------\n",
      "Epoch1449 Batch0 Loss: 0.7998\n",
      "Epoch1449 Batch10 Loss: 1.7293\n",
      "Epoch1449 Batch20 Loss: 0.8157\n",
      "Epoch1449 Batch30 Loss: 0.8981\n",
      "Epoch1449 Batch40 Loss: 0.8405\n",
      "Epoch1449 Batch50 Loss: 0.6335\n",
      "Epoch1449 Batch60 Loss: 0.7685\n",
      "Epoch1449 Batch70 Loss: 0.7518\n",
      "shuffling the dataset\n",
      "train Loss: 0.9485\n",
      "Epoch 1450/1999\n",
      "----------\n",
      "Epoch1450 Batch0 Loss: 0.8825\n",
      "Epoch1450 Batch10 Loss: 0.8223\n",
      "Epoch1450 Batch20 Loss: 0.8849\n",
      "Epoch1450 Batch30 Loss: 0.8443\n",
      "Epoch1450 Batch40 Loss: 0.9421\n",
      "Epoch1450 Batch50 Loss: 0.6236\n",
      "Epoch1450 Batch60 Loss: 1.5543\n",
      "Epoch1450 Batch70 Loss: 0.8376\n",
      "shuffling the dataset\n",
      "train Loss: 0.9478\n",
      "Epoch 1451/1999\n",
      "----------\n",
      "Epoch1451 Batch0 Loss: 0.8202\n",
      "Epoch1451 Batch10 Loss: 1.0825\n",
      "Epoch1451 Batch20 Loss: 0.8693\n",
      "Epoch1451 Batch30 Loss: 1.1647\n",
      "Epoch1451 Batch40 Loss: 1.0535\n",
      "Epoch1451 Batch50 Loss: 0.6539\n",
      "Epoch1451 Batch60 Loss: 0.8414\n",
      "Epoch1451 Batch70 Loss: 0.9617\n",
      "shuffling the dataset\n",
      "train Loss: 0.9475\n",
      "Epoch 1452/1999\n",
      "----------\n",
      "Epoch1452 Batch0 Loss: 0.7236\n",
      "Epoch1452 Batch10 Loss: 0.9115\n",
      "Epoch1452 Batch20 Loss: 0.7513\n",
      "Epoch1452 Batch30 Loss: 0.9681\n",
      "Epoch1452 Batch40 Loss: 0.9537\n",
      "Epoch1452 Batch50 Loss: 0.8567\n",
      "Epoch1452 Batch60 Loss: 0.7938\n",
      "Epoch1452 Batch70 Loss: 0.7043\n",
      "shuffling the dataset\n",
      "train Loss: 0.9441\n",
      "Epoch 1453/1999\n",
      "----------\n",
      "Epoch1453 Batch0 Loss: 0.8515\n",
      "Epoch1453 Batch10 Loss: 0.9123\n",
      "Epoch1453 Batch20 Loss: 0.9539\n",
      "Epoch1453 Batch30 Loss: 1.0345\n",
      "Epoch1453 Batch40 Loss: 0.7207\n",
      "Epoch1453 Batch50 Loss: 0.6839\n",
      "Epoch1453 Batch60 Loss: 0.9127\n",
      "Epoch1453 Batch70 Loss: 0.9024\n",
      "shuffling the dataset\n",
      "train Loss: 0.9501\n",
      "Epoch 1454/1999\n",
      "----------\n",
      "Epoch1454 Batch0 Loss: 0.9236\n",
      "Epoch1454 Batch10 Loss: 0.9303\n",
      "Epoch1454 Batch20 Loss: 0.8497\n",
      "Epoch1454 Batch30 Loss: 1.0489\n",
      "Epoch1454 Batch40 Loss: 0.8452\n",
      "Epoch1454 Batch50 Loss: 0.9692\n",
      "Epoch1454 Batch60 Loss: 0.8462\n",
      "Epoch1454 Batch70 Loss: 1.0855\n",
      "shuffling the dataset\n",
      "train Loss: 0.9470\n",
      "Epoch 1455/1999\n",
      "----------\n",
      "Epoch1455 Batch0 Loss: 0.9027\n",
      "Epoch1455 Batch10 Loss: 0.7963\n",
      "Epoch1455 Batch20 Loss: 0.6988\n",
      "Epoch1455 Batch30 Loss: 0.7019\n",
      "Epoch1455 Batch40 Loss: 1.1137\n",
      "Epoch1455 Batch50 Loss: 0.9608\n",
      "Epoch1455 Batch60 Loss: 0.8954\n",
      "Epoch1455 Batch70 Loss: 0.9954\n",
      "shuffling the dataset\n",
      "train Loss: 0.9472\n",
      "Epoch 1456/1999\n",
      "----------\n",
      "Epoch1456 Batch0 Loss: 0.8039\n",
      "Epoch1456 Batch10 Loss: 0.8305\n",
      "Epoch1456 Batch20 Loss: 0.8882\n",
      "Epoch1456 Batch30 Loss: 1.0270\n",
      "Epoch1456 Batch40 Loss: 0.7166\n",
      "Epoch1456 Batch50 Loss: 0.8371\n",
      "Epoch1456 Batch60 Loss: 0.7732\n",
      "Epoch1456 Batch70 Loss: 0.6611\n",
      "shuffling the dataset\n",
      "train Loss: 0.9459\n",
      "Epoch 1457/1999\n",
      "----------\n",
      "Epoch1457 Batch0 Loss: 0.8951\n",
      "Epoch1457 Batch10 Loss: 0.7252\n",
      "Epoch1457 Batch20 Loss: 1.4950\n",
      "Epoch1457 Batch30 Loss: 0.9270\n",
      "Epoch1457 Batch40 Loss: 0.7773\n",
      "Epoch1457 Batch50 Loss: 0.9855\n",
      "Epoch1457 Batch60 Loss: 1.0439\n",
      "Epoch1457 Batch70 Loss: 0.7700\n",
      "shuffling the dataset\n",
      "train Loss: 0.9470\n",
      "Epoch 1458/1999\n",
      "----------\n",
      "Epoch1458 Batch0 Loss: 1.3471\n",
      "Epoch1458 Batch10 Loss: 0.9148\n",
      "Epoch1458 Batch20 Loss: 1.0489\n",
      "Epoch1458 Batch30 Loss: 0.7967\n",
      "Epoch1458 Batch40 Loss: 0.7404\n",
      "Epoch1458 Batch50 Loss: 1.0458\n",
      "Epoch1458 Batch60 Loss: 0.9837\n",
      "Epoch1458 Batch70 Loss: 0.8119\n",
      "shuffling the dataset\n",
      "train Loss: 0.9469\n",
      "Epoch 1459/1999\n",
      "----------\n",
      "Epoch1459 Batch0 Loss: 0.8884\n",
      "Epoch1459 Batch10 Loss: 0.9344\n",
      "Epoch1459 Batch20 Loss: 0.9451\n",
      "Epoch1459 Batch30 Loss: 0.8600\n",
      "Epoch1459 Batch40 Loss: 0.8195\n",
      "Epoch1459 Batch50 Loss: 0.6581\n",
      "Epoch1459 Batch60 Loss: 0.7052\n",
      "Epoch1459 Batch70 Loss: 0.7978\n",
      "shuffling the dataset\n",
      "train Loss: 0.9546\n",
      "Epoch 1460/1999\n",
      "----------\n",
      "Epoch1460 Batch0 Loss: 1.0273\n",
      "Epoch1460 Batch10 Loss: 0.7342\n",
      "Epoch1460 Batch20 Loss: 0.8262\n",
      "Epoch1460 Batch30 Loss: 0.5548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1460 Batch40 Loss: 0.8151\n",
      "Epoch1460 Batch50 Loss: 1.0681\n",
      "Epoch1460 Batch60 Loss: 0.8059\n",
      "Epoch1460 Batch70 Loss: 0.9243\n",
      "shuffling the dataset\n",
      "train Loss: 0.9550\n",
      "Epoch 1461/1999\n",
      "----------\n",
      "Epoch1461 Batch0 Loss: 0.9755\n",
      "Epoch1461 Batch10 Loss: 0.8617\n",
      "Epoch1461 Batch20 Loss: 0.8899\n",
      "Epoch1461 Batch30 Loss: 0.6088\n",
      "Epoch1461 Batch40 Loss: 0.8261\n",
      "Epoch1461 Batch50 Loss: 1.0946\n",
      "Epoch1461 Batch60 Loss: 0.7882\n",
      "Epoch1461 Batch70 Loss: 0.9338\n",
      "shuffling the dataset\n",
      "train Loss: 0.9658\n",
      "Epoch 1462/1999\n",
      "----------\n",
      "Epoch1462 Batch0 Loss: 0.7302\n",
      "Epoch1462 Batch10 Loss: 0.8605\n",
      "Epoch1462 Batch20 Loss: 1.0012\n",
      "Epoch1462 Batch30 Loss: 3.7837\n",
      "Epoch1462 Batch40 Loss: 0.8529\n",
      "Epoch1462 Batch50 Loss: 0.6332\n",
      "Epoch1462 Batch60 Loss: 0.8124\n",
      "Epoch1462 Batch70 Loss: 0.7097\n",
      "shuffling the dataset\n",
      "train Loss: 0.9740\n",
      "Epoch 1463/1999\n",
      "----------\n",
      "Epoch1463 Batch0 Loss: 0.8716\n",
      "Epoch1463 Batch10 Loss: 0.8993\n",
      "Epoch1463 Batch20 Loss: 0.8774\n",
      "Epoch1463 Batch30 Loss: 0.8303\n",
      "Epoch1463 Batch40 Loss: 0.9743\n",
      "Epoch1463 Batch50 Loss: 0.7764\n",
      "Epoch1463 Batch60 Loss: 0.9360\n",
      "Epoch1463 Batch70 Loss: 0.9198\n",
      "shuffling the dataset\n",
      "train Loss: 0.9595\n",
      "Epoch 1464/1999\n",
      "----------\n",
      "Epoch1464 Batch0 Loss: 0.7689\n",
      "Epoch1464 Batch10 Loss: 0.9750\n",
      "Epoch1464 Batch20 Loss: 0.8015\n",
      "Epoch1464 Batch30 Loss: 0.8825\n",
      "Epoch1464 Batch40 Loss: 0.8583\n",
      "Epoch1464 Batch50 Loss: 1.0290\n",
      "Epoch1464 Batch60 Loss: 1.1040\n",
      "Epoch1464 Batch70 Loss: 0.9612\n",
      "shuffling the dataset\n",
      "train Loss: 0.9453\n",
      "Epoch 1465/1999\n",
      "----------\n",
      "Epoch1465 Batch0 Loss: 0.8389\n",
      "Epoch1465 Batch10 Loss: 1.1661\n",
      "Epoch1465 Batch20 Loss: 0.9045\n",
      "Epoch1465 Batch30 Loss: 0.9226\n",
      "Epoch1465 Batch40 Loss: 0.8396\n",
      "Epoch1465 Batch50 Loss: 0.8607\n",
      "Epoch1465 Batch60 Loss: 1.8222\n",
      "Epoch1465 Batch70 Loss: 2.0853\n",
      "shuffling the dataset\n",
      "train Loss: 1.1427\n",
      "Epoch 1466/1999\n",
      "----------\n",
      "Epoch1466 Batch0 Loss: 2.1053\n",
      "Epoch1466 Batch10 Loss: 2.3929\n",
      "Epoch1466 Batch20 Loss: 3.6662\n",
      "Epoch1466 Batch30 Loss: 3.8346\n",
      "Epoch1466 Batch40 Loss: 3.3071\n",
      "Epoch1466 Batch50 Loss: 3.9541\n",
      "Epoch1466 Batch60 Loss: 4.8953\n",
      "Epoch1466 Batch70 Loss: 4.0750\n",
      "shuffling the dataset\n",
      "train Loss: 3.7769\n",
      "Epoch 1467/1999\n",
      "----------\n",
      "Epoch1467 Batch0 Loss: 2.6373\n",
      "Epoch1467 Batch10 Loss: 2.6448\n",
      "Epoch1467 Batch20 Loss: 2.2856\n",
      "Epoch1467 Batch30 Loss: 2.1229\n",
      "Epoch1467 Batch40 Loss: 2.1730\n",
      "Epoch1467 Batch50 Loss: 3.0569\n",
      "Epoch1467 Batch60 Loss: 2.1450\n",
      "Epoch1467 Batch70 Loss: 1.7619\n",
      "shuffling the dataset\n",
      "train Loss: 2.4751\n",
      "Epoch 1468/1999\n",
      "----------\n",
      "Epoch1468 Batch0 Loss: 1.4982\n",
      "Epoch1468 Batch10 Loss: 1.4532\n",
      "Epoch1468 Batch20 Loss: 1.4156\n",
      "Epoch1468 Batch30 Loss: 1.6438\n",
      "Epoch1468 Batch40 Loss: 1.3180\n",
      "Epoch1468 Batch50 Loss: 1.3612\n",
      "Epoch1468 Batch60 Loss: 1.3127\n",
      "Epoch1468 Batch70 Loss: 1.6301\n",
      "shuffling the dataset\n",
      "train Loss: 1.4481\n",
      "Epoch 1469/1999\n",
      "----------\n",
      "Epoch1469 Batch0 Loss: 0.7337\n",
      "Epoch1469 Batch10 Loss: 0.6443\n",
      "Epoch1469 Batch20 Loss: 1.0736\n",
      "Epoch1469 Batch30 Loss: 0.7738\n",
      "Epoch1469 Batch40 Loss: 0.9390\n",
      "Epoch1469 Batch50 Loss: 0.8456\n",
      "Epoch1469 Batch60 Loss: 0.8591\n",
      "Epoch1469 Batch70 Loss: 0.7669\n",
      "shuffling the dataset\n",
      "train Loss: 1.0781\n",
      "Epoch 1470/1999\n",
      "----------\n",
      "Epoch1470 Batch0 Loss: 0.7616\n",
      "Epoch1470 Batch10 Loss: 0.8610\n",
      "Epoch1470 Batch20 Loss: 0.8696\n",
      "Epoch1470 Batch30 Loss: 0.9862\n",
      "Epoch1470 Batch40 Loss: 0.6888\n",
      "Epoch1470 Batch50 Loss: 0.8431\n",
      "Epoch1470 Batch60 Loss: 0.7742\n",
      "Epoch1470 Batch70 Loss: 0.6543\n",
      "shuffling the dataset\n",
      "train Loss: 0.9208\n",
      "Epoch 1471/1999\n",
      "----------\n",
      "Epoch1471 Batch0 Loss: 0.9664\n",
      "Epoch1471 Batch10 Loss: 0.6515\n",
      "Epoch1471 Batch20 Loss: 0.6098\n",
      "Epoch1471 Batch30 Loss: 0.8739\n",
      "Epoch1471 Batch40 Loss: 0.8377\n",
      "Epoch1471 Batch50 Loss: 0.8393\n",
      "Epoch1471 Batch60 Loss: 0.5886\n",
      "Epoch1471 Batch70 Loss: 0.6445\n",
      "shuffling the dataset\n",
      "train Loss: 0.8437\n",
      "Epoch 1472/1999\n",
      "----------\n",
      "Epoch1472 Batch0 Loss: 0.6390\n",
      "Epoch1472 Batch10 Loss: 0.7951\n",
      "Epoch1472 Batch20 Loss: 0.7398\n",
      "Epoch1472 Batch30 Loss: 0.6773\n",
      "Epoch1472 Batch40 Loss: 0.5230\n",
      "Epoch1472 Batch50 Loss: 0.9239\n",
      "Epoch1472 Batch60 Loss: 0.7097\n",
      "Epoch1472 Batch70 Loss: 0.6772\n",
      "shuffling the dataset\n",
      "train Loss: 0.8047\n",
      "Epoch 1473/1999\n",
      "----------\n",
      "Epoch1473 Batch0 Loss: 0.7752\n",
      "Epoch1473 Batch10 Loss: 0.5507\n",
      "Epoch1473 Batch20 Loss: 0.8767\n",
      "Epoch1473 Batch30 Loss: 0.7248\n",
      "Epoch1473 Batch40 Loss: 0.3969\n",
      "Epoch1473 Batch50 Loss: 0.6795\n",
      "Epoch1473 Batch60 Loss: 0.6579\n",
      "Epoch1473 Batch70 Loss: 1.0962\n",
      "shuffling the dataset\n",
      "train Loss: 0.7854\n",
      "Epoch 1474/1999\n",
      "----------\n",
      "Epoch1474 Batch0 Loss: 0.4806\n",
      "Epoch1474 Batch10 Loss: 0.4811\n",
      "Epoch1474 Batch20 Loss: 0.7310\n",
      "Epoch1474 Batch30 Loss: 0.6355\n",
      "Epoch1474 Batch40 Loss: 0.7397\n",
      "Epoch1474 Batch50 Loss: 1.4928\n",
      "Epoch1474 Batch60 Loss: 0.5157\n",
      "Epoch1474 Batch70 Loss: 0.7627\n",
      "shuffling the dataset\n",
      "train Loss: 0.7823\n",
      "Epoch 1475/1999\n",
      "----------\n",
      "Epoch1475 Batch0 Loss: 0.6426\n",
      "Epoch1475 Batch10 Loss: 0.7567\n",
      "Epoch1475 Batch20 Loss: 0.7665\n",
      "Epoch1475 Batch30 Loss: 0.5400\n",
      "Epoch1475 Batch40 Loss: 0.5589\n",
      "Epoch1475 Batch50 Loss: 0.6222\n",
      "Epoch1475 Batch60 Loss: 0.5896\n",
      "Epoch1475 Batch70 Loss: 2.9179\n",
      "shuffling the dataset\n",
      "train Loss: 0.7847\n",
      "Epoch 1476/1999\n",
      "----------\n",
      "Epoch1476 Batch0 Loss: 0.9471\n",
      "Epoch1476 Batch10 Loss: 0.8163\n",
      "Epoch1476 Batch20 Loss: 0.9052\n",
      "Epoch1476 Batch30 Loss: 0.8073\n",
      "Epoch1476 Batch40 Loss: 0.6462\n",
      "Epoch1476 Batch50 Loss: 0.7516\n",
      "Epoch1476 Batch60 Loss: 0.7620\n",
      "Epoch1476 Batch70 Loss: 0.6917\n",
      "shuffling the dataset\n",
      "train Loss: 0.7875\n",
      "Epoch 1477/1999\n",
      "----------\n",
      "Epoch1477 Batch0 Loss: 0.6185\n",
      "Epoch1477 Batch10 Loss: 0.7383\n",
      "Epoch1477 Batch20 Loss: 0.7332\n",
      "Epoch1477 Batch30 Loss: 0.4999\n",
      "Epoch1477 Batch40 Loss: 0.8073\n",
      "Epoch1477 Batch50 Loss: 0.7567\n",
      "Epoch1477 Batch60 Loss: 0.8321\n",
      "Epoch1477 Batch70 Loss: 0.6116\n",
      "shuffling the dataset\n",
      "train Loss: 0.7853\n",
      "Epoch 1478/1999\n",
      "----------\n",
      "Epoch1478 Batch0 Loss: 0.6818\n",
      "Epoch1478 Batch10 Loss: 0.6279\n",
      "Epoch1478 Batch20 Loss: 0.7282\n",
      "Epoch1478 Batch30 Loss: 0.8525\n",
      "Epoch1478 Batch40 Loss: 0.5597\n",
      "Epoch1478 Batch50 Loss: 0.6800\n",
      "Epoch1478 Batch60 Loss: 0.6308\n",
      "Epoch1478 Batch70 Loss: 0.7454\n",
      "shuffling the dataset\n",
      "train Loss: 0.7819\n",
      "Epoch 1479/1999\n",
      "----------\n",
      "Epoch1479 Batch0 Loss: 0.7532\n",
      "Epoch1479 Batch10 Loss: 0.5756\n",
      "Epoch1479 Batch20 Loss: 0.6097\n",
      "Epoch1479 Batch30 Loss: 0.5763\n",
      "Epoch1479 Batch40 Loss: 2.5433\n",
      "Epoch1479 Batch50 Loss: 0.5867\n",
      "Epoch1479 Batch60 Loss: 0.6874\n",
      "Epoch1479 Batch70 Loss: 0.6562\n",
      "shuffling the dataset\n",
      "train Loss: 0.7804\n",
      "Epoch 1480/1999\n",
      "----------\n",
      "Epoch1480 Batch0 Loss: 0.7890\n",
      "Epoch1480 Batch10 Loss: 0.6142\n",
      "Epoch1480 Batch20 Loss: 0.6321\n",
      "Epoch1480 Batch30 Loss: 0.7617\n",
      "Epoch1480 Batch40 Loss: 0.6992\n",
      "Epoch1480 Batch50 Loss: 0.7240\n",
      "Epoch1480 Batch60 Loss: 0.8051\n",
      "Epoch1480 Batch70 Loss: 0.7071\n",
      "shuffling the dataset\n",
      "train Loss: 0.7801\n",
      "Epoch 1481/1999\n",
      "----------\n",
      "Epoch1481 Batch0 Loss: 1.1031\n",
      "Epoch1481 Batch10 Loss: 0.6283\n",
      "Epoch1481 Batch20 Loss: 0.7603\n",
      "Epoch1481 Batch30 Loss: 0.6294\n",
      "Epoch1481 Batch40 Loss: 0.7186\n",
      "Epoch1481 Batch50 Loss: 0.6962\n",
      "Epoch1481 Batch60 Loss: 0.6025\n",
      "Epoch1481 Batch70 Loss: 0.6853\n",
      "shuffling the dataset\n",
      "train Loss: 0.7800\n",
      "Epoch 1482/1999\n",
      "----------\n",
      "Epoch1482 Batch0 Loss: 0.7510\n",
      "Epoch1482 Batch10 Loss: 0.5325\n",
      "Epoch1482 Batch20 Loss: 0.6766\n",
      "Epoch1482 Batch30 Loss: 0.6734\n",
      "Epoch1482 Batch40 Loss: 0.7896\n",
      "Epoch1482 Batch50 Loss: 1.7834\n",
      "Epoch1482 Batch60 Loss: 0.5215\n",
      "Epoch1482 Batch70 Loss: 0.7613\n",
      "shuffling the dataset\n",
      "train Loss: 0.7878\n",
      "Epoch 1483/1999\n",
      "----------\n",
      "Epoch1483 Batch0 Loss: 0.7951\n",
      "Epoch1483 Batch10 Loss: 0.5650\n",
      "Epoch1483 Batch20 Loss: 0.6237\n",
      "Epoch1483 Batch30 Loss: 0.7323\n",
      "Epoch1483 Batch40 Loss: 0.7432\n",
      "Epoch1483 Batch50 Loss: 0.5810\n",
      "Epoch1483 Batch60 Loss: 0.8180\n",
      "Epoch1483 Batch70 Loss: 0.7640\n",
      "shuffling the dataset\n",
      "train Loss: 0.8133\n",
      "Epoch 1484/1999\n",
      "----------\n",
      "Epoch1484 Batch0 Loss: 0.7225\n",
      "Epoch1484 Batch10 Loss: 0.9336\n",
      "Epoch1484 Batch20 Loss: 0.9373\n",
      "Epoch1484 Batch30 Loss: 0.7793\n",
      "Epoch1484 Batch40 Loss: 0.7851\n",
      "Epoch1484 Batch50 Loss: 0.6939\n",
      "Epoch1484 Batch60 Loss: 0.6746\n",
      "Epoch1484 Batch70 Loss: 0.6991\n",
      "shuffling the dataset\n",
      "train Loss: 0.8366\n",
      "Epoch 1485/1999\n",
      "----------\n",
      "Epoch1485 Batch0 Loss: 0.5884\n",
      "Epoch1485 Batch10 Loss: 0.7409\n",
      "Epoch1485 Batch20 Loss: 0.7030\n",
      "Epoch1485 Batch30 Loss: 0.8110\n",
      "Epoch1485 Batch40 Loss: 0.4647\n",
      "Epoch1485 Batch50 Loss: 0.9507\n",
      "Epoch1485 Batch60 Loss: 0.7579\n",
      "Epoch1485 Batch70 Loss: 0.7266\n",
      "shuffling the dataset\n",
      "train Loss: 0.8396\n",
      "Epoch 1486/1999\n",
      "----------\n",
      "Epoch1486 Batch0 Loss: 0.8328\n",
      "Epoch1486 Batch10 Loss: 0.8064\n",
      "Epoch1486 Batch20 Loss: 0.8411\n",
      "Epoch1486 Batch30 Loss: 0.6028\n",
      "Epoch1486 Batch40 Loss: 0.6866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1486 Batch50 Loss: 0.8174\n",
      "Epoch1486 Batch60 Loss: 0.5966\n",
      "Epoch1486 Batch70 Loss: 0.5911\n",
      "shuffling the dataset\n",
      "train Loss: 0.8265\n",
      "Epoch 1487/1999\n",
      "----------\n",
      "Epoch1487 Batch0 Loss: 0.6467\n",
      "Epoch1487 Batch10 Loss: 0.8067\n",
      "Epoch1487 Batch20 Loss: 0.8452\n",
      "Epoch1487 Batch30 Loss: 0.9912\n",
      "Epoch1487 Batch40 Loss: 0.5429\n",
      "Epoch1487 Batch50 Loss: 0.7645\n",
      "Epoch1487 Batch60 Loss: 0.5063\n",
      "Epoch1487 Batch70 Loss: 0.7509\n",
      "shuffling the dataset\n",
      "train Loss: 0.8182\n",
      "Epoch 1488/1999\n",
      "----------\n",
      "Epoch1488 Batch0 Loss: 0.6959\n",
      "Epoch1488 Batch10 Loss: 0.6996\n",
      "Epoch1488 Batch20 Loss: 3.5101\n",
      "Epoch1488 Batch30 Loss: 0.7399\n",
      "Epoch1488 Batch40 Loss: 0.6984\n",
      "Epoch1488 Batch50 Loss: 0.7435\n",
      "Epoch1488 Batch60 Loss: 0.7832\n",
      "Epoch1488 Batch70 Loss: 0.9552\n",
      "shuffling the dataset\n",
      "train Loss: 0.8163\n",
      "Epoch 1489/1999\n",
      "----------\n",
      "Epoch1489 Batch0 Loss: 0.5115\n",
      "Epoch1489 Batch10 Loss: 0.5910\n",
      "Epoch1489 Batch20 Loss: 0.6667\n",
      "Epoch1489 Batch30 Loss: 0.7017\n",
      "Epoch1489 Batch40 Loss: 0.9031\n",
      "Epoch1489 Batch50 Loss: 0.8952\n",
      "Epoch1489 Batch60 Loss: 0.7668\n",
      "Epoch1489 Batch70 Loss: 0.6455\n",
      "shuffling the dataset\n",
      "train Loss: 0.8354\n",
      "Epoch 1490/1999\n",
      "----------\n",
      "Epoch1490 Batch0 Loss: 0.6105\n",
      "Epoch1490 Batch10 Loss: 0.7318\n",
      "Epoch1490 Batch20 Loss: 0.8579\n",
      "Epoch1490 Batch30 Loss: 0.8009\n",
      "Epoch1490 Batch40 Loss: 0.8033\n",
      "Epoch1490 Batch50 Loss: 0.6837\n",
      "Epoch1490 Batch60 Loss: 0.8693\n",
      "Epoch1490 Batch70 Loss: 0.7857\n",
      "shuffling the dataset\n",
      "train Loss: 0.8498\n",
      "Epoch 1491/1999\n",
      "----------\n",
      "Epoch1491 Batch0 Loss: 0.6010\n",
      "Epoch1491 Batch10 Loss: 0.7330\n",
      "Epoch1491 Batch20 Loss: 0.8216\n",
      "Epoch1491 Batch30 Loss: 0.7361\n",
      "Epoch1491 Batch40 Loss: 1.0425\n",
      "Epoch1491 Batch50 Loss: 0.7066\n",
      "Epoch1491 Batch60 Loss: 0.8866\n",
      "Epoch1491 Batch70 Loss: 0.6956\n",
      "shuffling the dataset\n",
      "train Loss: 0.8204\n",
      "Epoch 1492/1999\n",
      "----------\n",
      "Epoch1492 Batch0 Loss: 0.5123\n",
      "Epoch1492 Batch10 Loss: 0.6430\n",
      "Epoch1492 Batch20 Loss: 0.6013\n",
      "Epoch1492 Batch30 Loss: 0.5789\n",
      "Epoch1492 Batch40 Loss: 0.5734\n",
      "Epoch1492 Batch50 Loss: 0.8227\n",
      "Epoch1492 Batch60 Loss: 0.7517\n",
      "Epoch1492 Batch70 Loss: 0.8716\n",
      "shuffling the dataset\n",
      "train Loss: 0.8059\n",
      "Epoch 1493/1999\n",
      "----------\n",
      "Epoch1493 Batch0 Loss: 0.6539\n",
      "Epoch1493 Batch10 Loss: 0.5999\n",
      "Epoch1493 Batch20 Loss: 0.8372\n",
      "Epoch1493 Batch30 Loss: 0.7344\n",
      "Epoch1493 Batch40 Loss: 0.6553\n",
      "Epoch1493 Batch50 Loss: 0.5486\n",
      "Epoch1493 Batch60 Loss: 0.7470\n",
      "Epoch1493 Batch70 Loss: 0.8417\n",
      "shuffling the dataset\n",
      "train Loss: 0.7898\n",
      "Epoch 1494/1999\n",
      "----------\n",
      "Epoch1494 Batch0 Loss: 0.5204\n",
      "Epoch1494 Batch10 Loss: 0.6525\n",
      "Epoch1494 Batch20 Loss: 0.7245\n",
      "Epoch1494 Batch30 Loss: 0.5474\n",
      "Epoch1494 Batch40 Loss: 0.5420\n",
      "Epoch1494 Batch50 Loss: 0.5860\n",
      "Epoch1494 Batch60 Loss: 0.5421\n",
      "Epoch1494 Batch70 Loss: 0.8824\n",
      "shuffling the dataset\n",
      "train Loss: 0.7742\n",
      "Epoch 1495/1999\n",
      "----------\n",
      "Epoch1495 Batch0 Loss: 0.6300\n",
      "Epoch1495 Batch10 Loss: 0.8410\n",
      "Epoch1495 Batch20 Loss: 0.6298\n",
      "Epoch1495 Batch30 Loss: 0.7315\n",
      "Epoch1495 Batch40 Loss: 2.8135\n",
      "Epoch1495 Batch50 Loss: 0.7010\n",
      "Epoch1495 Batch60 Loss: 0.7365\n",
      "Epoch1495 Batch70 Loss: 0.6321\n",
      "shuffling the dataset\n",
      "train Loss: 0.7721\n",
      "Epoch 1496/1999\n",
      "----------\n",
      "Epoch1496 Batch0 Loss: 0.6097\n",
      "Epoch1496 Batch10 Loss: 0.6688\n",
      "Epoch1496 Batch20 Loss: 0.6540\n",
      "Epoch1496 Batch30 Loss: 0.5581\n",
      "Epoch1496 Batch40 Loss: 1.4151\n",
      "Epoch1496 Batch50 Loss: 1.4857\n",
      "Epoch1496 Batch60 Loss: 0.7069\n",
      "Epoch1496 Batch70 Loss: 0.7962\n",
      "shuffling the dataset\n",
      "train Loss: 0.7815\n",
      "Epoch 1497/1999\n",
      "----------\n",
      "Epoch1497 Batch0 Loss: 0.5800\n",
      "Epoch1497 Batch10 Loss: 0.6493\n",
      "Epoch1497 Batch20 Loss: 0.6170\n",
      "Epoch1497 Batch30 Loss: 0.7021\n",
      "Epoch1497 Batch40 Loss: 0.6318\n",
      "Epoch1497 Batch50 Loss: 0.7225\n",
      "Epoch1497 Batch60 Loss: 0.9408\n",
      "Epoch1497 Batch70 Loss: 0.6345\n",
      "shuffling the dataset\n",
      "train Loss: 0.7898\n",
      "Epoch 1498/1999\n",
      "----------\n",
      "Epoch1498 Batch0 Loss: 0.7130\n",
      "Epoch1498 Batch10 Loss: 0.6963\n",
      "Epoch1498 Batch20 Loss: 0.9765\n",
      "Epoch1498 Batch30 Loss: 0.7243\n",
      "Epoch1498 Batch40 Loss: 0.7694\n",
      "Epoch1498 Batch50 Loss: 0.6784\n",
      "Epoch1498 Batch60 Loss: 1.3212\n",
      "Epoch1498 Batch70 Loss: 1.0285\n",
      "shuffling the dataset\n",
      "train Loss: 0.8157\n",
      "Epoch 1499/1999\n",
      "----------\n",
      "Epoch1499 Batch0 Loss: 0.6685\n",
      "Epoch1499 Batch10 Loss: 0.7299\n",
      "Epoch1499 Batch20 Loss: 0.7814\n",
      "Epoch1499 Batch30 Loss: 0.6657\n",
      "Epoch1499 Batch40 Loss: 0.7327\n",
      "Epoch1499 Batch50 Loss: 0.7548\n",
      "Epoch1499 Batch60 Loss: 1.5739\n",
      "Epoch1499 Batch70 Loss: 0.5096\n",
      "shuffling the dataset\n",
      "train Loss: 0.8224\n",
      "Epoch 1500/1999\n",
      "----------\n",
      "Epoch1500 Batch0 Loss: 0.5420\n",
      "Epoch1500 Batch10 Loss: 0.6991\n",
      "Epoch1500 Batch20 Loss: 0.6723\n",
      "Epoch1500 Batch30 Loss: 0.8695\n",
      "Epoch1500 Batch40 Loss: 0.5264\n",
      "Epoch1500 Batch50 Loss: 0.5493\n",
      "Epoch1500 Batch60 Loss: 0.6641\n",
      "Epoch1500 Batch70 Loss: 0.4958\n",
      "shuffling the dataset\n",
      "train Loss: 0.8102\n",
      "Epoch 1501/1999\n",
      "----------\n",
      "Epoch1501 Batch0 Loss: 0.8617\n",
      "Epoch1501 Batch10 Loss: 0.8971\n",
      "Epoch1501 Batch20 Loss: 0.7451\n",
      "Epoch1501 Batch30 Loss: 0.6666\n",
      "Epoch1501 Batch40 Loss: 1.0011\n",
      "Epoch1501 Batch50 Loss: 0.6854\n",
      "Epoch1501 Batch60 Loss: 0.6156\n",
      "Epoch1501 Batch70 Loss: 0.6295\n",
      "shuffling the dataset\n",
      "train Loss: 0.8001\n",
      "Epoch 1502/1999\n",
      "----------\n",
      "Epoch1502 Batch0 Loss: 0.5237\n",
      "Epoch1502 Batch10 Loss: 0.6122\n",
      "Epoch1502 Batch20 Loss: 0.6674\n",
      "Epoch1502 Batch30 Loss: 0.8192\n",
      "Epoch1502 Batch40 Loss: 0.9543\n",
      "Epoch1502 Batch50 Loss: 3.0084\n",
      "Epoch1502 Batch60 Loss: 0.7752\n",
      "Epoch1502 Batch70 Loss: 3.7494\n",
      "shuffling the dataset\n",
      "train Loss: 0.7849\n",
      "Epoch 1503/1999\n",
      "----------\n",
      "Epoch1503 Batch0 Loss: 0.5773\n",
      "Epoch1503 Batch10 Loss: 0.7250\n",
      "Epoch1503 Batch20 Loss: 0.7957\n",
      "Epoch1503 Batch30 Loss: 0.6748\n",
      "Epoch1503 Batch40 Loss: 0.7010\n",
      "Epoch1503 Batch50 Loss: 0.8819\n",
      "Epoch1503 Batch60 Loss: 0.7844\n",
      "Epoch1503 Batch70 Loss: 0.8831\n",
      "shuffling the dataset\n",
      "train Loss: 0.7813\n",
      "Epoch 1504/1999\n",
      "----------\n",
      "Epoch1504 Batch0 Loss: 0.7391\n",
      "Epoch1504 Batch10 Loss: 1.6405\n",
      "Epoch1504 Batch20 Loss: 0.9269\n",
      "Epoch1504 Batch30 Loss: 0.7288\n",
      "Epoch1504 Batch40 Loss: 0.8245\n",
      "Epoch1504 Batch50 Loss: 0.8781\n",
      "Epoch1504 Batch60 Loss: 0.6149\n",
      "Epoch1504 Batch70 Loss: 0.5589\n",
      "shuffling the dataset\n",
      "train Loss: 0.7797\n",
      "Epoch 1505/1999\n",
      "----------\n",
      "Epoch1505 Batch0 Loss: 0.6917\n",
      "Epoch1505 Batch10 Loss: 1.2458\n",
      "Epoch1505 Batch20 Loss: 0.8840\n",
      "Epoch1505 Batch30 Loss: 0.6692\n",
      "Epoch1505 Batch40 Loss: 0.7056\n",
      "Epoch1505 Batch50 Loss: 0.6623\n",
      "Epoch1505 Batch60 Loss: 0.9670\n",
      "Epoch1505 Batch70 Loss: 0.9934\n",
      "shuffling the dataset\n",
      "train Loss: 0.8236\n",
      "Epoch 1506/1999\n",
      "----------\n",
      "Epoch1506 Batch0 Loss: 0.7091\n",
      "Epoch1506 Batch10 Loss: 0.7075\n",
      "Epoch1506 Batch20 Loss: 0.9187\n",
      "Epoch1506 Batch30 Loss: 0.7772\n",
      "Epoch1506 Batch40 Loss: 0.8095\n",
      "Epoch1506 Batch50 Loss: 0.8946\n",
      "Epoch1506 Batch60 Loss: 0.7549\n",
      "Epoch1506 Batch70 Loss: 0.6653\n",
      "shuffling the dataset\n",
      "train Loss: 0.9019\n",
      "Epoch 1507/1999\n",
      "----------\n",
      "Epoch1507 Batch0 Loss: 0.5541\n",
      "Epoch1507 Batch10 Loss: 0.6926\n",
      "Epoch1507 Batch20 Loss: 0.7245\n",
      "Epoch1507 Batch30 Loss: 0.8750\n",
      "Epoch1507 Batch40 Loss: 0.7669\n",
      "Epoch1507 Batch50 Loss: 0.7239\n",
      "Epoch1507 Batch60 Loss: 0.8343\n",
      "Epoch1507 Batch70 Loss: 0.5478\n",
      "shuffling the dataset\n",
      "train Loss: 0.8393\n",
      "Epoch 1508/1999\n",
      "----------\n",
      "Epoch1508 Batch0 Loss: 0.6955\n",
      "Epoch1508 Batch10 Loss: 0.5259\n",
      "Epoch1508 Batch20 Loss: 0.6908\n",
      "Epoch1508 Batch30 Loss: 0.6553\n",
      "Epoch1508 Batch40 Loss: 3.8817\n"
     ]
    }
   ],
   "source": [
    "###################### Train Model #############################\n",
    "# Calculate total iter_num\n",
    "total_iter_num = 98000\n",
    "\n",
    "for epoch in range(1305, 1305+1500):\n",
    "    print('Epoch {}/{}'.format(epoch, opt.num_epochs+1500 - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Iterate over data.\n",
    "    for i in range(int(dataloader.size()/opt.batchSize)):\n",
    "        total_iter_num += 1\n",
    "        \n",
    "        # Get data\n",
    "        inputs, labels =  dataloader.get_batch()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #ToDo: get labels into correct format\n",
    "        \n",
    "        ## Create Graph ##\n",
    "        if graph_created == False:\n",
    "            graph_created = True\n",
    "            writer.add_graph(model, inputs, verbose=False)\n",
    "        \n",
    "        # Forward + Backward Prop\n",
    "        optimizer.zero_grad()\n",
    "        torch.set_grad_enabled(True)\n",
    "        normal_vectors = model(inputs)\n",
    "        normal_vectors_norm = nn.functional.normalize(normal_vectors, p=2, dim=1)\n",
    "        \n",
    "        loss = loss_fn(normal_vectors_norm, labels, reduction='elementwise_mean')\n",
    "        ### Scale Loss by a factor of 100 ###\n",
    "        loss = loss*100\n",
    "        #####################################\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar('loss', loss.item(), total_iter_num)\n",
    "        \n",
    "        if (i % 10 == 0):\n",
    "            print('Epoch{} Batch{} Loss: {:.4f}'.format(epoch, i, loss.item()))\n",
    "\n",
    "    #exp_lr_scheduler.step() # This is for the LR Scheduler\n",
    "    epoch_loss = running_loss / (dataloader.size()/opt.batchSize)\n",
    "    writer.add_scalar('epoch_loss', epoch_loss, epoch)\n",
    "    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    directory = opt.logs_path+'/checkpoints/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    if (epoch % 5 == 0):\n",
    "        filename = opt.logs_path + '/checkpoints/checkpoint-epoch_{}.pth'.format(epoch,i)\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        \n",
    "\n",
    "# Save final Checkpoint\n",
    "filename = opt.logs_path + '/checkpoints/checkpoint.pth'\n",
    "torch.save(model.state_dict(), filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
