{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from data_loader import Dataset,Options\n",
    "import models.unet_normals as unet\n",
    "from tensorboardX import SummaryWriter\n",
    "# import OpenEXR, Imath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Options\n",
    "Set the various parameters:\n",
    "- dataroot: The folder where the training data is stored\n",
    "- file_list: List of filenames of images for training\n",
    "- batchSize: Batch size for model\n",
    "- shuffle: If true, will shuffle the dataset\n",
    "- phase: If 'train', then it's in training mode.\n",
    "- num_epochs: Number of epochs to train the model for\n",
    "- imsize: Dimensions of the image (square)\n",
    "- num_classes: Num of classes in the output\n",
    "- gpu: Which GPU device to use\n",
    "- logs_path: The path where the log files (tensorboard) will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPT():\n",
    "    def __init__(self):\n",
    "        self.dataroot = './data/'\n",
    "        self.file_list = './data/datalist'\n",
    "        self.batchSize = 32\n",
    "        self.shuffle = True\n",
    "        self.phase = 'train'\n",
    "        self.num_epochs = 1000\n",
    "        self.imsize = 224\n",
    "        self.num_classes = int(3)\n",
    "        self.gpu = '0'\n",
    "        self.logs_path = 'logs/exp9'\n",
    "        self.use_pretrained = False\n",
    "\n",
    "opt = OPT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling the dataset\n"
     ]
    }
   ],
   "source": [
    "###################### Options #############################\n",
    "phase = opt.phase\n",
    "device = torch.device(\"cuda:\"+ opt.gpu if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###################### TensorBoardX #############################\n",
    "if os.path.exists(opt.logs_path):\n",
    "    raise Exception('The folder \\\"{}\\\" already exists! Define a new log path or delete old contents.'.format(opt.logs_path))\n",
    "    \n",
    "writer = SummaryWriter(opt.logs_path, comment='create-graph')\n",
    "graph_created = False\n",
    "\n",
    "###################### DataLoader #############################\n",
    "dataloader = Dataset(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "We use a UNet model. The last few layers of this model are modified to return a 3 channel image, containing the x,y,z values of surface normal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### ModelBuilder #############################\n",
    "model = unet.Unet(num_classes=opt.num_classes)\n",
    "\n",
    "# Load weights from checkpoint\n",
    "if (opt.use_pretrained == True):\n",
    "    checkpoint_path = 'logs/exp7/checkpoints/checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "###################### Setup Optimazation #############################\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "###################### Loss fuction #############################\n",
    "'''\n",
    "@input: The 2 vectors whose cosine loss is to be calculated\n",
    "The dimensions of the matrices are expected to be (batchSize, 3, imsize, imsize). \n",
    "\n",
    "@return: \n",
    "elementwise_mean: will return the sum of all losses divided by num of elements\n",
    "none: The loss will be calculated to be of size (batchSize, imsize, imsize) containing cosine loss of each pixel\n",
    "'''\n",
    "def loss_fn(input_vec, target_vec, reduction='elementwise_mean'):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    loss_val = 1.0 - cos(input_vec, target_vec)\n",
    "    if (reduction=='elementwise_mean'):\n",
    "        return torch.mean(loss_val)\n",
    "    elif (reduction=='none'):\n",
    "        return loss_val\n",
    "    else:\n",
    "        raise Exception('Warning! The reduction is invalid. Please use \\'elementwise_mean\\' or \\'none\\''.format())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/999\n",
      "----------\n",
      "Epoch0 Batch0 Loss: 0.6432\n",
      "Epoch0 Batch10 Loss: 0.2551\n",
      "Epoch0 Batch20 Loss: 0.2353\n",
      "Epoch0 Batch30 Loss: 0.1759\n",
      "Epoch0 Batch40 Loss: 0.1721\n",
      "Epoch0 Batch50 Loss: 0.2334\n",
      "Epoch0 Batch60 Loss: 0.1597\n",
      "Epoch0 Batch70 Loss: 0.1446\n",
      "train Loss: 0.2077\n",
      "Epoch 1/999\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch1 Batch0 Loss: 0.1781\n",
      "Epoch1 Batch10 Loss: 0.1653\n",
      "Epoch1 Batch20 Loss: 0.1393\n",
      "Epoch1 Batch30 Loss: 0.0831\n",
      "Epoch1 Batch40 Loss: 0.1055\n",
      "Epoch1 Batch50 Loss: 0.0872\n",
      "Epoch1 Batch60 Loss: 0.1272\n",
      "Epoch1 Batch70 Loss: 0.1200\n",
      "train Loss: 0.1201\n",
      "Epoch 2/999\n",
      "----------\n",
      "Epoch2 Batch0 Loss: 0.1179\n",
      "shuffling the dataset\n",
      "Epoch2 Batch10 Loss: 0.1460\n",
      "Epoch2 Batch20 Loss: 0.1004\n",
      "Epoch2 Batch30 Loss: 0.0986\n",
      "Epoch2 Batch40 Loss: 0.1218\n",
      "Epoch2 Batch50 Loss: 0.1061\n",
      "Epoch2 Batch60 Loss: 0.0778\n",
      "Epoch2 Batch70 Loss: 0.1125\n",
      "train Loss: 0.1103\n",
      "Epoch 3/999\n",
      "----------\n",
      "Epoch3 Batch0 Loss: 0.1246\n",
      "shuffling the dataset\n",
      "Epoch3 Batch10 Loss: 0.1489\n",
      "Epoch3 Batch20 Loss: 0.0967\n",
      "Epoch3 Batch30 Loss: 0.1139\n",
      "Epoch3 Batch40 Loss: 0.0948\n",
      "Epoch3 Batch50 Loss: 0.0928\n",
      "Epoch3 Batch60 Loss: 0.1114\n",
      "Epoch3 Batch70 Loss: 0.0833\n",
      "train Loss: 0.1084\n",
      "Epoch 4/999\n",
      "----------\n",
      "Epoch4 Batch0 Loss: 0.1183\n",
      "shuffling the dataset\n",
      "Epoch4 Batch10 Loss: 0.0734\n",
      "Epoch4 Batch20 Loss: 0.0779\n",
      "Epoch4 Batch30 Loss: 0.0634\n",
      "Epoch4 Batch40 Loss: 0.1324\n",
      "Epoch4 Batch50 Loss: 0.1180\n",
      "Epoch4 Batch60 Loss: 0.0926\n",
      "Epoch4 Batch70 Loss: 0.1197\n",
      "train Loss: 0.1021\n",
      "Epoch 5/999\n",
      "----------\n",
      "Epoch5 Batch0 Loss: 0.0951\n",
      "shuffling the dataset\n",
      "Epoch5 Batch10 Loss: 0.0645\n",
      "Epoch5 Batch20 Loss: 0.0960\n",
      "Epoch5 Batch30 Loss: 0.0726\n",
      "Epoch5 Batch40 Loss: 0.1218\n",
      "Epoch5 Batch50 Loss: 0.0748\n",
      "Epoch5 Batch60 Loss: 0.0896\n",
      "Epoch5 Batch70 Loss: 0.0855\n",
      "train Loss: 0.1016\n",
      "Epoch 6/999\n",
      "----------\n",
      "Epoch6 Batch0 Loss: 0.0635\n",
      "shuffling the dataset\n",
      "Epoch6 Batch10 Loss: 0.1182\n",
      "Epoch6 Batch20 Loss: 0.1321\n",
      "Epoch6 Batch30 Loss: 0.0873\n",
      "Epoch6 Batch40 Loss: 0.0673\n",
      "Epoch6 Batch50 Loss: 0.0974\n",
      "Epoch6 Batch60 Loss: 0.1271\n"
     ]
    }
   ],
   "source": [
    "###################### Train Model #############################\n",
    "# Calculate total iter_num\n",
    "total_iter_num = 0\n",
    "\n",
    "for epoch in range(opt.num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, opt.num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Iterate over data.\n",
    "    for i in range(int(dataloader.size()/opt.batchSize)):\n",
    "        total_iter_num += 1\n",
    "        \n",
    "        # Get data\n",
    "        inputs, labels =  dataloader.get_batch()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #ToDo: get labels into correct format\n",
    "        \n",
    "        ## Create Graph ##\n",
    "        if graph_created == False:\n",
    "            graph_created = True\n",
    "            writer.add_graph(model, inputs, verbose=False)\n",
    "        \n",
    "        # Forward + Backward Prop\n",
    "        optimizer.zero_grad()\n",
    "        torch.set_grad_enabled(True)\n",
    "        normal_vectors = model(inputs)\n",
    "        normal_vectors_norm = nn.functional.normalize(normal_vectors, p=2, dim=1)\n",
    "        \n",
    "        loss = loss_fn(normal_vectors_norm, labels, reduction='elementwise_mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar('loss', loss.item(), total_iter_num)\n",
    "        \n",
    "        if (i % 10 == 0):\n",
    "            print('Epoch{} Batch{} Loss: {:.4f}'.format(epoch, i, loss.item()))\n",
    "\n",
    "    epoch_loss = running_loss / (dataloader.size()/opt.batchSize)\n",
    "    writer.add_scalar('epoch_loss', epoch_loss, epoch)\n",
    "    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    directory = opt.logs_path+'/checkpoints/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    if (epoch % 25 == 0):\n",
    "        filename = opt.logs_path + '/checkpoints/checkpoint-epoch_{}.pth'.format(epoch,i)\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        \n",
    "        filename = opt.logs_path + '/checkpoints/checkpoint.pth'\n",
    "        torch.save(model.state_dict(), filename)\n",
    "\n",
    "# Save final Checkpoint\n",
    "filename = opt.logs_path + '/checkpoints/checkpoint.pth'\n",
    "torch.save(model.state_dict(), filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
