{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from data_loader import Dataset,Options\n",
    "import models.unet_normals as unet\n",
    "from tensorboardX import SummaryWriter\n",
    "# import OpenEXR, Imath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Options\n",
    "Set the various parameters:\n",
    "- dataroot: The folder where the training data is stored\n",
    "- file_list: List of filenames of images for training\n",
    "- batchSize: Batch size for model\n",
    "- shuffle: If true, will shuffle the dataset\n",
    "- phase: If 'train', then it's in training mode.\n",
    "- num_epochs: Number of epochs to train the model for\n",
    "- imsize: Dimensions of the image (square)\n",
    "- num_classes: Num of classes in the output\n",
    "- gpu: Which GPU device to use\n",
    "- logs_path: The path where the log files (tensorboard) will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPT():\n",
    "    def __init__(self):\n",
    "        self.dataroot = './data/'\n",
    "        self.file_list = './data/datalist'\n",
    "        self.batchSize = 32\n",
    "        self.shuffle = True\n",
    "        self.phase = 'train'\n",
    "        self.num_epochs = 500\n",
    "        self.imsize = 224\n",
    "        self.num_classes = int(3)\n",
    "        self.gpu = '0'\n",
    "        self.logs_path = 'logs/exp10'\n",
    "        self.use_pretrained = False\n",
    "\n",
    "opt = OPT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling the dataset\n"
     ]
    }
   ],
   "source": [
    "###################### Options #############################\n",
    "phase = opt.phase\n",
    "device = torch.device(\"cuda:\"+ opt.gpu if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###################### TensorBoardX #############################\n",
    "if os.path.exists(opt.logs_path):\n",
    "    raise Exception('The folder \\\"{}\\\" already exists! Define a new log path or delete old contents.'.format(opt.logs_path))\n",
    "    \n",
    "writer = SummaryWriter(opt.logs_path, comment='create-graph')\n",
    "graph_created = False\n",
    "\n",
    "###################### DataLoader #############################\n",
    "dataloader = Dataset(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "We use a UNet model. The last few layers of this model are modified to return a 3 channel image, containing the x,y,z values of surface normal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### ModelBuilder #############################\n",
    "model = unet.Unet(num_classes=opt.num_classes)\n",
    "\n",
    "# Load weights from checkpoint\n",
    "if (opt.use_pretrained == True):\n",
    "    checkpoint_path = 'logs/exp7/checkpoints/checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "###################### Setup Optimazation #############################\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "###################### Loss fuction #############################\n",
    "'''\n",
    "@input: The 2 vectors whose cosine loss is to be calculated\n",
    "The dimensions of the matrices are expected to be (batchSize, 3, imsize, imsize). \n",
    "\n",
    "@return: \n",
    "elementwise_mean: will return the sum of all losses divided by num of elements\n",
    "none: The loss will be calculated to be of size (batchSize, imsize, imsize) containing cosine loss of each pixel\n",
    "'''\n",
    "def loss_fn(input_vec, target_vec, reduction='elementwise_mean'):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    loss_val = 1.0 - cos(input_vec, target_vec)\n",
    "    if (reduction=='elementwise_mean'):\n",
    "        return torch.mean(loss_val)\n",
    "    elif (reduction=='none'):\n",
    "        return loss_val\n",
    "    else:\n",
    "        raise Exception('Warning! The reduction is invalid. Please use \\'elementwise_mean\\' or \\'none\\''.format())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "Epoch0 Batch0 Loss: 0.4729\n",
      "Epoch0 Batch10 Loss: 0.2546\n",
      "Epoch0 Batch20 Loss: 0.2095\n",
      "Epoch0 Batch30 Loss: 0.2140\n",
      "Epoch0 Batch40 Loss: 0.2456\n",
      "Epoch0 Batch50 Loss: 0.1891\n",
      "Epoch0 Batch60 Loss: 0.2391\n",
      "Epoch0 Batch70 Loss: 0.2504\n",
      "shuffling the dataset\n",
      "train Loss: 0.2354\n",
      "Epoch 1/499\n",
      "----------\n",
      "Epoch1 Batch0 Loss: 0.2073\n",
      "Epoch1 Batch10 Loss: 0.2040\n",
      "Epoch1 Batch20 Loss: 0.2815\n",
      "Epoch1 Batch30 Loss: 0.1884\n",
      "Epoch1 Batch40 Loss: 0.1900\n",
      "Epoch1 Batch50 Loss: 0.2387\n",
      "Epoch1 Batch60 Loss: 0.2338\n",
      "Epoch1 Batch70 Loss: 0.2166\n",
      "shuffling the dataset\n",
      "train Loss: 0.2220\n",
      "Epoch 2/499\n",
      "----------\n",
      "Epoch2 Batch0 Loss: 0.2118\n",
      "Epoch2 Batch10 Loss: 0.2367\n",
      "Epoch2 Batch20 Loss: 0.2001\n",
      "Epoch2 Batch30 Loss: 0.2276\n",
      "Epoch2 Batch40 Loss: 0.2048\n",
      "Epoch2 Batch50 Loss: 0.1665\n",
      "Epoch2 Batch60 Loss: 0.1885\n",
      "Epoch2 Batch70 Loss: 0.1945\n",
      "shuffling the dataset\n",
      "train Loss: 0.2162\n",
      "Epoch 3/499\n",
      "----------\n",
      "Epoch3 Batch0 Loss: 0.2504\n",
      "Epoch3 Batch10 Loss: 0.2218\n",
      "Epoch3 Batch20 Loss: 0.2444\n",
      "Epoch3 Batch30 Loss: 0.2307\n",
      "Epoch3 Batch40 Loss: 0.2567\n",
      "Epoch3 Batch50 Loss: 0.1972\n",
      "Epoch3 Batch60 Loss: 0.2004\n",
      "Epoch3 Batch70 Loss: 0.2459\n",
      "shuffling the dataset\n",
      "train Loss: 0.2126\n",
      "Epoch 4/499\n",
      "----------\n",
      "Epoch4 Batch0 Loss: 0.2290\n",
      "Epoch4 Batch10 Loss: 0.2442\n",
      "Epoch4 Batch20 Loss: 0.1879\n",
      "Epoch4 Batch30 Loss: 0.1960\n",
      "Epoch4 Batch40 Loss: 0.1984\n",
      "Epoch4 Batch50 Loss: 0.2128\n",
      "Epoch4 Batch60 Loss: 0.2412\n",
      "Epoch4 Batch70 Loss: 0.1698\n",
      "shuffling the dataset\n",
      "train Loss: 0.2097\n",
      "Epoch 5/499\n",
      "----------\n",
      "Epoch5 Batch0 Loss: 0.2000\n",
      "Epoch5 Batch10 Loss: 0.2085\n",
      "Epoch5 Batch20 Loss: 0.1695\n",
      "Epoch5 Batch30 Loss: 0.2112\n",
      "Epoch5 Batch40 Loss: 0.2153\n",
      "Epoch5 Batch50 Loss: 0.1894\n",
      "Epoch5 Batch60 Loss: 0.2488\n",
      "Epoch5 Batch70 Loss: 0.1946\n",
      "shuffling the dataset\n",
      "train Loss: 0.2083\n",
      "Epoch 6/499\n",
      "----------\n",
      "Epoch6 Batch0 Loss: 0.2044\n",
      "Epoch6 Batch10 Loss: 0.2383\n",
      "Epoch6 Batch20 Loss: 0.1635\n",
      "Epoch6 Batch30 Loss: 0.1888\n",
      "Epoch6 Batch40 Loss: 0.1983\n",
      "Epoch6 Batch50 Loss: 0.1700\n",
      "Epoch6 Batch60 Loss: 0.1981\n",
      "Epoch6 Batch70 Loss: 0.1817\n",
      "shuffling the dataset\n",
      "train Loss: 0.2058\n",
      "Epoch 7/499\n",
      "----------\n",
      "Epoch7 Batch0 Loss: 0.2478\n",
      "Epoch7 Batch10 Loss: 0.1710\n",
      "Epoch7 Batch20 Loss: 0.2452\n",
      "Epoch7 Batch30 Loss: 0.1567\n",
      "Epoch7 Batch40 Loss: 0.2268\n",
      "Epoch7 Batch50 Loss: 0.2077\n",
      "Epoch7 Batch60 Loss: 0.2376\n",
      "Epoch7 Batch70 Loss: 0.2218\n",
      "shuffling the dataset\n",
      "train Loss: 0.2042\n",
      "Epoch 8/499\n",
      "----------\n",
      "Epoch8 Batch0 Loss: 0.2030\n",
      "Epoch8 Batch10 Loss: 0.2089\n",
      "Epoch8 Batch20 Loss: 0.1850\n",
      "Epoch8 Batch30 Loss: 0.2370\n",
      "Epoch8 Batch40 Loss: 0.2241\n",
      "Epoch8 Batch50 Loss: 0.1973\n",
      "Epoch8 Batch60 Loss: 0.2010\n",
      "Epoch8 Batch70 Loss: 0.2015\n",
      "shuffling the dataset\n",
      "train Loss: 0.2025\n",
      "Epoch 9/499\n",
      "----------\n",
      "Epoch9 Batch0 Loss: 0.1961\n",
      "Epoch9 Batch10 Loss: 0.2304\n",
      "Epoch9 Batch20 Loss: 0.1995\n",
      "Epoch9 Batch30 Loss: 0.1729\n",
      "Epoch9 Batch40 Loss: 0.2039\n",
      "Epoch9 Batch50 Loss: 0.1848\n",
      "Epoch9 Batch60 Loss: 0.1807\n",
      "Epoch9 Batch70 Loss: 0.1716\n",
      "shuffling the dataset\n",
      "train Loss: 0.1983\n",
      "Epoch 10/499\n",
      "----------\n",
      "Epoch10 Batch0 Loss: 0.2247\n",
      "Epoch10 Batch10 Loss: 0.1671\n",
      "Epoch10 Batch20 Loss: 0.2414\n",
      "Epoch10 Batch30 Loss: 0.2177\n",
      "Epoch10 Batch40 Loss: 0.1482\n",
      "Epoch10 Batch50 Loss: 0.1365\n",
      "Epoch10 Batch60 Loss: 0.2243\n",
      "Epoch10 Batch70 Loss: 0.2045\n",
      "shuffling the dataset\n",
      "train Loss: 0.1968\n",
      "Epoch 11/499\n",
      "----------\n",
      "Epoch11 Batch0 Loss: 0.2104\n",
      "Epoch11 Batch10 Loss: 0.2145\n",
      "Epoch11 Batch20 Loss: 0.2048\n",
      "Epoch11 Batch30 Loss: 0.2142\n",
      "Epoch11 Batch40 Loss: 0.2056\n",
      "Epoch11 Batch50 Loss: 0.1779\n",
      "Epoch11 Batch60 Loss: 0.1903\n",
      "Epoch11 Batch70 Loss: 0.2444\n",
      "shuffling the dataset\n",
      "train Loss: 0.1939\n",
      "Epoch 12/499\n",
      "----------\n",
      "Epoch12 Batch0 Loss: 0.1920\n",
      "Epoch12 Batch10 Loss: 0.1566\n",
      "Epoch12 Batch20 Loss: 0.2274\n",
      "Epoch12 Batch30 Loss: 0.1908\n",
      "Epoch12 Batch40 Loss: 0.2105\n",
      "Epoch12 Batch50 Loss: 0.2182\n",
      "Epoch12 Batch60 Loss: 0.1999\n",
      "Epoch12 Batch70 Loss: 0.1838\n",
      "shuffling the dataset\n",
      "train Loss: 0.1912\n",
      "Epoch 13/499\n",
      "----------\n",
      "Epoch13 Batch0 Loss: 0.2053\n",
      "Epoch13 Batch10 Loss: 0.1815\n",
      "Epoch13 Batch20 Loss: 0.2095\n",
      "Epoch13 Batch30 Loss: 0.1899\n",
      "Epoch13 Batch40 Loss: 0.1734\n",
      "Epoch13 Batch50 Loss: 0.1753\n",
      "Epoch13 Batch60 Loss: 0.1716\n",
      "Epoch13 Batch70 Loss: 0.1821\n",
      "shuffling the dataset\n",
      "train Loss: 0.1882\n",
      "Epoch 14/499\n",
      "----------\n",
      "Epoch14 Batch0 Loss: 0.1873\n",
      "Epoch14 Batch10 Loss: 0.1819\n",
      "Epoch14 Batch20 Loss: 0.2246\n",
      "Epoch14 Batch30 Loss: 0.1870\n",
      "Epoch14 Batch40 Loss: 0.1599\n",
      "Epoch14 Batch50 Loss: 0.1702\n",
      "Epoch14 Batch60 Loss: 0.1802\n",
      "Epoch14 Batch70 Loss: 0.1776\n",
      "shuffling the dataset\n",
      "train Loss: 0.1851\n",
      "Epoch 15/499\n",
      "----------\n",
      "Epoch15 Batch0 Loss: 0.1489\n",
      "Epoch15 Batch10 Loss: 0.1965\n",
      "Epoch15 Batch20 Loss: 0.1697\n",
      "Epoch15 Batch30 Loss: 0.1650\n",
      "Epoch15 Batch40 Loss: 0.1983\n",
      "Epoch15 Batch50 Loss: 0.1581\n",
      "Epoch15 Batch60 Loss: 0.1732\n",
      "Epoch15 Batch70 Loss: 0.1850\n",
      "shuffling the dataset\n",
      "train Loss: 0.1823\n",
      "Epoch 16/499\n",
      "----------\n",
      "Epoch16 Batch0 Loss: 0.1901\n",
      "Epoch16 Batch10 Loss: 0.1911\n",
      "Epoch16 Batch20 Loss: 0.1777\n",
      "Epoch16 Batch30 Loss: 0.1799\n",
      "Epoch16 Batch40 Loss: 0.2413\n",
      "Epoch16 Batch50 Loss: 0.1372\n",
      "Epoch16 Batch60 Loss: 0.1861\n",
      "Epoch16 Batch70 Loss: 0.1689\n",
      "shuffling the dataset\n",
      "train Loss: 0.1800\n",
      "Epoch 17/499\n",
      "----------\n",
      "Epoch17 Batch0 Loss: 0.2080\n",
      "Epoch17 Batch10 Loss: 0.1938\n",
      "Epoch17 Batch20 Loss: 0.1615\n",
      "Epoch17 Batch30 Loss: 0.2262\n",
      "Epoch17 Batch40 Loss: 0.2127\n",
      "Epoch17 Batch50 Loss: 0.1334\n",
      "Epoch17 Batch60 Loss: 0.1709\n",
      "Epoch17 Batch70 Loss: 0.1676\n",
      "shuffling the dataset\n",
      "train Loss: 0.1757\n",
      "Epoch 18/499\n",
      "----------\n",
      "Epoch18 Batch0 Loss: 0.1973\n",
      "Epoch18 Batch10 Loss: 0.1665\n",
      "Epoch18 Batch20 Loss: 0.1984\n",
      "Epoch18 Batch30 Loss: 0.1682\n",
      "Epoch18 Batch40 Loss: 0.1431\n",
      "Epoch18 Batch50 Loss: 0.1691\n",
      "Epoch18 Batch60 Loss: 0.2268\n",
      "Epoch18 Batch70 Loss: 0.1973\n",
      "shuffling the dataset\n",
      "train Loss: 0.1714\n",
      "Epoch 19/499\n",
      "----------\n",
      "Epoch19 Batch0 Loss: 0.1944\n",
      "Epoch19 Batch10 Loss: 0.1466\n",
      "Epoch19 Batch20 Loss: 0.1661\n",
      "Epoch19 Batch30 Loss: 0.1773\n",
      "Epoch19 Batch40 Loss: 0.1617\n",
      "Epoch19 Batch50 Loss: 0.1744\n",
      "Epoch19 Batch60 Loss: 0.1507\n",
      "Epoch19 Batch70 Loss: 0.1759\n",
      "shuffling the dataset\n",
      "train Loss: 0.1693\n",
      "Epoch 20/499\n",
      "----------\n",
      "Epoch20 Batch0 Loss: 0.1575\n",
      "Epoch20 Batch10 Loss: 0.1259\n",
      "Epoch20 Batch20 Loss: 0.1230\n",
      "Epoch20 Batch30 Loss: 0.1812\n",
      "Epoch20 Batch40 Loss: 0.1596\n",
      "Epoch20 Batch50 Loss: 0.1418\n",
      "Epoch20 Batch60 Loss: 0.1505\n",
      "Epoch20 Batch70 Loss: 0.1828\n",
      "shuffling the dataset\n",
      "train Loss: 0.1678\n",
      "Epoch 21/499\n",
      "----------\n",
      "Epoch21 Batch0 Loss: 0.1513\n",
      "Epoch21 Batch10 Loss: 0.2043\n",
      "Epoch21 Batch20 Loss: 0.1370\n",
      "Epoch21 Batch30 Loss: 0.1428\n",
      "Epoch21 Batch40 Loss: 0.1180\n",
      "Epoch21 Batch50 Loss: 0.1533\n",
      "Epoch21 Batch60 Loss: 0.1617\n",
      "Epoch21 Batch70 Loss: 0.1481\n",
      "shuffling the dataset\n",
      "train Loss: 0.1620\n",
      "Epoch 22/499\n",
      "----------\n",
      "Epoch22 Batch0 Loss: 0.2023\n",
      "Epoch22 Batch10 Loss: 0.1409\n",
      "Epoch22 Batch20 Loss: 0.1570\n",
      "Epoch22 Batch30 Loss: 0.1787\n",
      "Epoch22 Batch40 Loss: 0.1540\n",
      "Epoch22 Batch50 Loss: 0.1667\n",
      "Epoch22 Batch60 Loss: 0.1406\n",
      "Epoch22 Batch70 Loss: 0.1602\n",
      "shuffling the dataset\n",
      "train Loss: 0.1566\n",
      "Epoch 23/499\n",
      "----------\n",
      "Epoch23 Batch0 Loss: 0.1744\n",
      "Epoch23 Batch10 Loss: 0.1496\n",
      "Epoch23 Batch20 Loss: 0.1610\n",
      "Epoch23 Batch30 Loss: 0.1366\n",
      "Epoch23 Batch40 Loss: 0.1656\n",
      "Epoch23 Batch50 Loss: 0.1821\n",
      "Epoch23 Batch60 Loss: 0.1585\n",
      "Epoch23 Batch70 Loss: 0.1607\n",
      "shuffling the dataset\n",
      "train Loss: 0.1578\n",
      "Epoch 24/499\n",
      "----------\n",
      "Epoch24 Batch0 Loss: 0.1157\n",
      "Epoch24 Batch10 Loss: 0.2002\n",
      "Epoch24 Batch20 Loss: 0.1777\n",
      "Epoch24 Batch30 Loss: 0.1541\n",
      "Epoch24 Batch40 Loss: 0.1240\n",
      "Epoch24 Batch50 Loss: 0.1186\n",
      "Epoch24 Batch60 Loss: 0.1876\n",
      "Epoch24 Batch70 Loss: 0.1217\n",
      "shuffling the dataset\n",
      "train Loss: 0.1515\n",
      "Epoch 25/499\n",
      "----------\n",
      "Epoch25 Batch0 Loss: 0.1724\n",
      "Epoch25 Batch10 Loss: 0.1660\n",
      "Epoch25 Batch20 Loss: 0.1328\n",
      "Epoch25 Batch30 Loss: 0.1214\n",
      "Epoch25 Batch40 Loss: 0.1394\n",
      "Epoch25 Batch50 Loss: 0.1439\n",
      "Epoch25 Batch60 Loss: 0.1829\n",
      "Epoch25 Batch70 Loss: 0.1684\n",
      "shuffling the dataset\n",
      "train Loss: 0.1460\n",
      "Epoch 26/499\n",
      "----------\n",
      "Epoch26 Batch0 Loss: 0.1467\n",
      "Epoch26 Batch10 Loss: 0.1178\n",
      "Epoch26 Batch20 Loss: 0.1645\n",
      "Epoch26 Batch30 Loss: 0.1900\n",
      "Epoch26 Batch40 Loss: 0.1189\n",
      "Epoch26 Batch50 Loss: 0.1722\n",
      "Epoch26 Batch60 Loss: 0.1327\n",
      "Epoch26 Batch70 Loss: 0.1557\n",
      "shuffling the dataset\n",
      "train Loss: 0.1450\n",
      "Epoch 27/499\n",
      "----------\n",
      "Epoch27 Batch0 Loss: 0.1164\n",
      "Epoch27 Batch10 Loss: 0.1953\n",
      "Epoch27 Batch20 Loss: 0.1286\n",
      "Epoch27 Batch30 Loss: 0.1197\n",
      "Epoch27 Batch40 Loss: 0.1645\n",
      "Epoch27 Batch50 Loss: 0.2041\n",
      "Epoch27 Batch60 Loss: 0.1705\n",
      "Epoch27 Batch70 Loss: 0.1724\n",
      "shuffling the dataset\n",
      "train Loss: 0.1391\n",
      "Epoch 28/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch28 Batch0 Loss: 0.1104\n",
      "Epoch28 Batch10 Loss: 0.1445\n",
      "Epoch28 Batch20 Loss: 0.1147\n",
      "Epoch28 Batch30 Loss: 0.1155\n",
      "Epoch28 Batch40 Loss: 0.1237\n",
      "Epoch28 Batch50 Loss: 0.0910\n",
      "Epoch28 Batch60 Loss: 0.1342\n",
      "Epoch28 Batch70 Loss: 0.1288\n",
      "shuffling the dataset\n",
      "train Loss: 0.1361\n",
      "Epoch 29/499\n",
      "----------\n",
      "Epoch29 Batch0 Loss: 0.1254\n",
      "Epoch29 Batch10 Loss: 0.1216\n",
      "Epoch29 Batch20 Loss: 0.1255\n",
      "Epoch29 Batch30 Loss: 0.1349\n",
      "Epoch29 Batch40 Loss: 0.1655\n",
      "Epoch29 Batch50 Loss: 0.1442\n",
      "Epoch29 Batch60 Loss: 0.1627\n",
      "Epoch29 Batch70 Loss: 0.1153\n",
      "shuffling the dataset\n",
      "train Loss: 0.1359\n",
      "Epoch 30/499\n",
      "----------\n",
      "Epoch30 Batch0 Loss: 0.1064\n",
      "Epoch30 Batch10 Loss: 0.1067\n",
      "Epoch30 Batch20 Loss: 0.1171\n",
      "Epoch30 Batch30 Loss: 0.1306\n",
      "Epoch30 Batch40 Loss: 0.1185\n",
      "Epoch30 Batch50 Loss: 0.1420\n",
      "Epoch30 Batch60 Loss: 0.1224\n",
      "Epoch30 Batch70 Loss: 0.1361\n",
      "shuffling the dataset\n",
      "train Loss: 0.1302\n",
      "Epoch 31/499\n",
      "----------\n",
      "Epoch31 Batch0 Loss: 0.1540\n",
      "Epoch31 Batch10 Loss: 0.1107\n",
      "Epoch31 Batch20 Loss: 0.1267\n",
      "Epoch31 Batch30 Loss: 0.1547\n",
      "Epoch31 Batch40 Loss: 0.1208\n",
      "Epoch31 Batch50 Loss: 0.1253\n",
      "Epoch31 Batch60 Loss: 0.1568\n",
      "Epoch31 Batch70 Loss: 0.1160\n",
      "shuffling the dataset\n",
      "train Loss: 0.1284\n",
      "Epoch 32/499\n",
      "----------\n",
      "Epoch32 Batch0 Loss: 0.0983\n",
      "Epoch32 Batch10 Loss: 0.1682\n",
      "Epoch32 Batch20 Loss: 0.1237\n",
      "Epoch32 Batch30 Loss: 0.1198\n",
      "Epoch32 Batch40 Loss: 0.1125\n",
      "Epoch32 Batch50 Loss: 0.1190\n",
      "Epoch32 Batch60 Loss: 0.1561\n",
      "Epoch32 Batch70 Loss: 0.1249\n",
      "shuffling the dataset\n",
      "train Loss: 0.1238\n",
      "Epoch 33/499\n",
      "----------\n",
      "Epoch33 Batch0 Loss: 0.1052\n",
      "Epoch33 Batch10 Loss: 0.0908\n",
      "Epoch33 Batch20 Loss: 0.1338\n",
      "Epoch33 Batch30 Loss: 0.1225\n",
      "Epoch33 Batch40 Loss: 0.1247\n",
      "Epoch33 Batch50 Loss: 0.1108\n",
      "Epoch33 Batch60 Loss: 0.1149\n",
      "Epoch33 Batch70 Loss: 0.1007\n",
      "shuffling the dataset\n",
      "train Loss: 0.1197\n",
      "Epoch 34/499\n",
      "----------\n",
      "Epoch34 Batch0 Loss: 0.1101\n",
      "Epoch34 Batch10 Loss: 0.1102\n",
      "Epoch34 Batch20 Loss: 0.0993\n",
      "Epoch34 Batch30 Loss: 0.1061\n",
      "Epoch34 Batch40 Loss: 0.1022\n",
      "Epoch34 Batch50 Loss: 0.0894\n",
      "Epoch34 Batch60 Loss: 0.1291\n",
      "Epoch34 Batch70 Loss: 0.1072\n",
      "shuffling the dataset\n",
      "train Loss: 0.1147\n",
      "Epoch 35/499\n",
      "----------\n",
      "Epoch35 Batch0 Loss: 0.1009\n",
      "Epoch35 Batch10 Loss: 0.1250\n",
      "Epoch35 Batch20 Loss: 0.1081\n",
      "Epoch35 Batch30 Loss: 0.0911\n",
      "Epoch35 Batch40 Loss: 0.0989\n",
      "Epoch35 Batch50 Loss: 0.1034\n",
      "Epoch35 Batch60 Loss: 0.1205\n",
      "Epoch35 Batch70 Loss: 0.1095\n",
      "shuffling the dataset\n",
      "train Loss: 0.1111\n",
      "Epoch 36/499\n",
      "----------\n",
      "Epoch36 Batch0 Loss: 0.1239\n",
      "Epoch36 Batch10 Loss: 0.0881\n",
      "Epoch36 Batch20 Loss: 0.0991\n",
      "Epoch36 Batch30 Loss: 0.0873\n",
      "Epoch36 Batch40 Loss: 0.1168\n",
      "Epoch36 Batch50 Loss: 0.1007\n",
      "Epoch36 Batch60 Loss: 0.1107\n",
      "Epoch36 Batch70 Loss: 0.1295\n",
      "shuffling the dataset\n",
      "train Loss: 0.1078\n",
      "Epoch 37/499\n",
      "----------\n",
      "Epoch37 Batch0 Loss: 0.1147\n",
      "Epoch37 Batch10 Loss: 0.1316\n",
      "Epoch37 Batch20 Loss: 0.0895\n",
      "Epoch37 Batch30 Loss: 0.0969\n",
      "Epoch37 Batch40 Loss: 0.0819\n",
      "Epoch37 Batch50 Loss: 0.0996\n",
      "Epoch37 Batch60 Loss: 0.1212\n",
      "Epoch37 Batch70 Loss: 0.1010\n",
      "shuffling the dataset\n",
      "train Loss: 0.1047\n",
      "Epoch 38/499\n",
      "----------\n",
      "Epoch38 Batch0 Loss: 0.0863\n",
      "Epoch38 Batch10 Loss: 0.0952\n",
      "Epoch38 Batch20 Loss: 0.0836\n",
      "Epoch38 Batch30 Loss: 0.1002\n",
      "Epoch38 Batch40 Loss: 0.0976\n",
      "Epoch38 Batch50 Loss: 0.1079\n",
      "Epoch38 Batch60 Loss: 0.1134\n",
      "Epoch38 Batch70 Loss: 0.0879\n",
      "shuffling the dataset\n",
      "train Loss: 0.0992\n",
      "Epoch 39/499\n",
      "----------\n",
      "Epoch39 Batch0 Loss: 0.0974\n",
      "Epoch39 Batch10 Loss: 0.0824\n",
      "Epoch39 Batch20 Loss: 0.0811\n",
      "Epoch39 Batch30 Loss: 0.0890\n",
      "Epoch39 Batch40 Loss: 0.0992\n",
      "Epoch39 Batch50 Loss: 0.1008\n",
      "Epoch39 Batch60 Loss: 0.1176\n",
      "Epoch39 Batch70 Loss: 0.0858\n",
      "shuffling the dataset\n",
      "train Loss: 0.0953\n",
      "Epoch 40/499\n",
      "----------\n",
      "Epoch40 Batch0 Loss: 0.0914\n",
      "Epoch40 Batch10 Loss: 0.0940\n",
      "Epoch40 Batch20 Loss: 0.0841\n",
      "Epoch40 Batch30 Loss: 0.1253\n",
      "Epoch40 Batch40 Loss: 0.0790\n",
      "Epoch40 Batch50 Loss: 0.0801\n",
      "Epoch40 Batch60 Loss: 0.0789\n",
      "Epoch40 Batch70 Loss: 0.1098\n",
      "shuffling the dataset\n",
      "train Loss: 0.0916\n",
      "Epoch 41/499\n",
      "----------\n",
      "Epoch41 Batch0 Loss: 0.0824\n",
      "Epoch41 Batch10 Loss: 0.0841\n",
      "Epoch41 Batch20 Loss: 0.0693\n",
      "Epoch41 Batch30 Loss: 0.0790\n",
      "Epoch41 Batch40 Loss: 0.0883\n",
      "Epoch41 Batch50 Loss: 0.0809\n",
      "Epoch41 Batch60 Loss: 0.0823\n",
      "Epoch41 Batch70 Loss: 0.0872\n",
      "shuffling the dataset\n",
      "train Loss: 0.0889\n",
      "Epoch 42/499\n",
      "----------\n",
      "Epoch42 Batch0 Loss: 0.0679\n",
      "Epoch42 Batch10 Loss: 0.0820\n",
      "Epoch42 Batch20 Loss: 0.0877\n",
      "Epoch42 Batch30 Loss: 0.0875\n",
      "Epoch42 Batch40 Loss: 0.0734\n",
      "Epoch42 Batch50 Loss: 0.0954\n",
      "Epoch42 Batch60 Loss: 0.0898\n",
      "Epoch42 Batch70 Loss: 0.0904\n",
      "shuffling the dataset\n",
      "train Loss: 0.0851\n",
      "Epoch 43/499\n",
      "----------\n",
      "Epoch43 Batch0 Loss: 0.0988\n",
      "Epoch43 Batch10 Loss: 0.0689\n",
      "Epoch43 Batch20 Loss: 0.0668\n",
      "Epoch43 Batch30 Loss: 0.0832\n",
      "Epoch43 Batch40 Loss: 0.0973\n",
      "Epoch43 Batch50 Loss: 0.0815\n",
      "Epoch43 Batch60 Loss: 0.0719\n",
      "Epoch43 Batch70 Loss: 0.0800\n",
      "shuffling the dataset\n",
      "train Loss: 0.0815\n",
      "Epoch 44/499\n",
      "----------\n",
      "Epoch44 Batch0 Loss: 0.0852\n",
      "Epoch44 Batch10 Loss: 0.0675\n",
      "Epoch44 Batch20 Loss: 0.0760\n",
      "Epoch44 Batch30 Loss: 0.0763\n",
      "Epoch44 Batch40 Loss: 0.0881\n",
      "Epoch44 Batch50 Loss: 0.0766\n",
      "Epoch44 Batch60 Loss: 0.0759\n",
      "Epoch44 Batch70 Loss: 0.0558\n",
      "shuffling the dataset\n",
      "train Loss: 0.0772\n",
      "Epoch 45/499\n",
      "----------\n",
      "Epoch45 Batch0 Loss: 0.0638\n",
      "Epoch45 Batch10 Loss: 0.0847\n",
      "Epoch45 Batch20 Loss: 0.0719\n",
      "Epoch45 Batch30 Loss: 0.0777\n",
      "Epoch45 Batch40 Loss: 0.0618\n",
      "Epoch45 Batch50 Loss: 0.0740\n",
      "Epoch45 Batch60 Loss: 0.0595\n",
      "Epoch45 Batch70 Loss: 0.0711\n",
      "shuffling the dataset\n",
      "train Loss: 0.0752\n",
      "Epoch 46/499\n",
      "----------\n",
      "Epoch46 Batch0 Loss: 0.0877\n",
      "Epoch46 Batch10 Loss: 0.1130\n",
      "Epoch46 Batch20 Loss: 0.0850\n",
      "Epoch46 Batch30 Loss: 0.0826\n",
      "Epoch46 Batch40 Loss: 0.0693\n",
      "Epoch46 Batch50 Loss: 0.0687\n",
      "Epoch46 Batch60 Loss: 0.0877\n",
      "Epoch46 Batch70 Loss: 0.0629\n",
      "shuffling the dataset\n",
      "train Loss: 0.0740\n",
      "Epoch 47/499\n",
      "----------\n",
      "Epoch47 Batch0 Loss: 0.0665\n",
      "Epoch47 Batch10 Loss: 0.0655\n",
      "Epoch47 Batch20 Loss: 0.0578\n",
      "Epoch47 Batch30 Loss: 0.0486\n",
      "Epoch47 Batch40 Loss: 0.0629\n",
      "Epoch47 Batch50 Loss: 0.0744\n",
      "Epoch47 Batch60 Loss: 0.0835\n",
      "Epoch47 Batch70 Loss: 0.0644\n",
      "shuffling the dataset\n",
      "train Loss: 0.0692\n",
      "Epoch 48/499\n",
      "----------\n",
      "Epoch48 Batch0 Loss: 0.0595\n",
      "Epoch48 Batch10 Loss: 0.0618\n",
      "Epoch48 Batch20 Loss: 0.0735\n",
      "Epoch48 Batch30 Loss: 0.0825\n",
      "Epoch48 Batch40 Loss: 0.0662\n",
      "Epoch48 Batch50 Loss: 0.0503\n",
      "Epoch48 Batch60 Loss: 0.0540\n",
      "Epoch48 Batch70 Loss: 0.0969\n",
      "shuffling the dataset\n",
      "train Loss: 0.0657\n",
      "Epoch 49/499\n",
      "----------\n",
      "Epoch49 Batch0 Loss: 0.0543\n",
      "Epoch49 Batch10 Loss: 0.0677\n",
      "Epoch49 Batch20 Loss: 0.0626\n",
      "Epoch49 Batch30 Loss: 0.0593\n",
      "Epoch49 Batch40 Loss: 0.0548\n",
      "Epoch49 Batch50 Loss: 0.0500\n",
      "Epoch49 Batch60 Loss: 0.0604\n",
      "Epoch49 Batch70 Loss: 0.0667\n",
      "shuffling the dataset\n",
      "train Loss: 0.0629\n",
      "Epoch 50/499\n",
      "----------\n",
      "Epoch50 Batch0 Loss: 0.0718\n",
      "Epoch50 Batch10 Loss: 0.0467\n",
      "Epoch50 Batch20 Loss: 0.0504\n",
      "Epoch50 Batch30 Loss: 0.0639\n",
      "Epoch50 Batch40 Loss: 0.0526\n",
      "Epoch50 Batch50 Loss: 0.0580\n",
      "Epoch50 Batch60 Loss: 0.0545\n",
      "Epoch50 Batch70 Loss: 0.0733\n",
      "shuffling the dataset\n",
      "train Loss: 0.0606\n",
      "Epoch 51/499\n",
      "----------\n",
      "Epoch51 Batch0 Loss: 0.0707\n",
      "Epoch51 Batch10 Loss: 0.0630\n",
      "Epoch51 Batch20 Loss: 0.0603\n",
      "Epoch51 Batch30 Loss: 0.0641\n",
      "Epoch51 Batch40 Loss: 0.0574\n",
      "Epoch51 Batch50 Loss: 0.0564\n",
      "Epoch51 Batch60 Loss: 0.0678\n",
      "Epoch51 Batch70 Loss: 0.0531\n",
      "shuffling the dataset\n",
      "train Loss: 0.0595\n",
      "Epoch 52/499\n",
      "----------\n",
      "Epoch52 Batch0 Loss: 0.0582\n",
      "Epoch52 Batch10 Loss: 0.0570\n",
      "Epoch52 Batch20 Loss: 0.0641\n",
      "Epoch52 Batch30 Loss: 0.0592\n",
      "Epoch52 Batch40 Loss: 0.0582\n",
      "Epoch52 Batch50 Loss: 0.0584\n",
      "Epoch52 Batch60 Loss: 0.0561\n",
      "Epoch52 Batch70 Loss: 0.0605\n",
      "shuffling the dataset\n",
      "train Loss: 0.0583\n",
      "Epoch 53/499\n",
      "----------\n",
      "Epoch53 Batch0 Loss: 0.0688\n",
      "Epoch53 Batch10 Loss: 0.0539\n",
      "Epoch53 Batch20 Loss: 0.0571\n",
      "Epoch53 Batch30 Loss: 0.0517\n",
      "Epoch53 Batch40 Loss: 0.0550\n",
      "Epoch53 Batch50 Loss: 0.0725\n",
      "Epoch53 Batch60 Loss: 0.0451\n",
      "Epoch53 Batch70 Loss: 0.0498\n",
      "shuffling the dataset\n",
      "train Loss: 0.0573\n",
      "Epoch 54/499\n",
      "----------\n",
      "Epoch54 Batch0 Loss: 0.0448\n",
      "Epoch54 Batch10 Loss: 0.0579\n",
      "Epoch54 Batch20 Loss: 0.0548\n",
      "Epoch54 Batch30 Loss: 0.0522\n",
      "Epoch54 Batch40 Loss: 0.0655\n",
      "Epoch54 Batch50 Loss: 0.0548\n",
      "Epoch54 Batch60 Loss: 0.0529\n",
      "Epoch54 Batch70 Loss: 0.0506\n",
      "shuffling the dataset\n",
      "train Loss: 0.0545\n",
      "Epoch 55/499\n",
      "----------\n",
      "Epoch55 Batch0 Loss: 0.0505\n",
      "Epoch55 Batch10 Loss: 0.0549\n",
      "Epoch55 Batch20 Loss: 0.0569\n",
      "Epoch55 Batch30 Loss: 0.0472\n",
      "Epoch55 Batch40 Loss: 0.0573\n",
      "Epoch55 Batch50 Loss: 0.0482\n",
      "Epoch55 Batch60 Loss: 0.0480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch55 Batch70 Loss: 0.0560\n",
      "shuffling the dataset\n",
      "train Loss: 0.0516\n",
      "Epoch 56/499\n",
      "----------\n",
      "Epoch56 Batch0 Loss: 0.0509\n",
      "Epoch56 Batch10 Loss: 0.0509\n",
      "Epoch56 Batch20 Loss: 0.0508\n",
      "Epoch56 Batch30 Loss: 0.0444\n",
      "Epoch56 Batch40 Loss: 0.0497\n",
      "Epoch56 Batch50 Loss: 0.0521\n",
      "Epoch56 Batch60 Loss: 0.0495\n",
      "Epoch56 Batch70 Loss: 0.0459\n",
      "shuffling the dataset\n",
      "train Loss: 0.0511\n",
      "Epoch 57/499\n",
      "----------\n",
      "Epoch57 Batch0 Loss: 0.0475\n",
      "Epoch57 Batch10 Loss: 0.0504\n",
      "Epoch57 Batch20 Loss: 0.0457\n",
      "Epoch57 Batch30 Loss: 0.0474\n",
      "Epoch57 Batch40 Loss: 0.0438\n",
      "Epoch57 Batch50 Loss: 0.0471\n",
      "Epoch57 Batch60 Loss: 0.0511\n",
      "Epoch57 Batch70 Loss: 0.0429\n",
      "shuffling the dataset\n",
      "train Loss: 0.0494\n",
      "Epoch 58/499\n",
      "----------\n",
      "Epoch58 Batch0 Loss: 0.0585\n",
      "Epoch58 Batch10 Loss: 0.0550\n",
      "Epoch58 Batch20 Loss: 0.0452\n",
      "Epoch58 Batch30 Loss: 0.0434\n",
      "Epoch58 Batch40 Loss: 0.0455\n",
      "Epoch58 Batch50 Loss: 0.0448\n",
      "Epoch58 Batch60 Loss: 0.0453\n",
      "Epoch58 Batch70 Loss: 0.0456\n",
      "shuffling the dataset\n",
      "train Loss: 0.0473\n",
      "Epoch 59/499\n",
      "----------\n",
      "Epoch59 Batch0 Loss: 0.0431\n",
      "Epoch59 Batch10 Loss: 0.0473\n",
      "Epoch59 Batch20 Loss: 0.0511\n",
      "Epoch59 Batch30 Loss: 0.0392\n",
      "Epoch59 Batch40 Loss: 0.0351\n",
      "Epoch59 Batch50 Loss: 0.0459\n",
      "Epoch59 Batch60 Loss: 0.0392\n",
      "Epoch59 Batch70 Loss: 0.0457\n",
      "shuffling the dataset\n",
      "train Loss: 0.0456\n",
      "Epoch 60/499\n",
      "----------\n",
      "Epoch60 Batch0 Loss: 0.0482\n",
      "Epoch60 Batch10 Loss: 0.0504\n",
      "Epoch60 Batch20 Loss: 0.0476\n",
      "Epoch60 Batch30 Loss: 0.0417\n",
      "Epoch60 Batch40 Loss: 0.0412\n",
      "Epoch60 Batch50 Loss: 0.0273\n",
      "Epoch60 Batch60 Loss: 0.0453\n",
      "Epoch60 Batch70 Loss: 0.0450\n",
      "shuffling the dataset\n",
      "train Loss: 0.0447\n",
      "Epoch 61/499\n",
      "----------\n",
      "Epoch61 Batch0 Loss: 0.0444\n",
      "Epoch61 Batch10 Loss: 0.0437\n",
      "Epoch61 Batch20 Loss: 0.0437\n",
      "Epoch61 Batch30 Loss: 0.0378\n",
      "Epoch61 Batch40 Loss: 0.0462\n",
      "Epoch61 Batch50 Loss: 0.0344\n",
      "Epoch61 Batch60 Loss: 0.0387\n",
      "Epoch61 Batch70 Loss: 0.0375\n",
      "shuffling the dataset\n",
      "train Loss: 0.0432\n",
      "Epoch 62/499\n",
      "----------\n",
      "Epoch62 Batch0 Loss: 0.0370\n",
      "Epoch62 Batch10 Loss: 0.0395\n",
      "Epoch62 Batch20 Loss: 0.0426\n",
      "Epoch62 Batch30 Loss: 0.0479\n",
      "Epoch62 Batch40 Loss: 0.0458\n",
      "Epoch62 Batch50 Loss: 0.0442\n",
      "Epoch62 Batch60 Loss: 0.0392\n",
      "Epoch62 Batch70 Loss: 0.0439\n",
      "shuffling the dataset\n",
      "train Loss: 0.0428\n",
      "Epoch 63/499\n",
      "----------\n",
      "Epoch63 Batch0 Loss: 0.0349\n",
      "Epoch63 Batch10 Loss: 0.0361\n",
      "Epoch63 Batch20 Loss: 0.0428\n",
      "Epoch63 Batch30 Loss: 0.0428\n",
      "Epoch63 Batch40 Loss: 0.0494\n",
      "Epoch63 Batch50 Loss: 0.0467\n",
      "Epoch63 Batch60 Loss: 0.0360\n",
      "Epoch63 Batch70 Loss: 0.0647\n",
      "shuffling the dataset\n",
      "train Loss: 0.0415\n",
      "Epoch 64/499\n",
      "----------\n",
      "Epoch64 Batch0 Loss: 0.0387\n",
      "Epoch64 Batch10 Loss: 0.0481\n",
      "Epoch64 Batch20 Loss: 0.0355\n",
      "Epoch64 Batch30 Loss: 0.0459\n",
      "Epoch64 Batch40 Loss: 0.0342\n",
      "Epoch64 Batch50 Loss: 0.0432\n",
      "Epoch64 Batch60 Loss: 0.0384\n",
      "Epoch64 Batch70 Loss: 0.0455\n",
      "shuffling the dataset\n",
      "train Loss: 0.0396\n",
      "Epoch 65/499\n",
      "----------\n",
      "Epoch65 Batch0 Loss: 0.0450\n",
      "Epoch65 Batch10 Loss: 0.0328\n",
      "Epoch65 Batch20 Loss: 0.0326\n",
      "Epoch65 Batch30 Loss: 0.0368\n",
      "Epoch65 Batch40 Loss: 0.0326\n",
      "Epoch65 Batch50 Loss: 0.0342\n",
      "Epoch65 Batch60 Loss: 0.0333\n",
      "Epoch65 Batch70 Loss: 0.0315\n",
      "shuffling the dataset\n",
      "train Loss: 0.0400\n",
      "Epoch 66/499\n",
      "----------\n",
      "Epoch66 Batch0 Loss: 0.0340\n",
      "Epoch66 Batch10 Loss: 0.0280\n",
      "Epoch66 Batch20 Loss: 0.0307\n",
      "Epoch66 Batch30 Loss: 0.0323\n",
      "Epoch66 Batch40 Loss: 0.0396\n",
      "Epoch66 Batch50 Loss: 0.0344\n",
      "Epoch66 Batch60 Loss: 0.0406\n",
      "Epoch66 Batch70 Loss: 0.0348\n",
      "shuffling the dataset\n",
      "train Loss: 0.0382\n",
      "Epoch 67/499\n",
      "----------\n",
      "Epoch67 Batch0 Loss: 0.0395\n",
      "Epoch67 Batch10 Loss: 0.0352\n",
      "Epoch67 Batch20 Loss: 0.0357\n",
      "Epoch67 Batch30 Loss: 0.0411\n",
      "Epoch67 Batch40 Loss: 0.0447\n",
      "Epoch67 Batch50 Loss: 0.0325\n",
      "Epoch67 Batch60 Loss: 0.0461\n",
      "Epoch67 Batch70 Loss: 0.0315\n",
      "shuffling the dataset\n",
      "train Loss: 0.0373\n",
      "Epoch 68/499\n",
      "----------\n",
      "Epoch68 Batch0 Loss: 0.0344\n",
      "Epoch68 Batch10 Loss: 0.0288\n",
      "Epoch68 Batch20 Loss: 0.0497\n",
      "Epoch68 Batch30 Loss: 0.0521\n",
      "Epoch68 Batch40 Loss: 0.0422\n",
      "Epoch68 Batch50 Loss: 0.0493\n",
      "Epoch68 Batch60 Loss: 0.0895\n",
      "Epoch68 Batch70 Loss: 0.0597\n",
      "shuffling the dataset\n",
      "train Loss: 0.0476\n",
      "Epoch 69/499\n",
      "----------\n",
      "Epoch69 Batch0 Loss: 0.0609\n",
      "Epoch69 Batch10 Loss: 0.0480\n",
      "Epoch69 Batch20 Loss: 0.0491\n",
      "Epoch69 Batch30 Loss: 0.0507\n",
      "Epoch69 Batch40 Loss: 0.0479\n",
      "Epoch69 Batch50 Loss: 0.0600\n",
      "Epoch69 Batch60 Loss: 0.0488\n",
      "Epoch69 Batch70 Loss: 0.0594\n",
      "shuffling the dataset\n",
      "train Loss: 0.0550\n",
      "Epoch 70/499\n",
      "----------\n",
      "Epoch70 Batch0 Loss: 0.0496\n",
      "Epoch70 Batch10 Loss: 0.0510\n",
      "Epoch70 Batch20 Loss: 0.0512\n",
      "Epoch70 Batch30 Loss: 0.0479\n",
      "Epoch70 Batch40 Loss: 0.0379\n",
      "Epoch70 Batch50 Loss: 0.0470\n",
      "Epoch70 Batch60 Loss: 0.0365\n",
      "Epoch70 Batch70 Loss: 0.0396\n",
      "shuffling the dataset\n",
      "train Loss: 0.0446\n",
      "Epoch 71/499\n",
      "----------\n",
      "Epoch71 Batch0 Loss: 0.0415\n",
      "Epoch71 Batch10 Loss: 0.0308\n",
      "Epoch71 Batch20 Loss: 0.0302\n",
      "Epoch71 Batch30 Loss: 0.0377\n",
      "Epoch71 Batch40 Loss: 0.0333\n",
      "Epoch71 Batch50 Loss: 0.0351\n",
      "Epoch71 Batch60 Loss: 0.0414\n",
      "Epoch71 Batch70 Loss: 0.0336\n",
      "shuffling the dataset\n",
      "train Loss: 0.0375\n",
      "Epoch 72/499\n",
      "----------\n",
      "Epoch72 Batch0 Loss: 0.0383\n",
      "Epoch72 Batch10 Loss: 0.0448\n",
      "Epoch72 Batch20 Loss: 0.0336\n",
      "Epoch72 Batch30 Loss: 0.0484\n",
      "Epoch72 Batch40 Loss: 0.0354\n",
      "Epoch72 Batch50 Loss: 0.0333\n",
      "Epoch72 Batch60 Loss: 0.0357\n",
      "Epoch72 Batch70 Loss: 0.0320\n",
      "shuffling the dataset\n",
      "train Loss: 0.0346\n",
      "Epoch 73/499\n",
      "----------\n",
      "Epoch73 Batch0 Loss: 0.0326\n",
      "Epoch73 Batch10 Loss: 0.0282\n",
      "Epoch73 Batch20 Loss: 0.0342\n",
      "Epoch73 Batch30 Loss: 0.0350\n",
      "Epoch73 Batch40 Loss: 0.0642\n",
      "Epoch73 Batch50 Loss: 0.0289\n",
      "Epoch73 Batch60 Loss: 0.0308\n",
      "Epoch73 Batch70 Loss: 0.0365\n",
      "shuffling the dataset\n",
      "train Loss: 0.0328\n",
      "Epoch 74/499\n",
      "----------\n",
      "Epoch74 Batch0 Loss: 0.0369\n",
      "Epoch74 Batch10 Loss: 0.0330\n",
      "Epoch74 Batch20 Loss: 0.0622\n",
      "Epoch74 Batch30 Loss: 0.0259\n",
      "Epoch74 Batch40 Loss: 0.0287\n",
      "Epoch74 Batch50 Loss: 0.0319\n",
      "Epoch74 Batch60 Loss: 0.0303\n",
      "Epoch74 Batch70 Loss: 0.0234\n",
      "shuffling the dataset\n",
      "train Loss: 0.0317\n",
      "Epoch 75/499\n",
      "----------\n",
      "Epoch75 Batch0 Loss: 0.0294\n",
      "Epoch75 Batch10 Loss: 0.0254\n",
      "Epoch75 Batch20 Loss: 0.0313\n",
      "Epoch75 Batch30 Loss: 0.0280\n",
      "Epoch75 Batch40 Loss: 0.0307\n",
      "Epoch75 Batch50 Loss: 0.0413\n",
      "Epoch75 Batch60 Loss: 0.0484\n",
      "Epoch75 Batch70 Loss: 0.0243\n",
      "shuffling the dataset\n",
      "train Loss: 0.0312\n",
      "Epoch 76/499\n",
      "----------\n",
      "Epoch76 Batch0 Loss: 0.0303\n",
      "Epoch76 Batch10 Loss: 0.0328\n",
      "Epoch76 Batch20 Loss: 0.0274\n",
      "Epoch76 Batch30 Loss: 0.0285\n",
      "Epoch76 Batch40 Loss: 0.0292\n",
      "Epoch76 Batch50 Loss: 0.0253\n",
      "Epoch76 Batch60 Loss: 0.0220\n",
      "Epoch76 Batch70 Loss: 0.0324\n",
      "shuffling the dataset\n",
      "train Loss: 0.0305\n",
      "Epoch 77/499\n",
      "----------\n",
      "Epoch77 Batch0 Loss: 0.0279\n",
      "Epoch77 Batch10 Loss: 0.0309\n",
      "Epoch77 Batch20 Loss: 0.0286\n",
      "Epoch77 Batch30 Loss: 0.0336\n",
      "Epoch77 Batch40 Loss: 0.0247\n",
      "Epoch77 Batch50 Loss: 0.0300\n",
      "Epoch77 Batch60 Loss: 0.0338\n",
      "Epoch77 Batch70 Loss: 0.0269\n",
      "shuffling the dataset\n",
      "train Loss: 0.0300\n",
      "Epoch 78/499\n",
      "----------\n",
      "Epoch78 Batch0 Loss: 0.0324\n",
      "Epoch78 Batch10 Loss: 0.0344\n",
      "Epoch78 Batch20 Loss: 0.0285\n",
      "Epoch78 Batch30 Loss: 0.0296\n",
      "Epoch78 Batch40 Loss: 0.0178\n",
      "Epoch78 Batch50 Loss: 0.0279\n",
      "Epoch78 Batch60 Loss: 0.0286\n",
      "Epoch78 Batch70 Loss: 0.0297\n",
      "shuffling the dataset\n",
      "train Loss: 0.0297\n",
      "Epoch 79/499\n",
      "----------\n",
      "Epoch79 Batch0 Loss: 0.0304\n",
      "Epoch79 Batch10 Loss: 0.0330\n",
      "Epoch79 Batch20 Loss: 0.0254\n",
      "Epoch79 Batch30 Loss: 0.0356\n",
      "Epoch79 Batch40 Loss: 0.0292\n",
      "Epoch79 Batch50 Loss: 0.0309\n",
      "Epoch79 Batch60 Loss: 0.0290\n",
      "Epoch79 Batch70 Loss: 0.0265\n",
      "shuffling the dataset\n",
      "train Loss: 0.0297\n",
      "Epoch 80/499\n",
      "----------\n",
      "Epoch80 Batch0 Loss: 0.0256\n",
      "Epoch80 Batch10 Loss: 0.0327\n",
      "Epoch80 Batch20 Loss: 0.0319\n",
      "Epoch80 Batch30 Loss: 0.0188\n",
      "Epoch80 Batch40 Loss: 0.0326\n",
      "Epoch80 Batch50 Loss: 0.0224\n",
      "Epoch80 Batch60 Loss: 0.0275\n",
      "Epoch80 Batch70 Loss: 0.0235\n",
      "shuffling the dataset\n",
      "train Loss: 0.0293\n",
      "Epoch 81/499\n",
      "----------\n",
      "Epoch81 Batch0 Loss: 0.0303\n",
      "Epoch81 Batch10 Loss: 0.0212\n",
      "Epoch81 Batch20 Loss: 0.0258\n",
      "Epoch81 Batch30 Loss: 0.0256\n",
      "Epoch81 Batch40 Loss: 0.0308\n",
      "Epoch81 Batch50 Loss: 0.0527\n",
      "Epoch81 Batch60 Loss: 0.0257\n",
      "Epoch81 Batch70 Loss: 0.0205\n",
      "shuffling the dataset\n",
      "train Loss: 0.0286\n",
      "Epoch 82/499\n",
      "----------\n",
      "Epoch82 Batch0 Loss: 0.0217\n",
      "Epoch82 Batch10 Loss: 0.0228\n",
      "Epoch82 Batch20 Loss: 0.0338\n",
      "Epoch82 Batch30 Loss: 0.0264\n",
      "Epoch82 Batch40 Loss: 0.0280\n",
      "Epoch82 Batch50 Loss: 0.0186\n",
      "Epoch82 Batch60 Loss: 0.0311\n",
      "Epoch82 Batch70 Loss: 0.0294\n",
      "shuffling the dataset\n",
      "train Loss: 0.0281\n",
      "Epoch 83/499\n",
      "----------\n",
      "Epoch83 Batch0 Loss: 0.0216\n",
      "Epoch83 Batch10 Loss: 0.0302\n",
      "Epoch83 Batch20 Loss: 0.0328\n",
      "Epoch83 Batch30 Loss: 0.0200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch83 Batch40 Loss: 0.0243\n",
      "Epoch83 Batch50 Loss: 0.0234\n",
      "Epoch83 Batch60 Loss: 0.0284\n",
      "Epoch83 Batch70 Loss: 0.0333\n",
      "shuffling the dataset\n",
      "train Loss: 0.0278\n",
      "Epoch 84/499\n",
      "----------\n",
      "Epoch84 Batch0 Loss: 0.0304\n",
      "Epoch84 Batch10 Loss: 0.0255\n",
      "Epoch84 Batch20 Loss: 0.0533\n",
      "Epoch84 Batch30 Loss: 0.0348\n",
      "Epoch84 Batch40 Loss: 0.0243\n",
      "Epoch84 Batch50 Loss: 0.0354\n",
      "Epoch84 Batch60 Loss: 0.0290\n",
      "Epoch84 Batch70 Loss: 0.0204\n",
      "shuffling the dataset\n",
      "train Loss: 0.0275\n",
      "Epoch 85/499\n",
      "----------\n",
      "Epoch85 Batch0 Loss: 0.0296\n",
      "Epoch85 Batch10 Loss: 0.0262\n",
      "Epoch85 Batch20 Loss: 0.0272\n",
      "Epoch85 Batch30 Loss: 0.0258\n",
      "Epoch85 Batch40 Loss: 0.0316\n",
      "Epoch85 Batch50 Loss: 0.0292\n",
      "Epoch85 Batch60 Loss: 0.0241\n",
      "Epoch85 Batch70 Loss: 0.0300\n",
      "shuffling the dataset\n",
      "train Loss: 0.0272\n",
      "Epoch 86/499\n",
      "----------\n",
      "Epoch86 Batch0 Loss: 0.0237\n",
      "Epoch86 Batch10 Loss: 0.0287\n",
      "Epoch86 Batch20 Loss: 0.0336\n",
      "Epoch86 Batch30 Loss: 0.0245\n",
      "Epoch86 Batch40 Loss: 0.0301\n",
      "Epoch86 Batch50 Loss: 0.0207\n",
      "Epoch86 Batch60 Loss: 0.0255\n",
      "Epoch86 Batch70 Loss: 0.0274\n",
      "shuffling the dataset\n",
      "train Loss: 0.0269\n",
      "Epoch 87/499\n",
      "----------\n",
      "Epoch87 Batch0 Loss: 0.0249\n",
      "Epoch87 Batch10 Loss: 0.0246\n",
      "Epoch87 Batch20 Loss: 0.0240\n",
      "Epoch87 Batch30 Loss: 0.0291\n",
      "Epoch87 Batch40 Loss: 0.0253\n",
      "Epoch87 Batch50 Loss: 0.0284\n",
      "Epoch87 Batch60 Loss: 0.0266\n",
      "Epoch87 Batch70 Loss: 0.0176\n",
      "shuffling the dataset\n",
      "train Loss: 0.0266\n",
      "Epoch 88/499\n",
      "----------\n",
      "Epoch88 Batch0 Loss: 0.0262\n",
      "Epoch88 Batch10 Loss: 0.0238\n",
      "Epoch88 Batch20 Loss: 0.0263\n",
      "Epoch88 Batch30 Loss: 0.0279\n",
      "Epoch88 Batch40 Loss: 0.0514\n",
      "Epoch88 Batch50 Loss: 0.0243\n",
      "Epoch88 Batch60 Loss: 0.0246\n",
      "Epoch88 Batch70 Loss: 0.0166\n",
      "shuffling the dataset\n",
      "train Loss: 0.0265\n",
      "Epoch 89/499\n",
      "----------\n",
      "Epoch89 Batch0 Loss: 0.0223\n",
      "Epoch89 Batch10 Loss: 0.0267\n",
      "Epoch89 Batch20 Loss: 0.0317\n",
      "Epoch89 Batch30 Loss: 0.0213\n",
      "Epoch89 Batch40 Loss: 0.0299\n",
      "Epoch89 Batch50 Loss: 0.0220\n",
      "Epoch89 Batch60 Loss: 0.0212\n",
      "Epoch89 Batch70 Loss: 0.0210\n",
      "shuffling the dataset\n",
      "train Loss: 0.0257\n",
      "Epoch 90/499\n",
      "----------\n",
      "Epoch90 Batch0 Loss: 0.0204\n",
      "Epoch90 Batch10 Loss: 0.0257\n",
      "Epoch90 Batch20 Loss: 0.0270\n",
      "Epoch90 Batch30 Loss: 0.0266\n",
      "Epoch90 Batch40 Loss: 0.0285\n",
      "Epoch90 Batch50 Loss: 0.0316\n",
      "Epoch90 Batch60 Loss: 0.0289\n",
      "Epoch90 Batch70 Loss: 0.0261\n",
      "shuffling the dataset\n",
      "train Loss: 0.0266\n",
      "Epoch 91/499\n",
      "----------\n",
      "Epoch91 Batch0 Loss: 0.0279\n",
      "Epoch91 Batch10 Loss: 0.0342\n",
      "Epoch91 Batch20 Loss: 0.0248\n",
      "Epoch91 Batch30 Loss: 0.0213\n",
      "Epoch91 Batch40 Loss: 0.0233\n",
      "Epoch91 Batch50 Loss: 0.0268\n",
      "Epoch91 Batch60 Loss: 0.0275\n",
      "Epoch91 Batch70 Loss: 0.0208\n",
      "shuffling the dataset\n",
      "train Loss: 0.0266\n",
      "Epoch 92/499\n",
      "----------\n",
      "Epoch92 Batch0 Loss: 0.0193\n",
      "Epoch92 Batch10 Loss: 0.0185\n",
      "Epoch92 Batch20 Loss: 0.0228\n",
      "Epoch92 Batch30 Loss: 0.0266\n",
      "Epoch92 Batch40 Loss: 0.0274\n",
      "Epoch92 Batch50 Loss: 0.0291\n",
      "Epoch92 Batch60 Loss: 0.0237\n",
      "Epoch92 Batch70 Loss: 0.0217\n",
      "shuffling the dataset\n",
      "train Loss: 0.0250\n",
      "Epoch 93/499\n",
      "----------\n",
      "Epoch93 Batch0 Loss: 0.0205\n",
      "Epoch93 Batch10 Loss: 0.0244\n",
      "Epoch93 Batch20 Loss: 0.0233\n",
      "Epoch93 Batch30 Loss: 0.0222\n",
      "Epoch93 Batch40 Loss: 0.0269\n",
      "Epoch93 Batch50 Loss: 0.0282\n",
      "Epoch93 Batch60 Loss: 0.0234\n",
      "Epoch93 Batch70 Loss: 0.0310\n",
      "shuffling the dataset\n",
      "train Loss: 0.0247\n",
      "Epoch 94/499\n",
      "----------\n",
      "Epoch94 Batch0 Loss: 0.0244\n",
      "Epoch94 Batch10 Loss: 0.0203\n",
      "Epoch94 Batch20 Loss: 0.0257\n",
      "Epoch94 Batch30 Loss: 0.0247\n",
      "Epoch94 Batch40 Loss: 0.0269\n",
      "Epoch94 Batch50 Loss: 0.0207\n",
      "Epoch94 Batch60 Loss: 0.0270\n",
      "Epoch94 Batch70 Loss: 0.0201\n",
      "shuffling the dataset\n",
      "train Loss: 0.0243\n",
      "Epoch 95/499\n",
      "----------\n",
      "Epoch95 Batch0 Loss: 0.0189\n",
      "Epoch95 Batch10 Loss: 0.0218\n",
      "Epoch95 Batch20 Loss: 0.0195\n",
      "Epoch95 Batch30 Loss: 0.0240\n",
      "Epoch95 Batch40 Loss: 0.0206\n",
      "Epoch95 Batch50 Loss: 0.0250\n",
      "Epoch95 Batch60 Loss: 0.0249\n",
      "Epoch95 Batch70 Loss: 0.0258\n",
      "shuffling the dataset\n",
      "train Loss: 0.0242\n",
      "Epoch 96/499\n",
      "----------\n",
      "Epoch96 Batch0 Loss: 0.0227\n",
      "Epoch96 Batch10 Loss: 0.0213\n",
      "Epoch96 Batch20 Loss: 0.0181\n",
      "Epoch96 Batch30 Loss: 0.0198\n",
      "Epoch96 Batch40 Loss: 0.0241\n",
      "Epoch96 Batch50 Loss: 0.0171\n",
      "Epoch96 Batch60 Loss: 0.0199\n",
      "Epoch96 Batch70 Loss: 0.0172\n",
      "shuffling the dataset\n",
      "train Loss: 0.0239\n",
      "Epoch 97/499\n",
      "----------\n",
      "Epoch97 Batch0 Loss: 0.0183\n",
      "Epoch97 Batch10 Loss: 0.0227\n",
      "Epoch97 Batch20 Loss: 0.0227\n",
      "Epoch97 Batch30 Loss: 0.0173\n",
      "Epoch97 Batch40 Loss: 0.0275\n",
      "Epoch97 Batch50 Loss: 0.0522\n",
      "Epoch97 Batch60 Loss: 0.0207\n",
      "Epoch97 Batch70 Loss: 0.0216\n",
      "shuffling the dataset\n",
      "train Loss: 0.0232\n",
      "Epoch 98/499\n",
      "----------\n",
      "Epoch98 Batch0 Loss: 0.0246\n",
      "Epoch98 Batch10 Loss: 0.0249\n",
      "Epoch98 Batch20 Loss: 0.0221\n",
      "Epoch98 Batch30 Loss: 0.0154\n",
      "Epoch98 Batch40 Loss: 0.0196\n",
      "Epoch98 Batch50 Loss: 0.0211\n",
      "Epoch98 Batch60 Loss: 0.0241\n",
      "Epoch98 Batch70 Loss: 0.0207\n",
      "shuffling the dataset\n",
      "train Loss: 0.0229\n",
      "Epoch 99/499\n",
      "----------\n",
      "Epoch99 Batch0 Loss: 0.0220\n",
      "Epoch99 Batch10 Loss: 0.0188\n",
      "Epoch99 Batch20 Loss: 0.0226\n",
      "Epoch99 Batch30 Loss: 0.0188\n",
      "Epoch99 Batch40 Loss: 0.0171\n",
      "Epoch99 Batch50 Loss: 0.0246\n",
      "Epoch99 Batch60 Loss: 0.0261\n",
      "Epoch99 Batch70 Loss: 0.0182\n",
      "shuffling the dataset\n",
      "train Loss: 0.0227\n",
      "Epoch 100/499\n",
      "----------\n",
      "Epoch100 Batch0 Loss: 0.0205\n",
      "Epoch100 Batch10 Loss: 0.0199\n",
      "Epoch100 Batch20 Loss: 0.0251\n",
      "Epoch100 Batch30 Loss: 0.0192\n",
      "Epoch100 Batch40 Loss: 0.0223\n",
      "Epoch100 Batch50 Loss: 0.0232\n",
      "Epoch100 Batch60 Loss: 0.0260\n",
      "Epoch100 Batch70 Loss: 0.0216\n",
      "shuffling the dataset\n",
      "train Loss: 0.0231\n",
      "Epoch 101/499\n",
      "----------\n",
      "Epoch101 Batch0 Loss: 0.0206\n",
      "Epoch101 Batch10 Loss: 0.0334\n",
      "Epoch101 Batch20 Loss: 0.0209\n",
      "Epoch101 Batch30 Loss: 0.0183\n",
      "Epoch101 Batch40 Loss: 0.0253\n",
      "Epoch101 Batch50 Loss: 0.0215\n",
      "Epoch101 Batch60 Loss: 0.0257\n",
      "Epoch101 Batch70 Loss: 0.0202\n",
      "shuffling the dataset\n",
      "train Loss: 0.0229\n",
      "Epoch 102/499\n",
      "----------\n",
      "Epoch102 Batch0 Loss: 0.0339\n",
      "Epoch102 Batch10 Loss: 0.0222\n",
      "Epoch102 Batch20 Loss: 0.0259\n",
      "Epoch102 Batch30 Loss: 0.0246\n",
      "Epoch102 Batch40 Loss: 0.0218\n",
      "Epoch102 Batch50 Loss: 0.0512\n",
      "Epoch102 Batch60 Loss: 0.0206\n",
      "Epoch102 Batch70 Loss: 0.0205\n",
      "shuffling the dataset\n",
      "train Loss: 0.0222\n",
      "Epoch 103/499\n",
      "----------\n",
      "Epoch103 Batch0 Loss: 0.0165\n",
      "Epoch103 Batch10 Loss: 0.0212\n",
      "Epoch103 Batch20 Loss: 0.0204\n",
      "Epoch103 Batch30 Loss: 0.0226\n",
      "Epoch103 Batch40 Loss: 0.0203\n",
      "Epoch103 Batch50 Loss: 0.0211\n",
      "Epoch103 Batch60 Loss: 0.0225\n",
      "Epoch103 Batch70 Loss: 0.0148\n",
      "shuffling the dataset\n",
      "train Loss: 0.0216\n",
      "Epoch 104/499\n",
      "----------\n",
      "Epoch104 Batch0 Loss: 0.0172\n",
      "Epoch104 Batch10 Loss: 0.0245\n",
      "Epoch104 Batch20 Loss: 0.0221\n",
      "Epoch104 Batch30 Loss: 0.0219\n",
      "Epoch104 Batch40 Loss: 0.0259\n",
      "Epoch104 Batch50 Loss: 0.0304\n",
      "Epoch104 Batch60 Loss: 0.0248\n",
      "Epoch104 Batch70 Loss: 0.0264\n",
      "shuffling the dataset\n",
      "train Loss: 0.0236\n",
      "Epoch 105/499\n",
      "----------\n",
      "Epoch105 Batch0 Loss: 0.0229\n",
      "Epoch105 Batch10 Loss: 0.0254\n",
      "Epoch105 Batch20 Loss: 0.0276\n",
      "Epoch105 Batch30 Loss: 0.0347\n",
      "Epoch105 Batch40 Loss: 0.0304\n",
      "Epoch105 Batch50 Loss: 0.0316\n",
      "Epoch105 Batch60 Loss: 0.0387\n",
      "Epoch105 Batch70 Loss: 0.0381\n",
      "shuffling the dataset\n",
      "train Loss: 0.0303\n",
      "Epoch 106/499\n",
      "----------\n",
      "Epoch106 Batch0 Loss: 0.0285\n",
      "Epoch106 Batch10 Loss: 0.0395\n",
      "Epoch106 Batch20 Loss: 0.0488\n",
      "Epoch106 Batch30 Loss: 0.0365\n",
      "Epoch106 Batch40 Loss: 0.0365\n",
      "Epoch106 Batch50 Loss: 0.0477\n",
      "Epoch106 Batch60 Loss: 0.0408\n",
      "Epoch106 Batch70 Loss: 0.0264\n",
      "shuffling the dataset\n",
      "train Loss: 0.0396\n",
      "Epoch 107/499\n",
      "----------\n",
      "Epoch107 Batch0 Loss: 0.0248\n",
      "Epoch107 Batch10 Loss: 0.0305\n",
      "Epoch107 Batch20 Loss: 0.0462\n",
      "Epoch107 Batch30 Loss: 0.0425\n",
      "Epoch107 Batch40 Loss: 0.0369\n",
      "Epoch107 Batch50 Loss: 0.0370\n",
      "Epoch107 Batch60 Loss: 0.0540\n",
      "Epoch107 Batch70 Loss: 0.0421\n",
      "shuffling the dataset\n",
      "train Loss: 0.0398\n",
      "Epoch 108/499\n",
      "----------\n",
      "Epoch108 Batch0 Loss: 0.0411\n",
      "Epoch108 Batch10 Loss: 0.0341\n",
      "Epoch108 Batch20 Loss: 0.0288\n",
      "Epoch108 Batch30 Loss: 0.0299\n",
      "Epoch108 Batch40 Loss: 0.0273\n",
      "Epoch108 Batch50 Loss: 0.0287\n",
      "Epoch108 Batch60 Loss: 0.0213\n",
      "Epoch108 Batch70 Loss: 0.0267\n",
      "shuffling the dataset\n",
      "train Loss: 0.0308\n",
      "Epoch 109/499\n",
      "----------\n",
      "Epoch109 Batch0 Loss: 0.0217\n",
      "Epoch109 Batch10 Loss: 0.0249\n",
      "Epoch109 Batch20 Loss: 0.0309\n",
      "Epoch109 Batch30 Loss: 0.0241\n",
      "Epoch109 Batch40 Loss: 0.0219\n",
      "Epoch109 Batch50 Loss: 0.0236\n",
      "Epoch109 Batch60 Loss: 0.0272\n",
      "Epoch109 Batch70 Loss: 0.0173\n",
      "shuffling the dataset\n",
      "train Loss: 0.0237\n",
      "Epoch 110/499\n",
      "----------\n",
      "Epoch110 Batch0 Loss: 0.0235\n",
      "Epoch110 Batch10 Loss: 0.0190\n",
      "Epoch110 Batch20 Loss: 0.0186\n",
      "Epoch110 Batch30 Loss: 0.0213\n",
      "Epoch110 Batch40 Loss: 0.0190\n",
      "Epoch110 Batch50 Loss: 0.0166\n",
      "Epoch110 Batch60 Loss: 0.0158\n",
      "Epoch110 Batch70 Loss: 0.0224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling the dataset\n",
      "train Loss: 0.0212\n",
      "Epoch 111/499\n",
      "----------\n",
      "Epoch111 Batch0 Loss: 0.0244\n",
      "Epoch111 Batch10 Loss: 0.0262\n",
      "Epoch111 Batch20 Loss: 0.0224\n",
      "Epoch111 Batch30 Loss: 0.0183\n",
      "Epoch111 Batch40 Loss: 0.0160\n",
      "Epoch111 Batch50 Loss: 0.0163\n",
      "Epoch111 Batch60 Loss: 0.0184\n",
      "Epoch111 Batch70 Loss: 0.0224\n",
      "shuffling the dataset\n",
      "train Loss: 0.0218\n",
      "Epoch 112/499\n",
      "----------\n",
      "Epoch112 Batch0 Loss: 0.0143\n",
      "Epoch112 Batch10 Loss: 0.0211\n",
      "Epoch112 Batch20 Loss: 0.0225\n",
      "Epoch112 Batch30 Loss: 0.0195\n",
      "Epoch112 Batch40 Loss: 0.0214\n",
      "Epoch112 Batch50 Loss: 0.0201\n",
      "Epoch112 Batch60 Loss: 0.0209\n",
      "Epoch112 Batch70 Loss: 0.0142\n",
      "shuffling the dataset\n",
      "train Loss: 0.0200\n",
      "Epoch 113/499\n",
      "----------\n",
      "Epoch113 Batch0 Loss: 0.0182\n",
      "Epoch113 Batch10 Loss: 0.0164\n",
      "Epoch113 Batch20 Loss: 0.0194\n",
      "Epoch113 Batch30 Loss: 0.0253\n",
      "Epoch113 Batch40 Loss: 0.0165\n",
      "Epoch113 Batch50 Loss: 0.0197\n",
      "Epoch113 Batch60 Loss: 0.0196\n",
      "Epoch113 Batch70 Loss: 0.0207\n",
      "shuffling the dataset\n",
      "train Loss: 0.0200\n",
      "Epoch 114/499\n",
      "----------\n",
      "Epoch114 Batch0 Loss: 0.0187\n",
      "Epoch114 Batch10 Loss: 0.0152\n",
      "Epoch114 Batch20 Loss: 0.0209\n",
      "Epoch114 Batch30 Loss: 0.0211\n",
      "Epoch114 Batch40 Loss: 0.0172\n",
      "Epoch114 Batch50 Loss: 0.0224\n",
      "Epoch114 Batch60 Loss: 0.0234\n",
      "Epoch114 Batch70 Loss: 0.0163\n",
      "shuffling the dataset\n",
      "train Loss: 0.0192\n",
      "Epoch 115/499\n",
      "----------\n",
      "Epoch115 Batch0 Loss: 0.0185\n",
      "Epoch115 Batch10 Loss: 0.0200\n",
      "Epoch115 Batch20 Loss: 0.0238\n",
      "Epoch115 Batch30 Loss: 0.0158\n",
      "Epoch115 Batch40 Loss: 0.0173\n",
      "Epoch115 Batch50 Loss: 0.0219\n",
      "Epoch115 Batch60 Loss: 0.0185\n",
      "Epoch115 Batch70 Loss: 0.0176\n",
      "shuffling the dataset\n",
      "train Loss: 0.0184\n",
      "Epoch 116/499\n",
      "----------\n",
      "Epoch116 Batch0 Loss: 0.0139\n",
      "Epoch116 Batch10 Loss: 0.0207\n",
      "Epoch116 Batch20 Loss: 0.0181\n",
      "Epoch116 Batch30 Loss: 0.0217\n",
      "Epoch116 Batch40 Loss: 0.0157\n",
      "Epoch116 Batch50 Loss: 0.0193\n",
      "Epoch116 Batch60 Loss: 0.0122\n",
      "Epoch116 Batch70 Loss: 0.0154\n",
      "shuffling the dataset\n",
      "train Loss: 0.0179\n",
      "Epoch 117/499\n",
      "----------\n",
      "Epoch117 Batch0 Loss: 0.0161\n",
      "Epoch117 Batch10 Loss: 0.0208\n",
      "Epoch117 Batch20 Loss: 0.0192\n",
      "Epoch117 Batch30 Loss: 0.0129\n",
      "Epoch117 Batch40 Loss: 0.0221\n",
      "Epoch117 Batch50 Loss: 0.0134\n",
      "Epoch117 Batch60 Loss: 0.0144\n",
      "Epoch117 Batch70 Loss: 0.0179\n",
      "shuffling the dataset\n",
      "train Loss: 0.0179\n",
      "Epoch 118/499\n",
      "----------\n",
      "Epoch118 Batch0 Loss: 0.0148\n",
      "Epoch118 Batch10 Loss: 0.0185\n",
      "Epoch118 Batch20 Loss: 0.0181\n",
      "Epoch118 Batch30 Loss: 0.0165\n",
      "Epoch118 Batch40 Loss: 0.0157\n",
      "Epoch118 Batch50 Loss: 0.0118\n",
      "Epoch118 Batch60 Loss: 0.0128\n",
      "Epoch118 Batch70 Loss: 0.0192\n",
      "shuffling the dataset\n",
      "train Loss: 0.0175\n",
      "Epoch 119/499\n",
      "----------\n",
      "Epoch119 Batch0 Loss: 0.0349\n",
      "Epoch119 Batch10 Loss: 0.0147\n",
      "Epoch119 Batch20 Loss: 0.0179\n",
      "Epoch119 Batch30 Loss: 0.0153\n",
      "Epoch119 Batch40 Loss: 0.0186\n",
      "Epoch119 Batch50 Loss: 0.0174\n",
      "Epoch119 Batch60 Loss: 0.0186\n",
      "Epoch119 Batch70 Loss: 0.0150\n",
      "shuffling the dataset\n",
      "train Loss: 0.0174\n",
      "Epoch 120/499\n",
      "----------\n",
      "Epoch120 Batch0 Loss: 0.0209\n",
      "Epoch120 Batch10 Loss: 0.0151\n",
      "Epoch120 Batch20 Loss: 0.0127\n",
      "Epoch120 Batch30 Loss: 0.0214\n",
      "Epoch120 Batch40 Loss: 0.0138\n",
      "Epoch120 Batch50 Loss: 0.0161\n",
      "Epoch120 Batch60 Loss: 0.0196\n",
      "Epoch120 Batch70 Loss: 0.0145\n",
      "shuffling the dataset\n",
      "train Loss: 0.0173\n",
      "Epoch 121/499\n",
      "----------\n",
      "Epoch121 Batch0 Loss: 0.0244\n",
      "Epoch121 Batch10 Loss: 0.0169\n",
      "Epoch121 Batch20 Loss: 0.0487\n",
      "Epoch121 Batch30 Loss: 0.0187\n",
      "Epoch121 Batch40 Loss: 0.0157\n",
      "Epoch121 Batch50 Loss: 0.0174\n",
      "Epoch121 Batch60 Loss: 0.0134\n",
      "Epoch121 Batch70 Loss: 0.0185\n",
      "shuffling the dataset\n",
      "train Loss: 0.0172\n",
      "Epoch 122/499\n",
      "----------\n",
      "Epoch122 Batch0 Loss: 0.0253\n",
      "Epoch122 Batch10 Loss: 0.0158\n",
      "Epoch122 Batch20 Loss: 0.0149\n",
      "Epoch122 Batch30 Loss: 0.0158\n",
      "Epoch122 Batch40 Loss: 0.0182\n",
      "Epoch122 Batch50 Loss: 0.0198\n",
      "Epoch122 Batch60 Loss: 0.0171\n",
      "Epoch122 Batch70 Loss: 0.0205\n",
      "shuffling the dataset\n",
      "train Loss: 0.0173\n",
      "Epoch 123/499\n",
      "----------\n",
      "Epoch123 Batch0 Loss: 0.0189\n",
      "Epoch123 Batch10 Loss: 0.0151\n",
      "Epoch123 Batch20 Loss: 0.0157\n",
      "Epoch123 Batch30 Loss: 0.0151\n",
      "Epoch123 Batch40 Loss: 0.0145\n",
      "Epoch123 Batch50 Loss: 0.0161\n",
      "Epoch123 Batch60 Loss: 0.0182\n",
      "Epoch123 Batch70 Loss: 0.0135\n",
      "shuffling the dataset\n",
      "train Loss: 0.0172\n",
      "Epoch 124/499\n",
      "----------\n",
      "Epoch124 Batch0 Loss: 0.0191\n",
      "Epoch124 Batch10 Loss: 0.0212\n",
      "Epoch124 Batch20 Loss: 0.0171\n",
      "Epoch124 Batch30 Loss: 0.0164\n",
      "Epoch124 Batch40 Loss: 0.0258\n",
      "Epoch124 Batch50 Loss: 0.0147\n",
      "Epoch124 Batch60 Loss: 0.0185\n",
      "Epoch124 Batch70 Loss: 0.0139\n",
      "shuffling the dataset\n",
      "train Loss: 0.0172\n",
      "Epoch 125/499\n",
      "----------\n",
      "Epoch125 Batch0 Loss: 0.0110\n",
      "Epoch125 Batch10 Loss: 0.0113\n",
      "Epoch125 Batch20 Loss: 0.0160\n",
      "Epoch125 Batch30 Loss: 0.0163\n",
      "Epoch125 Batch40 Loss: 0.0198\n",
      "Epoch125 Batch50 Loss: 0.0185\n",
      "Epoch125 Batch60 Loss: 0.0155\n",
      "Epoch125 Batch70 Loss: 0.0241\n",
      "shuffling the dataset\n",
      "train Loss: 0.0170\n",
      "Epoch 126/499\n",
      "----------\n",
      "Epoch126 Batch0 Loss: 0.0357\n",
      "Epoch126 Batch10 Loss: 0.0165\n",
      "Epoch126 Batch20 Loss: 0.0162\n",
      "Epoch126 Batch30 Loss: 0.0163\n",
      "Epoch126 Batch40 Loss: 0.0167\n",
      "Epoch126 Batch50 Loss: 0.0174\n",
      "Epoch126 Batch60 Loss: 0.0146\n",
      "Epoch126 Batch70 Loss: 0.0164\n",
      "shuffling the dataset\n",
      "train Loss: 0.0172\n",
      "Epoch 127/499\n",
      "----------\n",
      "Epoch127 Batch0 Loss: 0.0151\n",
      "Epoch127 Batch10 Loss: 0.0122\n",
      "Epoch127 Batch20 Loss: 0.0184\n",
      "Epoch127 Batch30 Loss: 0.0125\n",
      "Epoch127 Batch40 Loss: 0.0151\n",
      "Epoch127 Batch50 Loss: 0.0172\n",
      "Epoch127 Batch60 Loss: 0.0226\n",
      "Epoch127 Batch70 Loss: 0.0142\n",
      "shuffling the dataset\n",
      "train Loss: 0.0173\n",
      "Epoch 128/499\n",
      "----------\n",
      "Epoch128 Batch0 Loss: 0.0124\n",
      "Epoch128 Batch10 Loss: 0.0165\n",
      "Epoch128 Batch20 Loss: 0.0159\n",
      "Epoch128 Batch30 Loss: 0.0204\n",
      "Epoch128 Batch40 Loss: 0.0178\n",
      "Epoch128 Batch50 Loss: 0.0167\n",
      "Epoch128 Batch60 Loss: 0.0158\n",
      "Epoch128 Batch70 Loss: 0.0140\n",
      "shuffling the dataset\n",
      "train Loss: 0.0172\n",
      "Epoch 129/499\n",
      "----------\n",
      "Epoch129 Batch0 Loss: 0.0189\n",
      "Epoch129 Batch10 Loss: 0.0135\n",
      "Epoch129 Batch20 Loss: 0.0154\n",
      "Epoch129 Batch30 Loss: 0.0151\n",
      "Epoch129 Batch40 Loss: 0.0137\n",
      "Epoch129 Batch50 Loss: 0.0169\n",
      "Epoch129 Batch60 Loss: 0.0135\n",
      "Epoch129 Batch70 Loss: 0.0375\n",
      "shuffling the dataset\n",
      "train Loss: 0.0171\n",
      "Epoch 130/499\n",
      "----------\n",
      "Epoch130 Batch0 Loss: 0.0152\n",
      "Epoch130 Batch10 Loss: 0.0115\n",
      "Epoch130 Batch20 Loss: 0.0147\n",
      "Epoch130 Batch30 Loss: 0.0115\n",
      "Epoch130 Batch40 Loss: 0.0193\n",
      "Epoch130 Batch50 Loss: 0.0161\n",
      "Epoch130 Batch60 Loss: 0.0098\n",
      "Epoch130 Batch70 Loss: 0.0149\n",
      "shuffling the dataset\n",
      "train Loss: 0.0167\n",
      "Epoch 131/499\n",
      "----------\n",
      "Epoch131 Batch0 Loss: 0.0147\n",
      "Epoch131 Batch10 Loss: 0.0142\n",
      "Epoch131 Batch20 Loss: 0.0153\n",
      "Epoch131 Batch30 Loss: 0.0132\n",
      "Epoch131 Batch40 Loss: 0.0197\n",
      "Epoch131 Batch50 Loss: 0.0151\n",
      "Epoch131 Batch60 Loss: 0.0168\n",
      "Epoch131 Batch70 Loss: 0.0144\n",
      "shuffling the dataset\n",
      "train Loss: 0.0166\n",
      "Epoch 132/499\n",
      "----------\n",
      "Epoch132 Batch0 Loss: 0.0171\n",
      "Epoch132 Batch10 Loss: 0.0241\n",
      "Epoch132 Batch20 Loss: 0.0122\n",
      "Epoch132 Batch30 Loss: 0.0120\n",
      "Epoch132 Batch40 Loss: 0.0131\n",
      "Epoch132 Batch50 Loss: 0.0133\n",
      "Epoch132 Batch60 Loss: 0.0172\n",
      "Epoch132 Batch70 Loss: 0.0146\n",
      "shuffling the dataset\n",
      "train Loss: 0.0163\n",
      "Epoch 133/499\n",
      "----------\n",
      "Epoch133 Batch0 Loss: 0.0162\n",
      "Epoch133 Batch10 Loss: 0.0151\n",
      "Epoch133 Batch20 Loss: 0.0178\n",
      "Epoch133 Batch30 Loss: 0.0105\n",
      "Epoch133 Batch40 Loss: 0.0141\n",
      "Epoch133 Batch50 Loss: 0.0175\n",
      "Epoch133 Batch60 Loss: 0.0138\n",
      "Epoch133 Batch70 Loss: 0.0178\n",
      "shuffling the dataset\n",
      "train Loss: 0.0162\n",
      "Epoch 134/499\n",
      "----------\n",
      "Epoch134 Batch0 Loss: 0.0138\n",
      "Epoch134 Batch10 Loss: 0.0235\n",
      "Epoch134 Batch20 Loss: 0.0166\n",
      "Epoch134 Batch30 Loss: 0.0228\n",
      "Epoch134 Batch40 Loss: 0.0145\n",
      "Epoch134 Batch50 Loss: 0.0158\n",
      "Epoch134 Batch60 Loss: 0.0201\n",
      "Epoch134 Batch70 Loss: 0.0169\n",
      "shuffling the dataset\n",
      "train Loss: 0.0163\n",
      "Epoch 135/499\n",
      "----------\n",
      "Epoch135 Batch0 Loss: 0.0122\n",
      "Epoch135 Batch10 Loss: 0.0169\n",
      "Epoch135 Batch20 Loss: 0.0146\n",
      "Epoch135 Batch30 Loss: 0.0139\n",
      "Epoch135 Batch40 Loss: 0.0170\n",
      "Epoch135 Batch50 Loss: 0.0160\n",
      "Epoch135 Batch60 Loss: 0.0163\n",
      "Epoch135 Batch70 Loss: 0.0158\n",
      "shuffling the dataset\n",
      "train Loss: 0.0165\n",
      "Epoch 136/499\n",
      "----------\n",
      "Epoch136 Batch0 Loss: 0.0363\n",
      "Epoch136 Batch10 Loss: 0.0199\n",
      "Epoch136 Batch20 Loss: 0.0125\n",
      "Epoch136 Batch30 Loss: 0.0179\n",
      "Epoch136 Batch40 Loss: 0.0144\n",
      "Epoch136 Batch50 Loss: 0.0248\n",
      "Epoch136 Batch60 Loss: 0.0133\n",
      "Epoch136 Batch70 Loss: 0.0180\n",
      "shuffling the dataset\n",
      "train Loss: 0.0164\n",
      "Epoch 137/499\n",
      "----------\n",
      "Epoch137 Batch0 Loss: 0.0157\n",
      "Epoch137 Batch10 Loss: 0.0108\n",
      "Epoch137 Batch20 Loss: 0.0158\n",
      "Epoch137 Batch30 Loss: 0.0168\n",
      "Epoch137 Batch40 Loss: 0.0285\n",
      "Epoch137 Batch50 Loss: 0.0166\n"
     ]
    }
   ],
   "source": [
    "###################### Train Model #############################\n",
    "# Calculate total iter_num\n",
    "total_iter_num = 0\n",
    "\n",
    "for epoch in range(opt.num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, opt.num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Iterate over data.\n",
    "    for i in range(int(dataloader.size()/opt.batchSize)):\n",
    "        total_iter_num += 1\n",
    "        \n",
    "        # Get data\n",
    "        inputs, labels =  dataloader.get_batch()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #ToDo: get labels into correct format\n",
    "        \n",
    "        ## Create Graph ##\n",
    "        if graph_created == False:\n",
    "            graph_created = True\n",
    "            writer.add_graph(model, inputs, verbose=False)\n",
    "        \n",
    "        # Forward + Backward Prop\n",
    "        optimizer.zero_grad()\n",
    "        torch.set_grad_enabled(True)\n",
    "        normal_vectors = model(inputs)\n",
    "        normal_vectors_norm = nn.functional.normalize(normal_vectors, p=2, dim=1)\n",
    "        \n",
    "        loss = loss_fn(normal_vectors_norm, labels, reduction='elementwise_mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar('loss', loss.item(), total_iter_num)\n",
    "        \n",
    "        if (i % 10 == 0):\n",
    "            print('Epoch{} Batch{} Loss: {:.4f}'.format(epoch, i, loss.item()))\n",
    "\n",
    "    epoch_loss = running_loss / (dataloader.size()/opt.batchSize)\n",
    "    writer.add_scalar('epoch_loss', epoch_loss, epoch)\n",
    "    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    directory = opt.logs_path+'/checkpoints/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    if (epoch % 5 == 0):\n",
    "        filename = opt.logs_path + '/checkpoints/checkpoint-epoch_{}.pth'.format(epoch,i)\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        \n",
    "\n",
    "# Save final Checkpoint\n",
    "filename = opt.logs_path + '/checkpoints/checkpoint.pth'\n",
    "torch.save(model.state_dict(), filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
