{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from data_loader import Dataset\n",
    "import models.unet_normals as unet\n",
    "from tensorboardX import SummaryWriter\n",
    "# import OpenEXR, Imath\n",
    "import imageio\n",
    "from torchvision.utils import make_grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Options\n",
    "Set the various parameters:\n",
    "- dataroot: The folder where the training data is stored\n",
    "- file_list: List of filenames of images for training\n",
    "- batchSize: Batch size for model\n",
    "- shuffle: If true, will shuffle the dataset\n",
    "- phase: If 'train', then it's in training mode.\n",
    "- num_epochs: Number of epochs to train the model for\n",
    "- imsize: Dimensions of the image (square)\n",
    "- num_classes: Num of classes in the output\n",
    "- gpu: Which GPU device to use\n",
    "- logs_path: The path where the log files (tensorboard) will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPT():\n",
    "    def __init__(self):\n",
    "        self.dataroot = './data/'\n",
    "        self.file_list = './data/datalist'\n",
    "        self.batchSize = 16\n",
    "        self.shuffle = False\n",
    "        self.phase = 'train'\n",
    "        self.num_epochs = 500\n",
    "        self.imsize = (288,512)\n",
    "        self.num_classes = int(2)\n",
    "        self.gpu = '1'\n",
    "        self.logs_path = 'logs/exp3'\n",
    "        self.use_pretrained = False\n",
    "\n",
    "opt = OPT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Options #############################\n",
    "phase = opt.phase\n",
    "device = torch.device(\"cuda:\"+ opt.gpu if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###################### TensorBoardX #############################\n",
    "if os.path.exists(opt.logs_path):\n",
    "    raise Exception('The folder \\\"{}\\\" already exists! Define a new log path or delete old contents.'.format(opt.logs_path))\n",
    "    \n",
    "writer = SummaryWriter(opt.logs_path, comment='create-graph')\n",
    "graph_created = False\n",
    "\n",
    "###################### DataLoader #############################\n",
    "dataloader = Dataset(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "We use a UNet model. The last few layers of this model are modified to return a 3 channel image, containing the x,y,z values of surface normal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### ModelBuilder #############################\n",
    "model = unet.Unet(num_classes=opt.num_classes)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum').to(device)\n",
    "\n",
    "# Load weights from checkpoint\n",
    "if (opt.use_pretrained == True):\n",
    "    checkpoint_path = 'logs/exp1/checkpoints/checkpoint-epoch_405.pth'\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "###################### Setup Optimazation #############################\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "###################### Loss fuction #############################\n",
    "def flatten_logits(logits, number_of_classes):\n",
    "    \"\"\"Flattens the logits batch except for the logits dimension\"\"\"\n",
    "    logits_permuted = logits.permute(0, 2, 3, 1)\n",
    "    logits_permuted_cont = logits_permuted.contiguous()\n",
    "    logits_flatten = logits_permuted_cont.view(-1, number_of_classes)\n",
    "    return logits_flatten\n",
    "\n",
    "def flatten_annotations(annotations):\n",
    "    return annotations.view(-1)\n",
    "\n",
    "def get_valid_annotations_index(flatten_annotations, mask_out_value=255):\n",
    "    return torch.squeeze( torch.nonzero((flatten_annotations != mask_out_value )), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "Epoch0 Batch0 Loss: 1645235.0000\n",
      "Epoch0 Batch2 Loss: 1593438.3750\n",
      "Epoch0 Batch4 Loss: 1528245.8750\n",
      "train Loss: 1510709.6000\n",
      "Epoch 1/499\n",
      "----------\n",
      "Epoch1 Batch0 Loss: 1451585.0000\n",
      "Epoch1 Batch2 Loss: 1367963.8750\n",
      "Epoch1 Batch4 Loss: 1065669.7500\n",
      "train Loss: 1163361.9300\n",
      "Epoch 2/499\n",
      "----------\n",
      "Epoch2 Batch0 Loss: 1144395.8750\n",
      "Epoch2 Batch2 Loss: 644112.4375\n",
      "Epoch2 Batch4 Loss: 949840.5000\n",
      "train Loss: 879449.6300\n",
      "Epoch 3/499\n",
      "----------\n",
      "Epoch3 Batch0 Loss: 802621.6875\n",
      "Epoch3 Batch2 Loss: 829533.9375\n",
      "Epoch3 Batch4 Loss: 838463.4375\n",
      "train Loss: 760407.0700\n",
      "Epoch 4/499\n",
      "----------\n",
      "Epoch4 Batch0 Loss: 606759.1875\n",
      "Epoch4 Batch2 Loss: 615435.8125\n",
      "Epoch4 Batch4 Loss: 941206.5625\n",
      "train Loss: 644992.4000\n",
      "Epoch 5/499\n",
      "----------\n",
      "Epoch5 Batch0 Loss: 602447.8750\n",
      "Epoch5 Batch2 Loss: 677729.1250\n",
      "Epoch5 Batch4 Loss: 739058.6250\n",
      "train Loss: 623530.7100\n",
      "Epoch 6/499\n",
      "----------\n",
      "Epoch6 Batch0 Loss: 574438.6875\n",
      "Epoch6 Batch2 Loss: 675520.1250\n",
      "Epoch6 Batch4 Loss: 662643.3125\n",
      "train Loss: 603131.1000\n",
      "Epoch 7/499\n",
      "----------\n",
      "Epoch7 Batch0 Loss: 649318.8125\n",
      "Epoch7 Batch2 Loss: 664547.5625\n",
      "Epoch7 Batch4 Loss: 593591.6250\n",
      "train Loss: 607692.7900\n",
      "Epoch 8/499\n",
      "----------\n",
      "Epoch8 Batch0 Loss: 614362.5000\n",
      "Epoch8 Batch2 Loss: 615078.2500\n",
      "Epoch8 Batch4 Loss: 573136.2500\n",
      "train Loss: 603844.5400\n",
      "Epoch 9/499\n",
      "----------\n",
      "Epoch9 Batch0 Loss: 565205.8750\n",
      "Epoch9 Batch2 Loss: 549837.1250\n",
      "Epoch9 Batch4 Loss: 546780.0000\n",
      "train Loss: 579369.3300\n",
      "Epoch 10/499\n",
      "----------\n",
      "Epoch10 Batch0 Loss: 690342.1875\n",
      "Epoch10 Batch2 Loss: 536445.4375\n",
      "Epoch10 Batch4 Loss: 549833.6875\n",
      "train Loss: 584817.0600\n",
      "Epoch 11/499\n",
      "----------\n",
      "Epoch11 Batch0 Loss: 723449.4375\n",
      "Epoch11 Batch2 Loss: 516573.4688\n",
      "Epoch11 Batch4 Loss: 550776.5625\n",
      "train Loss: 587131.1650\n",
      "Epoch 12/499\n",
      "----------\n",
      "Epoch12 Batch0 Loss: 795621.3125\n",
      "Epoch12 Batch2 Loss: 562324.5000\n",
      "Epoch12 Batch4 Loss: 551007.0000\n",
      "train Loss: 590484.5300\n",
      "Epoch 13/499\n",
      "----------\n",
      "Epoch13 Batch0 Loss: 824566.3125\n",
      "Epoch13 Batch2 Loss: 583564.2500\n",
      "Epoch13 Batch4 Loss: 646967.4375\n",
      "train Loss: 591785.5400\n",
      "Epoch 14/499\n",
      "----------\n",
      "Epoch14 Batch0 Loss: 717828.5000\n",
      "Epoch14 Batch2 Loss: 555869.3750\n",
      "Epoch14 Batch4 Loss: 662640.9375\n",
      "train Loss: 589810.0400\n",
      "Epoch 15/499\n",
      "----------\n",
      "Epoch15 Batch0 Loss: 650904.3750\n",
      "Epoch15 Batch2 Loss: 637126.0625\n",
      "Epoch15 Batch4 Loss: 647895.6875\n",
      "train Loss: 593837.7800\n",
      "Epoch 16/499\n",
      "----------\n",
      "Epoch16 Batch0 Loss: 584128.3750\n",
      "Epoch16 Batch2 Loss: 598766.3125\n",
      "Epoch16 Batch4 Loss: 608612.0625\n",
      "train Loss: 589520.8000\n",
      "Epoch 17/499\n",
      "----------\n",
      "Epoch17 Batch0 Loss: 564626.5625\n",
      "Epoch17 Batch2 Loss: 558669.0000\n",
      "Epoch17 Batch4 Loss: 537508.6250\n",
      "train Loss: 592246.5200\n",
      "Epoch 18/499\n",
      "----------\n",
      "Epoch18 Batch0 Loss: 534448.0000\n",
      "Epoch18 Batch2 Loss: 676717.4375\n",
      "Epoch18 Batch4 Loss: 521213.4375\n",
      "train Loss: 585806.4000\n",
      "Epoch 19/499\n",
      "----------\n",
      "Epoch19 Batch0 Loss: 534477.9375\n",
      "Epoch19 Batch2 Loss: 709937.2500\n",
      "Epoch19 Batch4 Loss: 511709.4688\n",
      "train Loss: 589463.6150\n",
      "Epoch 20/499\n",
      "----------\n",
      "Epoch20 Batch0 Loss: 542936.6875\n",
      "Epoch20 Batch2 Loss: 777619.7500\n",
      "Epoch20 Batch4 Loss: 555882.4375\n",
      "train Loss: 585602.4000\n",
      "Epoch 21/499\n",
      "----------\n",
      "Epoch21 Batch0 Loss: 548237.9375\n",
      "Epoch21 Batch2 Loss: 807135.9375\n",
      "Epoch21 Batch4 Loss: 580227.2500\n",
      "train Loss: 573703.4400\n",
      "Epoch 22/499\n",
      "----------\n",
      "Epoch22 Batch0 Loss: 643704.1875\n",
      "Epoch22 Batch2 Loss: 711813.3750\n",
      "Epoch22 Batch4 Loss: 552107.1250\n",
      "train Loss: 580269.9900\n",
      "Epoch 23/499\n",
      "----------\n",
      "Epoch23 Batch0 Loss: 657494.5000\n",
      "Epoch23 Batch2 Loss: 649005.5000\n",
      "Epoch23 Batch4 Loss: 629183.5625\n",
      "train Loss: 588931.8350\n",
      "Epoch 24/499\n",
      "----------\n",
      "Epoch24 Batch0 Loss: 638946.2500\n",
      "Epoch24 Batch2 Loss: 580996.3750\n",
      "Epoch24 Batch4 Loss: 590995.2500\n",
      "train Loss: 588280.8300\n",
      "Epoch 25/499\n",
      "----------\n",
      "Epoch25 Batch0 Loss: 600303.1875\n",
      "Epoch25 Batch2 Loss: 557056.9375\n",
      "Epoch25 Batch4 Loss: 552397.1250\n",
      "train Loss: 581465.1500\n",
      "Epoch 26/499\n",
      "----------\n",
      "Epoch26 Batch0 Loss: 526615.2500\n",
      "Epoch26 Batch2 Loss: 527544.6875\n",
      "Epoch26 Batch4 Loss: 665146.5625\n",
      "train Loss: 577815.6300\n",
      "Epoch 27/499\n",
      "----------\n",
      "Epoch27 Batch0 Loss: 510216.6875\n",
      "Epoch27 Batch2 Loss: 526443.1875\n",
      "Epoch27 Batch4 Loss: 696991.8125\n",
      "train Loss: 584104.6800\n",
      "Epoch 28/499\n",
      "----------\n",
      "Epoch28 Batch0 Loss: 504040.5938\n",
      "Epoch28 Batch2 Loss: 536649.3750\n",
      "Epoch28 Batch4 Loss: 767309.0625\n",
      "train Loss: 574788.4250\n",
      "Epoch 29/499\n",
      "----------\n",
      "Epoch29 Batch0 Loss: 542669.7500\n",
      "Epoch29 Batch2 Loss: 537886.3125\n",
      "Epoch29 Batch4 Loss: 793789.0000\n",
      "train Loss: 568720.1100\n",
      "Epoch 30/499\n",
      "----------\n",
      "Epoch30 Batch0 Loss: 562457.8750\n",
      "Epoch30 Batch2 Loss: 625361.3750\n",
      "Epoch30 Batch4 Loss: 692675.1250\n",
      "train Loss: 570469.4050\n",
      "Epoch 31/499\n",
      "----------\n",
      "Epoch31 Batch0 Loss: 531913.9375\n",
      "Epoch31 Batch2 Loss: 629461.0000\n",
      "Epoch31 Batch4 Loss: 632821.3750\n",
      "train Loss: 558104.2250\n",
      "Epoch 32/499\n",
      "----------\n",
      "Epoch32 Batch0 Loss: 597285.3750\n",
      "Epoch32 Batch2 Loss: 606609.6875\n",
      "Epoch32 Batch4 Loss: 571392.5000\n",
      "train Loss: 575763.9750\n",
      "Epoch 33/499\n",
      "----------\n",
      "Epoch33 Batch0 Loss: 555548.8750\n",
      "Epoch33 Batch2 Loss: 602632.6250\n",
      "Epoch33 Batch4 Loss: 541072.5625\n",
      "train Loss: 568857.1600\n",
      "Epoch 34/499\n",
      "----------\n",
      "Epoch34 Batch0 Loss: 523667.5938\n",
      "Epoch34 Batch2 Loss: 492660.8125\n",
      "Epoch34 Batch4 Loss: 496524.0625\n",
      "train Loss: 533373.1450\n",
      "Epoch 35/499\n",
      "----------\n",
      "Epoch35 Batch0 Loss: 620820.3125\n",
      "Epoch35 Batch2 Loss: 468788.1562\n",
      "Epoch35 Batch4 Loss: 494528.0938\n",
      "train Loss: 528260.3850\n",
      "Epoch 36/499\n",
      "----------\n",
      "Epoch36 Batch0 Loss: 649787.0625\n",
      "Epoch36 Batch2 Loss: 492301.6250\n",
      "Epoch36 Batch4 Loss: 481748.4062\n",
      "train Loss: 535059.5650\n",
      "Epoch 37/499\n",
      "----------\n",
      "Epoch37 Batch0 Loss: 772733.2500\n",
      "Epoch37 Batch2 Loss: 555340.3125\n",
      "Epoch37 Batch4 Loss: 500202.9688\n",
      "train Loss: 557895.6050\n",
      "Epoch 38/499\n",
      "----------\n",
      "Epoch38 Batch0 Loss: 894911.9375\n",
      "Epoch38 Batch2 Loss: 516547.3125\n",
      "Epoch38 Batch4 Loss: 648087.0000\n",
      "train Loss: 580961.0050\n",
      "Epoch 39/499\n",
      "----------\n",
      "Epoch39 Batch0 Loss: 644519.0000\n",
      "Epoch39 Batch2 Loss: 520652.6875\n",
      "Epoch39 Batch4 Loss: 601690.0625\n",
      "train Loss: 539280.5850\n",
      "Epoch 40/499\n",
      "----------\n",
      "Epoch40 Batch0 Loss: 585265.6250\n",
      "Epoch40 Batch2 Loss: 569852.1875\n",
      "Epoch40 Batch4 Loss: 561802.1875\n",
      "train Loss: 534430.9200\n",
      "Epoch 41/499\n",
      "----------\n",
      "Epoch41 Batch0 Loss: 538699.0000\n",
      "Epoch41 Batch2 Loss: 533365.7500\n",
      "Epoch41 Batch4 Loss: 550729.6250\n",
      "train Loss: 548938.5850\n",
      "Epoch 42/499\n",
      "----------\n",
      "Epoch42 Batch0 Loss: 525804.1250\n",
      "Epoch42 Batch2 Loss: 488631.4375\n",
      "Epoch42 Batch4 Loss: 466147.5938\n",
      "train Loss: 522762.7050\n",
      "Epoch 43/499\n",
      "----------\n",
      "Epoch43 Batch0 Loss: 461322.4688\n",
      "Epoch43 Batch2 Loss: 575685.8125\n",
      "Epoch43 Batch4 Loss: 440237.7188\n",
      "train Loss: 505316.4150\n",
      "Epoch 44/499\n",
      "----------\n",
      "Epoch44 Batch0 Loss: 473532.9688\n",
      "Epoch44 Batch2 Loss: 624460.6250\n",
      "Epoch44 Batch4 Loss: 483810.7812\n",
      "train Loss: 512068.2900\n",
      "Epoch 45/499\n",
      "----------\n",
      "Epoch45 Batch0 Loss: 455616.4062\n",
      "Epoch45 Batch2 Loss: 773495.7500\n",
      "Epoch45 Batch4 Loss: 535141.5000\n",
      "train Loss: 539346.9700\n",
      "Epoch 46/499\n",
      "----------\n",
      "Epoch46 Batch0 Loss: 489412.8750\n",
      "Epoch46 Batch2 Loss: 873722.9375\n",
      "Epoch46 Batch4 Loss: 488227.6875\n",
      "train Loss: 534508.2150\n",
      "Epoch 47/499\n",
      "----------\n",
      "Epoch47 Batch0 Loss: 601094.2500\n",
      "Epoch47 Batch2 Loss: 630172.9375\n",
      "Epoch47 Batch4 Loss: 505567.4375\n",
      "train Loss: 530561.5950\n",
      "Epoch 48/499\n",
      "----------\n",
      "Epoch48 Batch0 Loss: 552672.3750\n",
      "Epoch48 Batch2 Loss: 559326.9375\n",
      "Epoch48 Batch4 Loss: 530207.9375\n",
      "train Loss: 510585.3800\n",
      "Epoch 49/499\n",
      "----------\n",
      "Epoch49 Batch0 Loss: 523514.7500\n",
      "Epoch49 Batch2 Loss: 496628.0312\n",
      "Epoch49 Batch4 Loss: 483077.9062\n",
      "train Loss: 500052.9600\n",
      "Epoch 50/499\n",
      "----------\n",
      "Epoch50 Batch0 Loss: 497498.4062\n",
      "Epoch50 Batch2 Loss: 465398.4062\n",
      "Epoch50 Batch4 Loss: 481211.0938\n",
      "train Loss: 503793.5650\n",
      "Epoch 51/499\n",
      "----------\n",
      "Epoch51 Batch0 Loss: 469588.0000\n",
      "Epoch51 Batch2 Loss: 450324.0625\n",
      "Epoch51 Batch4 Loss: 578062.6250\n",
      "train Loss: 504373.0600\n",
      "Epoch 52/499\n",
      "----------\n",
      "Epoch52 Batch0 Loss: 478349.1875\n",
      "Epoch52 Batch2 Loss: 458891.6250\n",
      "Epoch52 Batch4 Loss: 606423.9375\n",
      "train Loss: 499740.5600\n",
      "Epoch 53/499\n",
      "----------\n",
      "Epoch53 Batch0 Loss: 434418.6250\n",
      "Epoch53 Batch2 Loss: 476897.6250\n",
      "Epoch53 Batch4 Loss: 726553.2500\n",
      "train Loss: 495750.4600\n",
      "Epoch 54/499\n",
      "----------\n",
      "Epoch54 Batch0 Loss: 461762.4062\n",
      "Epoch54 Batch2 Loss: 497252.0000\n",
      "Epoch54 Batch4 Loss: 724866.3750\n",
      "train Loss: 490047.7450\n",
      "Epoch 55/499\n",
      "----------\n",
      "Epoch55 Batch0 Loss: 461543.1250\n",
      "Epoch55 Batch2 Loss: 535380.1875\n",
      "Epoch55 Batch4 Loss: 603415.1875\n",
      "train Loss: 480391.0250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/499\n",
      "----------\n",
      "Epoch56 Batch0 Loss: 430143.0625\n",
      "Epoch56 Batch2 Loss: 519114.4688\n",
      "Epoch56 Batch4 Loss: 532301.8750\n",
      "train Loss: 469853.9300\n",
      "Epoch 57/499\n",
      "----------\n",
      "Epoch57 Batch0 Loss: 469449.1250\n",
      "Epoch57 Batch2 Loss: 472018.7500\n",
      "Epoch57 Batch4 Loss: 480425.7500\n",
      "train Loss: 486129.3550\n",
      "Epoch 58/499\n",
      "----------\n",
      "Epoch58 Batch0 Loss: 428577.2812\n",
      "Epoch58 Batch2 Loss: 509430.6875\n",
      "Epoch58 Batch4 Loss: 453721.2188\n",
      "train Loss: 500055.1750\n",
      "Epoch 59/499\n",
      "----------\n",
      "Epoch59 Batch0 Loss: 431414.8438\n",
      "Epoch59 Batch2 Loss: 458687.5312\n",
      "Epoch59 Batch4 Loss: 409134.8438\n",
      "train Loss: 455027.5200\n",
      "Epoch 60/499\n",
      "----------\n",
      "Epoch60 Batch0 Loss: 562472.4375\n",
      "Epoch60 Batch2 Loss: 400434.9062\n",
      "Epoch60 Batch4 Loss: 468270.5000\n",
      "train Loss: 459010.7000\n",
      "Epoch 61/499\n",
      "----------\n",
      "Epoch61 Batch0 Loss: 535811.0625\n",
      "Epoch61 Batch2 Loss: 409031.2188\n",
      "Epoch61 Batch4 Loss: 433123.3750\n",
      "train Loss: 441603.0600\n",
      "Epoch 62/499\n",
      "----------\n",
      "Epoch62 Batch0 Loss: 591772.7500\n",
      "Epoch62 Batch2 Loss: 436178.2500\n",
      "Epoch62 Batch4 Loss: 426187.5312\n",
      "train Loss: 434838.6600\n",
      "Epoch 63/499\n",
      "----------\n",
      "Epoch63 Batch0 Loss: 645967.0000\n",
      "Epoch63 Batch2 Loss: 451143.7188\n",
      "Epoch63 Batch4 Loss: 483873.0000\n",
      "train Loss: 448180.4800\n",
      "Epoch 64/499\n",
      "----------\n",
      "Epoch64 Batch0 Loss: 577325.5000\n",
      "Epoch64 Batch2 Loss: 438556.5625\n",
      "Epoch64 Batch4 Loss: 466771.1562\n",
      "train Loss: 449336.6950\n",
      "Epoch 65/499\n",
      "----------\n",
      "Epoch65 Batch0 Loss: 506106.3125\n",
      "Epoch65 Batch2 Loss: 440534.9688\n",
      "Epoch65 Batch4 Loss: 475595.7188\n",
      "train Loss: 458533.5600\n",
      "Epoch 66/499\n",
      "----------\n",
      "Epoch66 Batch0 Loss: 457053.5938\n",
      "Epoch66 Batch2 Loss: 445938.0312\n",
      "Epoch66 Batch4 Loss: 456372.3750\n",
      "train Loss: 473736.2100\n",
      "Epoch 67/499\n",
      "----------\n",
      "Epoch67 Batch0 Loss: 411316.8125\n",
      "Epoch67 Batch2 Loss: 405076.5625\n",
      "Epoch67 Batch4 Loss: 391356.5000\n",
      "train Loss: 434767.0800\n",
      "Epoch 68/499\n",
      "----------\n",
      "Epoch68 Batch0 Loss: 395490.7188\n",
      "Epoch68 Batch2 Loss: 486514.4375\n",
      "Epoch68 Batch4 Loss: 386261.5000\n",
      "train Loss: 427916.3500\n",
      "Epoch 69/499\n",
      "----------\n",
      "Epoch69 Batch0 Loss: 415627.8750\n",
      "Epoch69 Batch2 Loss: 543411.6875\n",
      "Epoch69 Batch4 Loss: 401760.2812\n",
      "train Loss: 429122.5450\n",
      "Epoch 70/499\n",
      "----------\n",
      "Epoch70 Batch0 Loss: 414101.5312\n",
      "Epoch70 Batch2 Loss: 602227.0625\n",
      "Epoch70 Batch4 Loss: 442123.5000\n",
      "train Loss: 434209.3550\n",
      "Epoch 71/499\n",
      "----------\n",
      "Epoch71 Batch0 Loss: 416791.9375\n",
      "Epoch71 Batch2 Loss: 632413.3750\n",
      "Epoch71 Batch4 Loss: 414111.3125\n",
      "train Loss: 419490.6050\n",
      "Epoch 72/499\n",
      "----------\n",
      "Epoch72 Batch0 Loss: 476368.5000\n",
      "Epoch72 Batch2 Loss: 527282.6875\n",
      "Epoch72 Batch4 Loss: 379336.1562\n",
      "train Loss: 418182.2100\n",
      "Epoch 73/499\n",
      "----------\n",
      "Epoch73 Batch0 Loss: 458701.8438\n",
      "Epoch73 Batch2 Loss: 445786.9062\n",
      "Epoch73 Batch4 Loss: 417905.2500\n",
      "train Loss: 424289.5800\n",
      "Epoch 74/499\n",
      "----------\n",
      "Epoch74 Batch0 Loss: 434657.0000\n",
      "Epoch74 Batch2 Loss: 413645.2812\n",
      "Epoch74 Batch4 Loss: 383721.5312\n",
      "train Loss: 424912.9600\n",
      "Epoch 75/499\n",
      "----------\n",
      "Epoch75 Batch0 Loss: 412572.3438\n",
      "Epoch75 Batch2 Loss: 377380.5000\n",
      "Epoch75 Batch4 Loss: 356366.0938\n",
      "train Loss: 404326.4700\n",
      "Epoch 76/499\n",
      "----------\n",
      "Epoch76 Batch0 Loss: 378560.0312\n",
      "Epoch76 Batch2 Loss: 374909.6250\n",
      "Epoch76 Batch4 Loss: 444879.0625\n",
      "train Loss: 404814.4100\n",
      "Epoch 77/499\n",
      "----------\n",
      "Epoch77 Batch0 Loss: 354237.1250\n",
      "Epoch77 Batch2 Loss: 406775.6562\n",
      "Epoch77 Batch4 Loss: 481548.1562\n",
      "train Loss: 397292.1400\n",
      "Epoch 78/499\n",
      "----------\n",
      "Epoch78 Batch0 Loss: 365156.2188\n",
      "Epoch78 Batch2 Loss: 372056.7812\n",
      "Epoch78 Batch4 Loss: 514309.9688\n",
      "train Loss: 386340.6900\n",
      "Epoch 79/499\n",
      "----------\n",
      "Epoch79 Batch0 Loss: 396844.1562\n",
      "Epoch79 Batch2 Loss: 387584.5938\n",
      "Epoch79 Batch4 Loss: 542725.0000\n",
      "train Loss: 391839.2650\n",
      "Epoch 80/499\n",
      "----------\n",
      "Epoch80 Batch0 Loss: 392087.2188\n",
      "Epoch80 Batch2 Loss: 433070.3438\n",
      "Epoch80 Batch4 Loss: 482362.4688\n",
      "train Loss: 388195.3900\n",
      "Epoch 81/499\n",
      "----------\n",
      "Epoch81 Batch0 Loss: 375816.4062\n",
      "Epoch81 Batch2 Loss: 411353.7188\n",
      "Epoch81 Batch4 Loss: 435292.7500\n",
      "train Loss: 393303.2750\n",
      "Epoch 82/499\n",
      "----------\n",
      "Epoch82 Batch0 Loss: 397807.2500\n",
      "Epoch82 Batch2 Loss: 400345.2500\n",
      "Epoch82 Batch4 Loss: 404746.1562\n",
      "train Loss: 400250.8950\n",
      "Epoch 83/499\n",
      "----------\n",
      "Epoch83 Batch0 Loss: 354176.5000\n",
      "Epoch83 Batch2 Loss: 446785.3125\n",
      "Epoch83 Batch4 Loss: 364844.8438\n",
      "train Loss: 411961.3150\n",
      "Epoch 84/499\n",
      "----------\n",
      "Epoch84 Batch0 Loss: 379530.9062\n",
      "Epoch84 Batch2 Loss: 354034.7188\n",
      "Epoch84 Batch4 Loss: 388892.1250\n",
      "train Loss: 393860.6800\n",
      "Epoch 85/499\n",
      "----------\n",
      "Epoch85 Batch0 Loss: 428055.1250\n",
      "Epoch85 Batch2 Loss: 359753.8750\n",
      "Epoch85 Batch4 Loss: 400964.1562\n",
      "train Loss: 381029.1550\n",
      "Epoch 86/499\n",
      "----------\n",
      "Epoch86 Batch0 Loss: 480082.9375\n",
      "Epoch86 Batch2 Loss: 345969.2188\n",
      "Epoch86 Batch4 Loss: 368431.7188\n",
      "train Loss: 376236.2700\n",
      "Epoch 87/499\n",
      "----------\n",
      "Epoch87 Batch0 Loss: 526489.3750\n",
      "Epoch87 Batch2 Loss: 390131.6562\n",
      "Epoch87 Batch4 Loss: 385421.3125\n",
      "train Loss: 387196.1500\n",
      "Epoch 88/499\n",
      "----------\n",
      "Epoch88 Batch0 Loss: 621992.1250\n",
      "Epoch88 Batch2 Loss: 390276.9375\n",
      "Epoch88 Batch4 Loss: 450684.8750\n",
      "train Loss: 466308.7700\n",
      "Epoch 89/499\n",
      "----------\n",
      "Epoch89 Batch0 Loss: 626063.4375\n",
      "Epoch89 Batch2 Loss: 453574.0625\n",
      "Epoch89 Batch4 Loss: 508060.8750\n",
      "train Loss: 495971.9600\n",
      "Epoch 90/499\n",
      "----------\n",
      "Epoch90 Batch0 Loss: 645868.5000\n",
      "Epoch90 Batch2 Loss: 550909.1875\n",
      "Epoch90 Batch4 Loss: 529930.6250\n",
      "train Loss: 537861.6100\n",
      "Epoch 91/499\n",
      "----------\n",
      "Epoch91 Batch0 Loss: 509501.3125\n",
      "Epoch91 Batch2 Loss: 465115.9062\n",
      "Epoch91 Batch4 Loss: 462204.0000\n",
      "train Loss: 486888.7850\n",
      "Epoch 92/499\n",
      "----------\n",
      "Epoch92 Batch0 Loss: 415343.5625\n",
      "Epoch92 Batch2 Loss: 470287.7500\n",
      "Epoch92 Batch4 Loss: 381762.9062\n",
      "train Loss: 452875.9400\n",
      "Epoch 93/499\n",
      "----------\n",
      "Epoch93 Batch0 Loss: 393433.4688\n",
      "Epoch93 Batch2 Loss: 474365.0000\n",
      "Epoch93 Batch4 Loss: 372965.2812\n",
      "train Loss: 418851.3200\n",
      "Epoch 94/499\n",
      "----------\n",
      "Epoch94 Batch0 Loss: 407206.4688\n",
      "Epoch94 Batch2 Loss: 510792.1250\n",
      "Epoch94 Batch4 Loss: 400428.0625\n",
      "train Loss: 418186.6450\n",
      "Epoch 95/499\n",
      "----------\n",
      "Epoch95 Batch0 Loss: 385374.4062\n",
      "Epoch95 Batch2 Loss: 549422.6875\n",
      "Epoch95 Batch4 Loss: 483580.5938\n",
      "train Loss: 424549.9950\n",
      "Epoch 96/499\n",
      "----------\n",
      "Epoch96 Batch0 Loss: 408028.5000\n",
      "Epoch96 Batch2 Loss: 709226.1875\n",
      "Epoch96 Batch4 Loss: 426559.5938\n",
      "train Loss: 450410.0900\n",
      "Epoch 97/499\n",
      "----------\n",
      "Epoch97 Batch0 Loss: 539269.6250\n",
      "Epoch97 Batch2 Loss: 516948.1562\n",
      "Epoch97 Batch4 Loss: 408409.5625\n",
      "train Loss: 444506.9600\n",
      "Epoch 98/499\n",
      "----------\n",
      "Epoch98 Batch0 Loss: 431042.5938\n",
      "Epoch98 Batch2 Loss: 428846.2500\n",
      "Epoch98 Batch4 Loss: 411611.5938\n",
      "train Loss: 402919.5250\n",
      "Epoch 99/499\n",
      "----------\n",
      "Epoch99 Batch0 Loss: 400038.8438\n",
      "Epoch99 Batch2 Loss: 404439.2188\n",
      "Epoch99 Batch4 Loss: 362257.3438\n",
      "train Loss: 396329.2500\n",
      "Epoch 100/499\n",
      "----------\n",
      "Epoch100 Batch0 Loss: 400181.3750\n",
      "Epoch100 Batch2 Loss: 364368.7812\n",
      "Epoch100 Batch4 Loss: 358175.0938\n",
      "train Loss: 400615.8750\n",
      "Epoch 101/499\n",
      "----------\n",
      "Epoch101 Batch0 Loss: 369625.5312\n",
      "Epoch101 Batch2 Loss: 357760.9375\n",
      "Epoch101 Batch4 Loss: 477003.6875\n",
      "train Loss: 410208.4750\n",
      "Epoch 102/499\n",
      "----------\n",
      "Epoch102 Batch0 Loss: 346496.6562\n",
      "Epoch102 Batch2 Loss: 444526.0312\n",
      "Epoch102 Batch4 Loss: 477273.8125\n",
      "train Loss: 398618.1550\n",
      "Epoch 103/499\n",
      "----------\n",
      "Epoch103 Batch0 Loss: 353330.0625\n",
      "Epoch103 Batch2 Loss: 374237.1250\n",
      "Epoch103 Batch4 Loss: 502760.8438\n",
      "train Loss: 381652.0700\n",
      "Epoch 104/499\n",
      "----------\n",
      "Epoch104 Batch0 Loss: 367889.8125\n",
      "Epoch104 Batch2 Loss: 364979.1875\n",
      "Epoch104 Batch4 Loss: 556699.6875\n",
      "train Loss: 375519.4550\n",
      "Epoch 105/499\n",
      "----------\n",
      "Epoch105 Batch0 Loss: 387048.1250\n",
      "Epoch105 Batch2 Loss: 410858.2188\n",
      "Epoch105 Batch4 Loss: 484935.5312\n",
      "train Loss: 386181.8900\n",
      "Epoch 106/499\n",
      "----------\n",
      "Epoch106 Batch0 Loss: 369243.3125\n",
      "Epoch106 Batch2 Loss: 395209.9375\n",
      "Epoch106 Batch4 Loss: 396091.4062\n",
      "train Loss: 377952.1500\n",
      "Epoch 107/499\n",
      "----------\n",
      "Epoch107 Batch0 Loss: 379289.9688\n",
      "Epoch107 Batch2 Loss: 377215.1562\n",
      "Epoch107 Batch4 Loss: 374172.3438\n",
      "train Loss: 376490.3700\n",
      "Epoch 108/499\n",
      "----------\n",
      "Epoch108 Batch0 Loss: 352461.2812\n",
      "Epoch108 Batch2 Loss: 381379.1562\n",
      "Epoch108 Batch4 Loss: 344579.6562\n",
      "train Loss: 370957.0750\n",
      "Epoch 109/499\n",
      "----------\n",
      "Epoch109 Batch0 Loss: 324969.4062\n",
      "Epoch109 Batch2 Loss: 309866.2500\n",
      "Epoch109 Batch4 Loss: 333535.1875\n",
      "train Loss: 344884.9550\n",
      "Epoch 110/499\n",
      "----------\n",
      "Epoch110 Batch0 Loss: 398900.6250\n",
      "Epoch110 Batch2 Loss: 311441.3125\n",
      "Epoch110 Batch4 Loss: 346962.8750\n",
      "train Loss: 341153.6500\n",
      "Epoch 111/499\n",
      "----------\n",
      "Epoch111 Batch0 Loss: 440105.2500\n",
      "Epoch111 Batch2 Loss: 323885.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch111 Batch4 Loss: 316915.6875\n",
      "train Loss: 345041.1950\n",
      "Epoch 112/499\n",
      "----------\n",
      "Epoch112 Batch0 Loss: 472875.7812\n",
      "Epoch112 Batch2 Loss: 347229.0938\n",
      "Epoch112 Batch4 Loss: 321459.2188\n",
      "train Loss: 340233.6850\n",
      "Epoch 113/499\n",
      "----------\n",
      "Epoch113 Batch0 Loss: 505697.8438\n",
      "Epoch113 Batch2 Loss: 344408.6250\n",
      "Epoch113 Batch4 Loss: 378801.6875\n",
      "train Loss: 343812.2900\n",
      "Epoch 114/499\n",
      "----------\n",
      "Epoch114 Batch0 Loss: 426714.0625\n",
      "Epoch114 Batch2 Loss: 321238.3750\n",
      "Epoch114 Batch4 Loss: 372100.2188\n",
      "train Loss: 346180.8050\n",
      "Epoch 115/499\n",
      "----------\n",
      "Epoch115 Batch0 Loss: 368272.0000\n",
      "Epoch115 Batch2 Loss: 346912.6875\n",
      "Epoch115 Batch4 Loss: 367658.6562\n",
      "train Loss: 355238.3200\n",
      "Epoch 116/499\n",
      "----------\n",
      "Epoch116 Batch0 Loss: 343285.6875\n",
      "Epoch116 Batch2 Loss: 342101.7812\n",
      "Epoch116 Batch4 Loss: 357392.6250\n",
      "train Loss: 359150.5850\n",
      "Epoch 117/499\n",
      "----------\n",
      "Epoch117 Batch0 Loss: 339125.0938\n",
      "Epoch117 Batch2 Loss: 315187.7188\n",
      "Epoch117 Batch4 Loss: 314113.1562\n",
      "train Loss: 350381.3800\n",
      "Epoch 118/499\n",
      "----------\n",
      "Epoch118 Batch0 Loss: 321002.5000\n",
      "Epoch118 Batch2 Loss: 390135.8125\n",
      "Epoch118 Batch4 Loss: 298882.5312\n",
      "train Loss: 342285.9750\n",
      "Epoch 119/499\n",
      "----------\n",
      "Epoch119 Batch0 Loss: 336491.9375\n",
      "Epoch119 Batch2 Loss: 428001.4375\n",
      "Epoch119 Batch4 Loss: 314553.5312\n",
      "train Loss: 342899.5450\n",
      "Epoch 120/499\n",
      "----------\n",
      "Epoch120 Batch0 Loss: 304595.6562\n",
      "Epoch120 Batch2 Loss: 445506.3125\n",
      "Epoch120 Batch4 Loss: 332390.6250\n",
      "train Loss: 332347.8250\n",
      "Epoch 121/499\n",
      "----------\n",
      "Epoch121 Batch0 Loss: 312719.0312\n",
      "Epoch121 Batch2 Loss: 473552.8125\n",
      "Epoch121 Batch4 Loss: 325490.7188\n",
      "train Loss: 319467.4550\n",
      "Epoch 122/499\n",
      "----------\n",
      "Epoch122 Batch0 Loss: 366271.7500\n",
      "Epoch122 Batch2 Loss: 395109.4688\n",
      "Epoch122 Batch4 Loss: 306693.4062\n",
      "train Loss: 322112.2300\n",
      "Epoch 123/499\n",
      "----------\n",
      "Epoch123 Batch0 Loss: 347874.0312\n",
      "Epoch123 Batch2 Loss: 335897.7812\n",
      "Epoch123 Batch4 Loss: 326957.5625\n",
      "train Loss: 323964.6750\n",
      "Epoch 124/499\n",
      "----------\n",
      "Epoch124 Batch0 Loss: 349790.8750\n",
      "Epoch124 Batch2 Loss: 322224.5312\n",
      "Epoch124 Batch4 Loss: 300332.0938\n",
      "train Loss: 320156.7200\n",
      "Epoch 125/499\n",
      "----------\n",
      "Epoch125 Batch0 Loss: 328054.5938\n",
      "Epoch125 Batch2 Loss: 284428.1562\n",
      "Epoch125 Batch4 Loss: 288738.8438\n",
      "train Loss: 314603.7900\n",
      "Epoch 126/499\n",
      "----------\n",
      "Epoch126 Batch0 Loss: 277801.5312\n",
      "Epoch126 Batch2 Loss: 276288.9062\n",
      "Epoch126 Batch4 Loss: 362716.4062\n",
      "train Loss: 314054.2300\n",
      "Epoch 127/499\n",
      "----------\n",
      "Epoch127 Batch0 Loss: 269593.0312\n",
      "Epoch127 Batch2 Loss: 297226.1875\n",
      "Epoch127 Batch4 Loss: 392973.9688\n",
      "train Loss: 312950.1800\n",
      "Epoch 128/499\n",
      "----------\n",
      "Epoch128 Batch0 Loss: 287415.7188\n",
      "Epoch128 Batch2 Loss: 277377.8438\n",
      "Epoch128 Batch4 Loss: 408701.9688\n",
      "train Loss: 306964.0250\n",
      "Epoch 129/499\n",
      "----------\n",
      "Epoch129 Batch0 Loss: 306746.2188\n",
      "Epoch129 Batch2 Loss: 285557.4688\n",
      "Epoch129 Batch4 Loss: 443418.0625\n",
      "train Loss: 307799.1100\n",
      "Epoch 130/499\n",
      "----------\n",
      "Epoch130 Batch0 Loss: 307039.1250\n",
      "Epoch130 Batch2 Loss: 359132.8125\n",
      "Epoch130 Batch4 Loss: 428978.7812\n",
      "train Loss: 324718.8450\n",
      "Epoch 131/499\n",
      "----------\n",
      "Epoch131 Batch0 Loss: 308481.3750\n",
      "Epoch131 Batch2 Loss: 360521.2188\n",
      "Epoch131 Batch4 Loss: 359119.7500\n",
      "train Loss: 333215.5700\n",
      "Epoch 132/499\n",
      "----------\n",
      "Epoch132 Batch0 Loss: 368721.4062\n",
      "Epoch132 Batch2 Loss: 364506.4688\n",
      "Epoch132 Batch4 Loss: 347558.0312\n",
      "train Loss: 355534.3150\n",
      "Epoch 133/499\n",
      "----------\n",
      "Epoch133 Batch0 Loss: 336644.0938\n",
      "Epoch133 Batch2 Loss: 352353.0000\n",
      "Epoch133 Batch4 Loss: 312390.0625\n",
      "train Loss: 346886.7100\n",
      "Epoch 134/499\n",
      "----------\n",
      "Epoch134 Batch0 Loss: 317046.9375\n",
      "Epoch134 Batch2 Loss: 299838.0938\n",
      "Epoch134 Batch4 Loss: 300200.2500\n",
      "train Loss: 323400.7600\n",
      "Epoch 135/499\n",
      "----------\n",
      "Epoch135 Batch0 Loss: 377326.7188\n",
      "Epoch135 Batch2 Loss: 286136.7812\n",
      "Epoch135 Batch4 Loss: 307795.5625\n",
      "train Loss: 314107.4750\n",
      "Epoch 136/499\n",
      "----------\n",
      "Epoch136 Batch0 Loss: 420401.8125\n",
      "Epoch136 Batch2 Loss: 293520.4688\n",
      "Epoch136 Batch4 Loss: 284535.1250\n",
      "train Loss: 316864.6350\n",
      "Epoch 137/499\n",
      "----------\n",
      "Epoch137 Batch0 Loss: 445706.2812\n",
      "Epoch137 Batch2 Loss: 308467.0312\n",
      "Epoch137 Batch4 Loss: 302141.5625\n",
      "train Loss: 317131.8050\n",
      "Epoch 138/499\n",
      "----------\n",
      "Epoch138 Batch0 Loss: 457750.6562\n",
      "Epoch138 Batch2 Loss: 320126.8125\n",
      "Epoch138 Batch4 Loss: 363944.9375\n",
      "train Loss: 316334.8350\n",
      "Epoch 139/499\n",
      "----------\n",
      "Epoch139 Batch0 Loss: 380757.1562\n",
      "Epoch139 Batch2 Loss: 296757.9688\n",
      "Epoch139 Batch4 Loss: 342776.5625\n",
      "train Loss: 314566.5200\n",
      "Epoch 140/499\n",
      "----------\n",
      "Epoch140 Batch0 Loss: 317064.8438\n",
      "Epoch140 Batch2 Loss: 331703.6562\n",
      "Epoch140 Batch4 Loss: 332089.8438\n",
      "train Loss: 310667.4050\n",
      "Epoch 141/499\n",
      "----------\n",
      "Epoch141 Batch0 Loss: 309381.7812\n",
      "Epoch141 Batch2 Loss: 297861.3750\n",
      "Epoch141 Batch4 Loss: 320085.5312\n",
      "train Loss: 309655.0550\n",
      "Epoch 142/499\n",
      "----------\n",
      "Epoch142 Batch0 Loss: 277981.8750\n",
      "Epoch142 Batch2 Loss: 279690.7188\n",
      "Epoch142 Batch4 Loss: 263733.3438\n",
      "train Loss: 302536.6100\n",
      "Epoch 143/499\n",
      "----------\n",
      "Epoch143 Batch0 Loss: 263323.9375\n",
      "Epoch143 Batch2 Loss: 340098.7812\n",
      "Epoch143 Batch4 Loss: 256050.4219\n",
      "train Loss: 299766.9875\n",
      "Epoch 144/499\n",
      "----------\n",
      "Epoch144 Batch0 Loss: 276238.7812\n",
      "Epoch144 Batch2 Loss: 383142.6250\n",
      "Epoch144 Batch4 Loss: 262880.4062\n",
      "train Loss: 300363.0900\n",
      "Epoch 145/499\n",
      "----------\n",
      "Epoch145 Batch0 Loss: 264628.1250\n",
      "Epoch145 Batch2 Loss: 393756.4062\n",
      "Epoch145 Batch4 Loss: 291486.8438\n",
      "train Loss: 294458.8100\n",
      "Epoch 146/499\n",
      "----------\n",
      "Epoch146 Batch0 Loss: 267656.9375\n",
      "Epoch146 Batch2 Loss: 424161.2500\n",
      "Epoch146 Batch4 Loss: 305040.6562\n",
      "train Loss: 288359.0525\n",
      "Epoch 147/499\n",
      "----------\n",
      "Epoch147 Batch0 Loss: 351982.3438\n",
      "Epoch147 Batch2 Loss: 349697.0312\n",
      "Epoch147 Batch4 Loss: 288805.6875\n",
      "train Loss: 293625.4125\n",
      "Epoch 148/499\n",
      "----------\n",
      "Epoch148 Batch0 Loss: 318408.4375\n",
      "Epoch148 Batch2 Loss: 305587.2812\n",
      "Epoch148 Batch4 Loss: 306557.4688\n",
      "train Loss: 294173.5500\n",
      "Epoch 149/499\n",
      "----------\n",
      "Epoch149 Batch0 Loss: 316477.5938\n",
      "Epoch149 Batch2 Loss: 296890.8438\n",
      "Epoch149 Batch4 Loss: 287999.4375\n",
      "train Loss: 299430.0950\n",
      "Epoch 150/499\n",
      "----------\n",
      "Epoch150 Batch0 Loss: 315028.5312\n",
      "Epoch150 Batch2 Loss: 272056.8125\n",
      "Epoch150 Batch4 Loss: 279463.6250\n",
      "train Loss: 300243.4550\n",
      "Epoch 151/499\n",
      "----------\n",
      "Epoch151 Batch0 Loss: 257491.7656\n",
      "Epoch151 Batch2 Loss: 252066.4062\n",
      "Epoch151 Batch4 Loss: 339804.7500\n",
      "train Loss: 290117.2775\n",
      "Epoch 152/499\n",
      "----------\n",
      "Epoch152 Batch0 Loss: 255754.0469\n",
      "Epoch152 Batch2 Loss: 273552.2500\n",
      "Epoch152 Batch4 Loss: 379375.9375\n",
      "train Loss: 293982.7075\n",
      "Epoch 153/499\n",
      "----------\n",
      "Epoch153 Batch0 Loss: 269642.2812\n",
      "Epoch153 Batch2 Loss: 276589.3125\n",
      "Epoch153 Batch4 Loss: 392511.6250\n",
      "train Loss: 296130.0100\n",
      "Epoch 154/499\n",
      "----------\n",
      "Epoch154 Batch0 Loss: 281898.0000\n",
      "Epoch154 Batch2 Loss: 289598.0625\n",
      "Epoch154 Batch4 Loss: 420424.8438\n",
      "train Loss: 292968.7600\n",
      "Epoch 155/499\n",
      "----------\n",
      "Epoch155 Batch0 Loss: 292995.3125\n",
      "Epoch155 Batch2 Loss: 338790.1562\n",
      "Epoch155 Batch4 Loss: 368699.6562\n",
      "train Loss: 298161.0675\n",
      "Epoch 156/499\n",
      "----------\n",
      "Epoch156 Batch0 Loss: 293606.3125\n",
      "Epoch156 Batch2 Loss: 322478.7500\n",
      "Epoch156 Batch4 Loss: 303913.7188\n",
      "train Loss: 292738.8225\n",
      "Epoch 157/499\n",
      "----------\n",
      "Epoch157 Batch0 Loss: 303387.3750\n",
      "Epoch157 Batch2 Loss: 326996.0625\n",
      "Epoch157 Batch4 Loss: 291991.6562\n",
      "train Loss: 291262.4200\n",
      "Epoch 158/499\n",
      "----------\n",
      "Epoch158 Batch0 Loss: 277855.9062\n",
      "Epoch158 Batch2 Loss: 297121.8125\n",
      "Epoch158 Batch4 Loss: 250721.6250\n",
      "train Loss: 280507.6100\n",
      "Epoch 159/499\n",
      "----------\n",
      "Epoch159 Batch0 Loss: 269212.2812\n",
      "Epoch159 Batch2 Loss: 236929.6719\n",
      "Epoch159 Batch4 Loss: 237687.9531\n",
      "train Loss: 272433.1650\n",
      "Epoch 160/499\n",
      "----------\n",
      "Epoch160 Batch0 Loss: 319905.2188\n",
      "Epoch160 Batch2 Loss: 231902.5469\n",
      "Epoch160 Batch4 Loss: 252578.1406\n",
      "train Loss: 266663.8900\n",
      "Epoch 161/499\n",
      "----------\n",
      "Epoch161 Batch0 Loss: 354507.3750\n",
      "Epoch161 Batch2 Loss: 240689.0312\n",
      "Epoch161 Batch4 Loss: 252739.0000\n",
      "train Loss: 277401.5500\n",
      "Epoch 162/499\n",
      "----------\n",
      "Epoch162 Batch0 Loss: 409831.2812\n",
      "Epoch162 Batch2 Loss: 277856.5000\n",
      "Epoch162 Batch4 Loss: 285704.3750\n",
      "train Loss: 296074.4950\n",
      "Epoch 163/499\n",
      "----------\n",
      "Epoch163 Batch0 Loss: 405801.1562\n",
      "Epoch163 Batch2 Loss: 316407.0625\n",
      "Epoch163 Batch4 Loss: 337200.6875\n",
      "train Loss: 299079.8000\n",
      "Epoch 164/499\n",
      "----------\n",
      "Epoch164 Batch0 Loss: 347745.7500\n",
      "Epoch164 Batch2 Loss: 274014.0625\n",
      "Epoch164 Batch4 Loss: 314671.8750\n",
      "train Loss: 283022.6050\n",
      "Epoch 165/499\n",
      "----------\n",
      "Epoch165 Batch0 Loss: 298474.8125\n",
      "Epoch165 Batch2 Loss: 296419.7500\n",
      "Epoch165 Batch4 Loss: 314794.5938\n",
      "train Loss: 286221.0425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/499\n",
      "----------\n",
      "Epoch166 Batch0 Loss: 278870.0625\n",
      "Epoch166 Batch2 Loss: 267109.8125\n",
      "Epoch166 Batch4 Loss: 290908.9688\n",
      "train Loss: 278686.2075\n",
      "Epoch 167/499\n",
      "----------\n",
      "Epoch167 Batch0 Loss: 251766.9062\n",
      "Epoch167 Batch2 Loss: 259148.0000\n",
      "Epoch167 Batch4 Loss: 231507.3281\n",
      "train Loss: 274274.0175\n",
      "Epoch 168/499\n",
      "----------\n",
      "Epoch168 Batch0 Loss: 239709.8906\n",
      "Epoch168 Batch2 Loss: 318648.6875\n",
      "Epoch168 Batch4 Loss: 235129.8438\n",
      "train Loss: 273559.0225\n",
      "Epoch 169/499\n",
      "----------\n",
      "Epoch169 Batch0 Loss: 245474.7656\n",
      "Epoch169 Batch2 Loss: 353323.4062\n",
      "Epoch169 Batch4 Loss: 229998.7969\n",
      "train Loss: 271593.9200\n",
      "Epoch 170/499\n",
      "----------\n",
      "Epoch170 Batch0 Loss: 231585.2031\n",
      "Epoch170 Batch2 Loss: 357853.4688\n",
      "Epoch170 Batch4 Loss: 248581.0312\n",
      "train Loss: 263268.3175\n",
      "Epoch 171/499\n",
      "----------\n",
      "Epoch171 Batch0 Loss: 235932.7656\n",
      "Epoch171 Batch2 Loss: 372118.3438\n",
      "Epoch171 Batch4 Loss: 253589.0312\n",
      "train Loss: 250303.6725\n",
      "Epoch 172/499\n",
      "----------\n",
      "Epoch172 Batch0 Loss: 302420.0625\n",
      "Epoch172 Batch2 Loss: 319280.3750\n",
      "Epoch172 Batch4 Loss: 253463.4531\n",
      "train Loss: 256391.8350\n",
      "Epoch 173/499\n",
      "----------\n",
      "Epoch173 Batch0 Loss: 293232.7188\n",
      "Epoch173 Batch2 Loss: 264160.7500\n",
      "Epoch173 Batch4 Loss: 277777.3438\n",
      "train Loss: 260058.5975\n",
      "Epoch 174/499\n",
      "----------\n",
      "Epoch174 Batch0 Loss: 281342.2500\n",
      "Epoch174 Batch2 Loss: 257472.9219\n",
      "Epoch174 Batch4 Loss: 249609.7344\n",
      "train Loss: 256164.6200\n",
      "Epoch 175/499\n",
      "----------\n",
      "Epoch175 Batch0 Loss: 269750.7812\n",
      "Epoch175 Batch2 Loss: 236207.1875\n",
      "Epoch175 Batch4 Loss: 245474.9062\n",
      "train Loss: 255747.1375\n",
      "Epoch 176/499\n",
      "----------\n",
      "Epoch176 Batch0 Loss: 218419.2812\n",
      "Epoch176 Batch2 Loss: 216250.2969\n",
      "Epoch176 Batch4 Loss: 304165.8750\n",
      "train Loss: 257774.9125\n",
      "Epoch 177/499\n",
      "----------\n",
      "Epoch177 Batch0 Loss: 202287.4688\n",
      "Epoch177 Batch2 Loss: 230996.7812\n",
      "Epoch177 Batch4 Loss: 332421.1250\n",
      "train Loss: 254729.8700\n",
      "Epoch 178/499\n",
      "----------\n",
      "Epoch178 Batch0 Loss: 213838.0156\n",
      "Epoch178 Batch2 Loss: 234927.2031\n",
      "Epoch178 Batch4 Loss: 354575.9375\n",
      "train Loss: 257163.4200\n",
      "Epoch 179/499\n",
      "----------\n",
      "Epoch179 Batch0 Loss: 240356.6719\n",
      "Epoch179 Batch2 Loss: 237109.7188\n",
      "Epoch179 Batch4 Loss: 375381.6562\n",
      "train Loss: 261262.9625\n",
      "Epoch 180/499\n",
      "----------\n",
      "Epoch180 Batch0 Loss: 267613.3125\n",
      "Epoch180 Batch2 Loss: 320454.8750\n",
      "Epoch180 Batch4 Loss: 325340.4375\n",
      "train Loss: 272312.9350\n",
      "Epoch 181/499\n",
      "----------\n",
      "Epoch181 Batch0 Loss: 256370.3281\n",
      "Epoch181 Batch2 Loss: 296929.4375\n",
      "Epoch181 Batch4 Loss: 271268.8750\n",
      "train Loss: 260253.1675\n",
      "Epoch 182/499\n",
      "----------\n",
      "Epoch182 Batch0 Loss: 283863.3125\n",
      "Epoch182 Batch2 Loss: 281491.6562\n",
      "Epoch182 Batch4 Loss: 272698.4062\n",
      "train Loss: 267771.5025\n",
      "Epoch 183/499\n",
      "----------\n",
      "Epoch183 Batch0 Loss: 259433.0469\n",
      "Epoch183 Batch2 Loss: 281986.6250\n",
      "Epoch183 Batch4 Loss: 267608.7188\n",
      "train Loss: 275151.1400\n",
      "Epoch 184/499\n",
      "----------\n",
      "Epoch184 Batch0 Loss: 252054.8438\n",
      "Epoch184 Batch2 Loss: 249710.5156\n",
      "Epoch184 Batch4 Loss: 256968.4844\n",
      "train Loss: 274500.4350\n",
      "Epoch 185/499\n",
      "----------\n",
      "Epoch185 Batch0 Loss: 318017.8750\n",
      "Epoch185 Batch2 Loss: 230555.8438\n",
      "Epoch185 Batch4 Loss: 250248.4844\n",
      "train Loss: 264788.0275\n",
      "Epoch 186/499\n",
      "----------\n",
      "Epoch186 Batch0 Loss: 371981.6562\n",
      "Epoch186 Batch2 Loss: 240526.5312\n",
      "Epoch186 Batch4 Loss: 279454.9375\n",
      "train Loss: 281700.7700\n",
      "Epoch 187/499\n",
      "----------\n",
      "Epoch187 Batch0 Loss: 381695.7188\n",
      "Epoch187 Batch2 Loss: 269812.6562\n",
      "Epoch187 Batch4 Loss: 235625.3125\n",
      "train Loss: 276921.3825\n",
      "Epoch 188/499\n",
      "----------\n",
      "Epoch188 Batch0 Loss: 411082.4062\n",
      "Epoch188 Batch2 Loss: 266460.7188\n",
      "Epoch188 Batch4 Loss: 312473.3438\n",
      "train Loss: 274444.6575\n",
      "Epoch 189/499\n",
      "----------\n",
      "Epoch189 Batch0 Loss: 331823.8750\n",
      "Epoch189 Batch2 Loss: 265929.0938\n",
      "Epoch189 Batch4 Loss: 292882.1875\n",
      "train Loss: 265577.2375\n",
      "Epoch 190/499\n",
      "----------\n",
      "Epoch190 Batch0 Loss: 275685.0312\n",
      "Epoch190 Batch2 Loss: 282600.3750\n",
      "Epoch190 Batch4 Loss: 289167.1875\n",
      "train Loss: 263219.3500\n",
      "Epoch 191/499\n",
      "----------\n",
      "Epoch191 Batch0 Loss: 244829.9062\n",
      "Epoch191 Batch2 Loss: 255180.6250\n",
      "Epoch191 Batch4 Loss: 259825.4375\n",
      "train Loss: 251083.7900\n",
      "Epoch 192/499\n",
      "----------\n",
      "Epoch192 Batch0 Loss: 223772.2344\n",
      "Epoch192 Batch2 Loss: 233675.4844\n",
      "Epoch192 Batch4 Loss: 200175.6562\n",
      "train Loss: 246376.2825\n",
      "Epoch 193/499\n",
      "----------\n",
      "Epoch193 Batch0 Loss: 209667.6875\n",
      "Epoch193 Batch2 Loss: 284512.3438\n",
      "Epoch193 Batch4 Loss: 187680.4375\n",
      "train Loss: 239706.7825\n",
      "Epoch 194/499\n",
      "----------\n",
      "Epoch194 Batch0 Loss: 215002.7344\n",
      "Epoch194 Batch2 Loss: 324041.0938\n",
      "Epoch194 Batch4 Loss: 203338.6250\n",
      "train Loss: 243651.3525\n",
      "Epoch 195/499\n",
      "----------\n",
      "Epoch195 Batch0 Loss: 208071.0312\n",
      "Epoch195 Batch2 Loss: 330392.4062\n",
      "Epoch195 Batch4 Loss: 217408.9062\n",
      "train Loss: 236119.2975\n",
      "Epoch 196/499\n",
      "----------\n",
      "Epoch196 Batch0 Loss: 212477.1250\n",
      "Epoch196 Batch2 Loss: 336501.1250\n",
      "Epoch196 Batch4 Loss: 229238.9531\n",
      "train Loss: 222411.8800\n",
      "Epoch 197/499\n",
      "----------\n",
      "Epoch197 Batch0 Loss: 268747.8750\n",
      "Epoch197 Batch2 Loss: 279260.0938\n",
      "Epoch197 Batch4 Loss: 222289.5312\n",
      "train Loss: 225977.9125\n",
      "Epoch 198/499\n",
      "----------\n",
      "Epoch198 Batch0 Loss: 263394.7812\n",
      "Epoch198 Batch2 Loss: 236636.2656\n",
      "Epoch198 Batch4 Loss: 246441.0938\n",
      "train Loss: 229372.5050\n",
      "Epoch 199/499\n",
      "----------\n",
      "Epoch199 Batch0 Loss: 257010.7188\n",
      "Epoch199 Batch2 Loss: 227475.4219\n",
      "Epoch199 Batch4 Loss: 231240.5000\n",
      "train Loss: 230213.5875\n",
      "Epoch 200/499\n",
      "----------\n",
      "Epoch200 Batch0 Loss: 234608.0469\n",
      "Epoch200 Batch2 Loss: 202632.3594\n",
      "Epoch200 Batch4 Loss: 239087.9219\n",
      "train Loss: 231551.2600\n",
      "Epoch 201/499\n",
      "----------\n",
      "Epoch201 Batch0 Loss: 186276.9531\n",
      "Epoch201 Batch2 Loss: 204325.8750\n",
      "Epoch201 Batch4 Loss: 275051.1250\n",
      "train Loss: 232293.5325\n",
      "Epoch 202/499\n",
      "----------\n",
      "Epoch202 Batch0 Loss: 173141.0625\n",
      "Epoch202 Batch2 Loss: 209245.7031\n",
      "Epoch202 Batch4 Loss: 309589.0312\n",
      "train Loss: 233715.5300\n",
      "Epoch 203/499\n",
      "----------\n",
      "Epoch203 Batch0 Loss: 179807.3438\n",
      "Epoch203 Batch2 Loss: 199340.6875\n",
      "Epoch203 Batch4 Loss: 318941.0312\n",
      "train Loss: 224984.4650\n",
      "Epoch 204/499\n",
      "----------\n",
      "Epoch204 Batch0 Loss: 206297.0000\n",
      "Epoch204 Batch2 Loss: 204080.9375\n",
      "Epoch204 Batch4 Loss: 351417.2188\n",
      "train Loss: 232338.1875\n",
      "Epoch 205/499\n",
      "----------\n",
      "Epoch205 Batch0 Loss: 235702.7031\n",
      "Epoch205 Batch2 Loss: 269012.4375\n",
      "Epoch205 Batch4 Loss: 291943.5938\n",
      "train Loss: 236382.0575\n",
      "Epoch 206/499\n",
      "----------\n",
      "Epoch206 Batch0 Loss: 248875.2969\n",
      "Epoch206 Batch2 Loss: 284902.6250\n",
      "Epoch206 Batch4 Loss: 244006.6406\n",
      "train Loss: 239085.6025\n",
      "Epoch 207/499\n",
      "----------\n",
      "Epoch207 Batch0 Loss: 253925.0625\n",
      "Epoch207 Batch2 Loss: 261873.0781\n",
      "Epoch207 Batch4 Loss: 240001.3750\n",
      "train Loss: 240410.6850\n",
      "Epoch 208/499\n",
      "----------\n",
      "Epoch208 Batch0 Loss: 235085.0156\n",
      "Epoch208 Batch2 Loss: 272549.1250\n",
      "Epoch208 Batch4 Loss: 220571.4219\n",
      "train Loss: 256227.5975\n",
      "Epoch 209/499\n",
      "----------\n",
      "Epoch209 Batch0 Loss: 226602.3125\n",
      "Epoch209 Batch2 Loss: 229033.2812\n",
      "Epoch209 Batch4 Loss: 221430.1562\n",
      "train Loss: 244037.2000\n",
      "Epoch 210/499\n",
      "----------\n",
      "Epoch210 Batch0 Loss: 321420.1875\n",
      "Epoch210 Batch2 Loss: 209927.3750\n",
      "Epoch210 Batch4 Loss: 241350.6250\n",
      "train Loss: 252273.2750\n",
      "Epoch 211/499\n",
      "----------\n",
      "Epoch211 Batch0 Loss: 330088.6562\n",
      "Epoch211 Batch2 Loss: 195829.6562\n",
      "Epoch211 Batch4 Loss: 221262.2656\n",
      "train Loss: 243756.9350\n",
      "Epoch 212/499\n",
      "----------\n",
      "Epoch212 Batch0 Loss: 332300.5625\n",
      "Epoch212 Batch2 Loss: 211047.4531\n",
      "Epoch212 Batch4 Loss: 210863.5312\n",
      "train Loss: 230124.8475\n",
      "Epoch 213/499\n",
      "----------\n",
      "Epoch213 Batch0 Loss: 338164.2812\n",
      "Epoch213 Batch2 Loss: 230324.6562\n",
      "Epoch213 Batch4 Loss: 269753.7812\n",
      "train Loss: 231284.1800\n",
      "Epoch 214/499\n",
      "----------\n",
      "Epoch214 Batch0 Loss: 277939.3438\n",
      "Epoch214 Batch2 Loss: 224777.3438\n",
      "Epoch214 Batch4 Loss: 264177.5312\n",
      "train Loss: 227138.3000\n",
      "Epoch 215/499\n",
      "----------\n",
      "Epoch215 Batch0 Loss: 232226.5312\n",
      "Epoch215 Batch2 Loss: 252599.6562\n",
      "Epoch215 Batch4 Loss: 246745.4062\n",
      "train Loss: 225688.9500\n",
      "Epoch 216/499\n",
      "----------\n",
      "Epoch216 Batch0 Loss: 224026.0625\n",
      "Epoch216 Batch2 Loss: 222105.0312\n",
      "Epoch216 Batch4 Loss: 226684.9844\n",
      "train Loss: 221376.7250\n",
      "Epoch 217/499\n",
      "----------\n",
      "Epoch217 Batch0 Loss: 197193.9375\n",
      "Epoch217 Batch2 Loss: 214761.0000\n",
      "Epoch217 Batch4 Loss: 177744.7500\n",
      "train Loss: 220311.6150\n",
      "Epoch 218/499\n",
      "----------\n",
      "Epoch218 Batch0 Loss: 184287.8750\n",
      "Epoch218 Batch2 Loss: 266025.0625\n",
      "Epoch218 Batch4 Loss: 158193.0469\n",
      "train Loss: 214883.0200\n",
      "Epoch 219/499\n",
      "----------\n",
      "Epoch219 Batch0 Loss: 188749.5469\n",
      "Epoch219 Batch2 Loss: 292376.9375\n",
      "Epoch219 Batch4 Loss: 164477.7188\n",
      "train Loss: 214923.8375\n",
      "Epoch 220/499\n",
      "----------\n",
      "Epoch220 Batch0 Loss: 185465.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch220 Batch2 Loss: 298526.3750\n",
      "Epoch220 Batch4 Loss: 186855.7500\n",
      "train Loss: 210703.5725\n",
      "Epoch 221/499\n",
      "----------\n",
      "Epoch221 Batch0 Loss: 194687.4219\n",
      "Epoch221 Batch2 Loss: 309280.2500\n",
      "Epoch221 Batch4 Loss: 215474.2812\n",
      "train Loss: 203731.6350\n",
      "Epoch 222/499\n",
      "----------\n",
      "Epoch222 Batch0 Loss: 249815.4219\n",
      "Epoch222 Batch2 Loss: 275040.8125\n",
      "Epoch222 Batch4 Loss: 219250.0625\n",
      "train Loss: 215109.7625\n",
      "Epoch 223/499\n",
      "----------\n",
      "Epoch223 Batch0 Loss: 253616.2188\n",
      "Epoch223 Batch2 Loss: 247948.5312\n",
      "Epoch223 Batch4 Loss: 232370.0156\n",
      "train Loss: 222481.4200\n",
      "Epoch 224/499\n",
      "----------\n",
      "Epoch224 Batch0 Loss: 286947.1250\n",
      "Epoch224 Batch2 Loss: 227853.0781\n",
      "Epoch224 Batch4 Loss: 236560.4844\n",
      "train Loss: 236859.0000\n",
      "Epoch 225/499\n",
      "----------\n",
      "Epoch225 Batch0 Loss: 247776.0938\n",
      "Epoch225 Batch2 Loss: 211202.9375\n",
      "Epoch225 Batch4 Loss: 230734.6406\n",
      "train Loss: 237164.1500\n",
      "Epoch 226/499\n",
      "----------\n",
      "Epoch226 Batch0 Loss: 171803.0625\n",
      "Epoch226 Batch2 Loss: 209385.3594\n",
      "Epoch226 Batch4 Loss: 264279.2188\n",
      "train Loss: 225807.0325\n",
      "Epoch 227/499\n",
      "----------\n",
      "Epoch227 Batch0 Loss: 186368.2969\n",
      "Epoch227 Batch2 Loss: 197988.3125\n",
      "Epoch227 Batch4 Loss: 308243.4375\n",
      "train Loss: 230366.9025\n",
      "Epoch 228/499\n",
      "----------\n",
      "Epoch228 Batch0 Loss: 176291.2969\n",
      "Epoch228 Batch2 Loss: 200325.4844\n",
      "Epoch228 Batch4 Loss: 308277.8750\n",
      "train Loss: 222080.2450\n",
      "Epoch 229/499\n",
      "----------\n",
      "Epoch229 Batch0 Loss: 200586.0938\n",
      "Epoch229 Batch2 Loss: 193269.9219\n",
      "Epoch229 Batch4 Loss: 321681.1250\n",
      "train Loss: 215932.4800\n",
      "Epoch 230/499\n",
      "----------\n",
      "Epoch230 Batch0 Loss: 217627.3438\n",
      "Epoch230 Batch2 Loss: 250712.8594\n",
      "Epoch230 Batch4 Loss: 265310.2812\n",
      "train Loss: 213711.0650\n",
      "Epoch 231/499\n",
      "----------\n",
      "Epoch231 Batch0 Loss: 203775.2344\n",
      "Epoch231 Batch2 Loss: 251410.8750\n",
      "Epoch231 Batch4 Loss: 220042.1094\n",
      "train Loss: 208070.4050\n",
      "Epoch 232/499\n",
      "----------\n",
      "Epoch232 Batch0 Loss: 235531.3906\n",
      "Epoch232 Batch2 Loss: 238186.8594\n",
      "Epoch232 Batch4 Loss: 213740.9688\n",
      "train Loss: 214344.2525\n",
      "Epoch 233/499\n",
      "----------\n",
      "Epoch233 Batch0 Loss: 210795.8438\n",
      "Epoch233 Batch2 Loss: 229653.6875\n",
      "Epoch233 Batch4 Loss: 187177.0312\n",
      "train Loss: 210084.0300\n",
      "Epoch 234/499\n",
      "----------\n",
      "Epoch234 Batch0 Loss: 201805.5312\n",
      "Epoch234 Batch2 Loss: 167918.5938\n",
      "Epoch234 Batch4 Loss: 177312.4531\n",
      "train Loss: 201314.6075\n",
      "Epoch 235/499\n",
      "----------\n",
      "Epoch235 Batch0 Loss: 261317.0938\n",
      "Epoch235 Batch2 Loss: 144892.9688\n",
      "Epoch235 Batch4 Loss: 185097.6562\n",
      "train Loss: 200699.4700\n",
      "Epoch 236/499\n",
      "----------\n",
      "Epoch236 Batch0 Loss: 274306.1875\n",
      "Epoch236 Batch2 Loss: 165064.3750\n",
      "Epoch236 Batch4 Loss: 172391.1875\n",
      "train Loss: 206330.9050\n",
      "Epoch 237/499\n",
      "----------\n",
      "Epoch237 Batch0 Loss: 306726.8750\n",
      "Epoch237 Batch2 Loss: 178807.4219\n",
      "Epoch237 Batch4 Loss: 189353.8750\n",
      "train Loss: 207162.0675\n",
      "Epoch 238/499\n",
      "----------\n",
      "Epoch238 Batch0 Loss: 302883.1250\n",
      "Epoch238 Batch2 Loss: 218421.8438\n",
      "Epoch238 Batch4 Loss: 249944.8594\n",
      "train Loss: 210681.8800\n",
      "Epoch 239/499\n",
      "----------\n",
      "Epoch239 Batch0 Loss: 257580.5312\n",
      "Epoch239 Batch2 Loss: 200349.8125\n",
      "Epoch239 Batch4 Loss: 246224.4531\n",
      "train Loss: 206567.3150\n",
      "Epoch 240/499\n",
      "----------\n",
      "Epoch240 Batch0 Loss: 219806.2188\n",
      "Epoch240 Batch2 Loss: 223253.5000\n",
      "Epoch240 Batch4 Loss: 236629.8125\n",
      "train Loss: 207832.3725\n",
      "Epoch 241/499\n",
      "----------\n",
      "Epoch241 Batch0 Loss: 205536.2656\n",
      "Epoch241 Batch2 Loss: 203750.5781\n",
      "Epoch241 Batch4 Loss: 213108.1562\n",
      "train Loss: 203509.2000\n",
      "Epoch 242/499\n",
      "----------\n",
      "Epoch242 Batch0 Loss: 177148.7500\n",
      "Epoch242 Batch2 Loss: 191153.2812\n",
      "Epoch242 Batch4 Loss: 152789.4844\n",
      "train Loss: 199067.1175\n",
      "Epoch 243/499\n",
      "----------\n",
      "Epoch243 Batch0 Loss: 166526.9688\n",
      "Epoch243 Batch2 Loss: 239751.7344\n",
      "Epoch243 Batch4 Loss: 135007.2344\n",
      "train Loss: 192496.2825\n",
      "Epoch 244/499\n",
      "----------\n",
      "Epoch244 Batch0 Loss: 169526.4688\n",
      "Epoch244 Batch2 Loss: 264589.7812\n",
      "Epoch244 Batch4 Loss: 146896.0938\n",
      "train Loss: 195277.7100\n",
      "Epoch 245/499\n",
      "----------\n",
      "Epoch245 Batch0 Loss: 164262.7188\n",
      "Epoch245 Batch2 Loss: 289827.5625\n",
      "Epoch245 Batch4 Loss: 165841.4688\n",
      "train Loss: 197043.6275\n",
      "Epoch 246/499\n",
      "----------\n",
      "Epoch246 Batch0 Loss: 187181.8906\n",
      "Epoch246 Batch2 Loss: 285571.9375\n",
      "Epoch246 Batch4 Loss: 194444.7344\n",
      "train Loss: 190563.5525\n",
      "Epoch 247/499\n",
      "----------\n",
      "Epoch247 Batch0 Loss: 233762.7031\n",
      "Epoch247 Batch2 Loss: 257046.8594\n",
      "Epoch247 Batch4 Loss: 195906.8281\n",
      "train Loss: 197946.1050\n",
      "Epoch 248/499\n",
      "----------\n",
      "Epoch248 Batch0 Loss: 250114.9219\n",
      "Epoch248 Batch2 Loss: 209406.2031\n",
      "Epoch248 Batch4 Loss: 221095.3281\n",
      "train Loss: 205043.0450\n",
      "Epoch 249/499\n",
      "----------\n",
      "Epoch249 Batch0 Loss: 224081.2031\n",
      "Epoch249 Batch2 Loss: 202820.0156\n",
      "Epoch249 Batch4 Loss: 204086.2969\n",
      "train Loss: 206718.6825\n",
      "Epoch 250/499\n",
      "----------\n",
      "Epoch250 Batch0 Loss: 223766.8906\n",
      "Epoch250 Batch2 Loss: 194645.5156\n",
      "Epoch250 Batch4 Loss: 195783.8438\n",
      "train Loss: 210868.2650\n",
      "Epoch 251/499\n",
      "----------\n",
      "Epoch251 Batch0 Loss: 177789.4688\n",
      "Epoch251 Batch2 Loss: 174302.2031\n",
      "Epoch251 Batch4 Loss: 245523.8750\n",
      "train Loss: 209378.7250\n",
      "Epoch 252/499\n",
      "----------\n",
      "Epoch252 Batch0 Loss: 159995.2031\n",
      "Epoch252 Batch2 Loss: 180582.7969\n",
      "Epoch252 Batch4 Loss: 274366.0000\n",
      "train Loss: 208179.8175\n",
      "Epoch 253/499\n",
      "----------\n",
      "Epoch253 Batch0 Loss: 151504.4219\n",
      "Epoch253 Batch2 Loss: 169612.3750\n",
      "Epoch253 Batch4 Loss: 283380.0625\n",
      "train Loss: 196738.9300\n",
      "Epoch 254/499\n",
      "----------\n",
      "Epoch254 Batch0 Loss: 173501.9219\n",
      "Epoch254 Batch2 Loss: 176972.1875\n",
      "Epoch254 Batch4 Loss: 287835.6250\n",
      "train Loss: 194005.1750\n",
      "Epoch 255/499\n",
      "----------\n",
      "Epoch255 Batch0 Loss: 183104.0000\n",
      "Epoch255 Batch2 Loss: 227884.6250\n",
      "Epoch255 Batch4 Loss: 230535.5938\n",
      "train Loss: 188874.9725\n",
      "Epoch 256/499\n",
      "----------\n",
      "Epoch256 Batch0 Loss: 181611.4375\n",
      "Epoch256 Batch2 Loss: 224936.8281\n",
      "Epoch256 Batch4 Loss: 193673.6250\n",
      "train Loss: 182706.8763\n",
      "Epoch 257/499\n",
      "----------\n",
      "Epoch257 Batch0 Loss: 203566.3906\n",
      "Epoch257 Batch2 Loss: 210476.8281\n",
      "Epoch257 Batch4 Loss: 182234.9844\n",
      "train Loss: 184357.3475\n",
      "Epoch 258/499\n",
      "----------\n",
      "Epoch258 Batch0 Loss: 182541.5938\n",
      "Epoch258 Batch2 Loss: 192935.1250\n",
      "Epoch258 Batch4 Loss: 162756.3438\n",
      "train Loss: 179587.3000\n",
      "Epoch 259/499\n",
      "----------\n",
      "Epoch259 Batch0 Loss: 176260.2031\n",
      "Epoch259 Batch2 Loss: 137520.8594\n",
      "Epoch259 Batch4 Loss: 151499.5625\n",
      "train Loss: 171889.2675\n",
      "Epoch 260/499\n",
      "----------\n",
      "Epoch260 Batch0 Loss: 222128.8125\n",
      "Epoch260 Batch2 Loss: 118418.7656\n",
      "Epoch260 Batch4 Loss: 152102.1250\n",
      "train Loss: 170890.4475\n",
      "Epoch 261/499\n",
      "----------\n",
      "Epoch261 Batch0 Loss: 240838.9375\n",
      "Epoch261 Batch2 Loss: 131332.5781\n",
      "Epoch261 Batch4 Loss: 150353.9688\n",
      "train Loss: 177535.4775\n",
      "Epoch 262/499\n",
      "----------\n",
      "Epoch262 Batch0 Loss: 262743.2188\n",
      "Epoch262 Batch2 Loss: 148965.3281\n",
      "Epoch262 Batch4 Loss: 167860.0938\n",
      "train Loss: 178247.3425\n",
      "Epoch 263/499\n",
      "----------\n",
      "Epoch263 Batch0 Loss: 263033.3125\n",
      "Epoch263 Batch2 Loss: 181891.2812\n",
      "Epoch263 Batch4 Loss: 222721.3438\n",
      "train Loss: 183410.7625\n",
      "Epoch 264/499\n",
      "----------\n",
      "Epoch264 Batch0 Loss: 238189.2500\n",
      "Epoch264 Batch2 Loss: 184218.6094\n",
      "Epoch264 Batch4 Loss: 236203.7656\n",
      "train Loss: 189653.4000\n",
      "Epoch 265/499\n",
      "----------\n",
      "Epoch265 Batch0 Loss: 201783.9531\n",
      "Epoch265 Batch2 Loss: 227030.4688\n",
      "Epoch265 Batch4 Loss: 223669.2344\n",
      "train Loss: 197845.1925\n",
      "Epoch 266/499\n",
      "----------\n",
      "Epoch266 Batch0 Loss: 197335.4219\n",
      "Epoch266 Batch2 Loss: 198775.7031\n",
      "Epoch266 Batch4 Loss: 208322.8594\n",
      "train Loss: 197876.9275\n",
      "Epoch 267/499\n",
      "----------\n",
      "Epoch267 Batch0 Loss: 185031.4688\n",
      "Epoch267 Batch2 Loss: 187794.5625\n",
      "Epoch267 Batch4 Loss: 150416.7969\n",
      "train Loss: 202282.0300\n",
      "Epoch 268/499\n",
      "----------\n",
      "Epoch268 Batch0 Loss: 163382.9062\n",
      "Epoch268 Batch2 Loss: 250743.7969\n",
      "Epoch268 Batch4 Loss: 140476.5938\n",
      "train Loss: 196284.6700\n",
      "Epoch 269/499\n",
      "----------\n",
      "Epoch269 Batch0 Loss: 182913.3125\n",
      "Epoch269 Batch2 Loss: 262484.4062\n",
      "Epoch269 Batch4 Loss: 154779.8750\n",
      "train Loss: 200322.4675\n",
      "Epoch 270/499\n",
      "----------\n",
      "Epoch270 Batch0 Loss: 169500.8438\n",
      "Epoch270 Batch2 Loss: 276486.3438\n",
      "Epoch270 Batch4 Loss: 176701.6250\n",
      "train Loss: 193127.2125\n",
      "Epoch 271/499\n",
      "----------\n",
      "Epoch271 Batch0 Loss: 173341.5781\n",
      "Epoch271 Batch2 Loss: 287559.6250\n",
      "Epoch271 Batch4 Loss: 176650.0312\n",
      "train Loss: 182299.3650\n",
      "Epoch 272/499\n",
      "----------\n",
      "Epoch272 Batch0 Loss: 226284.6875\n",
      "Epoch272 Batch2 Loss: 219758.5781\n",
      "Epoch272 Batch4 Loss: 183841.8438\n",
      "train Loss: 182272.8737\n",
      "Epoch 273/499\n",
      "----------\n",
      "Epoch273 Batch0 Loss: 220515.1250\n",
      "Epoch273 Batch2 Loss: 196158.2656\n",
      "Epoch273 Batch4 Loss: 196646.8125\n",
      "train Loss: 184057.7075\n",
      "Epoch 274/499\n",
      "----------\n",
      "Epoch274 Batch0 Loss: 209988.7188\n",
      "Epoch274 Batch2 Loss: 176417.0625\n",
      "Epoch274 Batch4 Loss: 205781.4531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 184484.9525\n",
      "Epoch 275/499\n",
      "----------\n",
      "Epoch275 Batch0 Loss: 186764.5156\n",
      "Epoch275 Batch2 Loss: 166870.7188\n",
      "Epoch275 Batch4 Loss: 185522.9062\n",
      "train Loss: 187920.5075\n",
      "Epoch 276/499\n",
      "----------\n",
      "Epoch276 Batch0 Loss: 163407.1875\n",
      "Epoch276 Batch2 Loss: 161622.1719\n",
      "Epoch276 Batch4 Loss: 236668.9688\n",
      "train Loss: 193599.8800\n",
      "Epoch 277/499\n",
      "----------\n",
      "Epoch277 Batch0 Loss: 134582.6875\n",
      "Epoch277 Batch2 Loss: 168839.1562\n",
      "Epoch277 Batch4 Loss: 258975.0781\n",
      "train Loss: 192382.1300\n",
      "Epoch 278/499\n",
      "----------\n",
      "Epoch278 Batch0 Loss: 139123.9062\n",
      "Epoch278 Batch2 Loss: 162627.0625\n",
      "Epoch278 Batch4 Loss: 272600.0938\n",
      "train Loss: 185106.9325\n",
      "Epoch 279/499\n",
      "----------\n",
      "Epoch279 Batch0 Loss: 160527.3438\n",
      "Epoch279 Batch2 Loss: 170276.9531\n",
      "Epoch279 Batch4 Loss: 271061.3438\n",
      "train Loss: 184817.2750\n",
      "Epoch 280/499\n",
      "----------\n",
      "Epoch280 Batch0 Loss: 177427.5156\n",
      "Epoch280 Batch2 Loss: 217738.7812\n",
      "Epoch280 Batch4 Loss: 222018.2031\n",
      "train Loss: 180054.2600\n",
      "Epoch 281/499\n",
      "----------\n",
      "Epoch281 Batch0 Loss: 171885.3125\n",
      "Epoch281 Batch2 Loss: 219642.3438\n",
      "Epoch281 Batch4 Loss: 187342.3125\n",
      "train Loss: 175377.1975\n",
      "Epoch 282/499\n",
      "----------\n",
      "Epoch282 Batch0 Loss: 197988.0625\n",
      "Epoch282 Batch2 Loss: 202138.4219\n",
      "Epoch282 Batch4 Loss: 172671.6250\n",
      "train Loss: 175510.5487\n",
      "Epoch 283/499\n",
      "----------\n",
      "Epoch283 Batch0 Loss: 176266.5625\n",
      "Epoch283 Batch2 Loss: 176314.0938\n",
      "Epoch283 Batch4 Loss: 152750.3125\n",
      "train Loss: 168643.2100\n",
      "Epoch 284/499\n",
      "----------\n",
      "Epoch284 Batch0 Loss: 163810.9688\n",
      "Epoch284 Batch2 Loss: 127521.2500\n",
      "Epoch284 Batch4 Loss: 142171.9062\n",
      "train Loss: 159754.5700\n",
      "Epoch 285/499\n",
      "----------\n",
      "Epoch285 Batch0 Loss: 206672.6406\n",
      "Epoch285 Batch2 Loss: 108167.9141\n",
      "Epoch285 Batch4 Loss: 141997.4688\n",
      "train Loss: 159532.4987\n",
      "Epoch 286/499\n",
      "----------\n",
      "Epoch286 Batch0 Loss: 224020.0000\n",
      "Epoch286 Batch2 Loss: 122696.5781\n",
      "Epoch286 Batch4 Loss: 139930.0312\n",
      "train Loss: 164692.2400\n",
      "Epoch 287/499\n",
      "----------\n",
      "Epoch287 Batch0 Loss: 245638.2812\n",
      "Epoch287 Batch2 Loss: 138069.2031\n",
      "Epoch287 Batch4 Loss: 154266.3281\n",
      "train Loss: 166459.3375\n",
      "Epoch 288/499\n",
      "----------\n",
      "Epoch288 Batch0 Loss: 250017.8438\n",
      "Epoch288 Batch2 Loss: 160782.1562\n",
      "Epoch288 Batch4 Loss: 208502.7969\n",
      "train Loss: 168992.5425\n",
      "Epoch 289/499\n",
      "----------\n",
      "Epoch289 Batch0 Loss: 214070.9531\n",
      "Epoch289 Batch2 Loss: 166258.8281\n",
      "Epoch289 Batch4 Loss: 207146.4688\n",
      "train Loss: 169701.5275\n",
      "Epoch 290/499\n",
      "----------\n",
      "Epoch290 Batch0 Loss: 179472.6719\n",
      "Epoch290 Batch2 Loss: 199568.1250\n",
      "Epoch290 Batch4 Loss: 193182.5781\n",
      "train Loss: 171860.3563\n",
      "Epoch 291/499\n",
      "----------\n",
      "Epoch291 Batch0 Loss: 174813.2969\n",
      "Epoch291 Batch2 Loss: 166728.2500\n",
      "Epoch291 Batch4 Loss: 187306.2500\n",
      "train Loss: 173330.6150\n",
      "Epoch 292/499\n",
      "----------\n",
      "Epoch292 Batch0 Loss: 156582.3438\n",
      "Epoch292 Batch2 Loss: 164873.8594\n",
      "Epoch292 Batch4 Loss: 139372.7812\n",
      "train Loss: 175920.8450\n",
      "Epoch 293/499\n",
      "----------\n",
      "Epoch293 Batch0 Loss: 154725.1562\n",
      "Epoch293 Batch2 Loss: 206087.2188\n",
      "Epoch293 Batch4 Loss: 112507.7734\n",
      "train Loss: 170570.8788\n",
      "Epoch 294/499\n",
      "----------\n",
      "Epoch294 Batch0 Loss: 143112.5781\n",
      "Epoch294 Batch2 Loss: 234217.5781\n",
      "Epoch294 Batch4 Loss: 123171.4297\n",
      "train Loss: 168747.4488\n",
      "Epoch 295/499\n",
      "----------\n",
      "Epoch295 Batch0 Loss: 141591.5625\n",
      "Epoch295 Batch2 Loss: 238136.8281\n",
      "Epoch295 Batch4 Loss: 137278.3750\n",
      "train Loss: 165768.6300\n",
      "Epoch 296/499\n",
      "----------\n",
      "Epoch296 Batch0 Loss: 154889.1406\n",
      "Epoch296 Batch2 Loss: 243681.8906\n",
      "Epoch296 Batch4 Loss: 157838.7656\n",
      "train Loss: 157542.5862\n",
      "Epoch 297/499\n",
      "----------\n",
      "Epoch297 Batch0 Loss: 198015.6250\n",
      "Epoch297 Batch2 Loss: 199382.1094\n",
      "Epoch297 Batch4 Loss: 157157.0312\n",
      "train Loss: 160500.6137\n",
      "Epoch 298/499\n",
      "----------\n",
      "Epoch298 Batch0 Loss: 198916.1094\n",
      "Epoch298 Batch2 Loss: 169662.2812\n",
      "Epoch298 Batch4 Loss: 178662.4531\n",
      "train Loss: 162504.2563\n",
      "Epoch 299/499\n",
      "----------\n",
      "Epoch299 Batch0 Loss: 184634.3594\n",
      "Epoch299 Batch2 Loss: 157724.6094\n",
      "Epoch299 Batch4 Loss: 157893.7656\n",
      "train Loss: 158607.7200\n",
      "Epoch 300/499\n",
      "----------\n",
      "Epoch300 Batch0 Loss: 162226.3906\n",
      "Epoch300 Batch2 Loss: 139025.3594\n",
      "Epoch300 Batch4 Loss: 149713.8125\n",
      "train Loss: 154560.9050\n",
      "Epoch 301/499\n",
      "----------\n",
      "Epoch301 Batch0 Loss: 114930.0859\n",
      "Epoch301 Batch2 Loss: 129657.1875\n",
      "Epoch301 Batch4 Loss: 192431.7188\n",
      "train Loss: 152274.6013\n",
      "Epoch 302/499\n",
      "----------\n",
      "Epoch302 Batch0 Loss: 95730.6641\n",
      "Epoch302 Batch2 Loss: 125796.2500\n",
      "Epoch302 Batch4 Loss: 203700.1406\n",
      "train Loss: 150131.4062\n",
      "Epoch 303/499\n",
      "----------\n",
      "Epoch303 Batch0 Loss: 102514.0547\n",
      "Epoch303 Batch2 Loss: 125976.3750\n",
      "Epoch303 Batch4 Loss: 215709.2500\n",
      "train Loss: 146468.3563\n",
      "Epoch 304/499\n",
      "----------\n",
      "Epoch304 Batch0 Loss: 123933.4922\n",
      "Epoch304 Batch2 Loss: 131913.3906\n",
      "Epoch304 Batch4 Loss: 228870.1406\n",
      "train Loss: 148237.3412\n",
      "Epoch 305/499\n",
      "----------\n",
      "Epoch305 Batch0 Loss: 143552.5781\n",
      "Epoch305 Batch2 Loss: 187949.7500\n",
      "Epoch305 Batch4 Loss: 189595.4375\n",
      "train Loss: 152178.7375\n",
      "Epoch 306/499\n",
      "----------\n",
      "Epoch306 Batch0 Loss: 155266.8125\n",
      "Epoch306 Batch2 Loss: 196366.8906\n",
      "Epoch306 Batch4 Loss: 172948.5938\n",
      "train Loss: 158460.1375\n",
      "Epoch 307/499\n",
      "----------\n",
      "Epoch307 Batch0 Loss: 177829.7656\n",
      "Epoch307 Batch2 Loss: 195038.6406\n",
      "Epoch307 Batch4 Loss: 161124.9219\n",
      "train Loss: 166222.7087\n",
      "Epoch 308/499\n",
      "----------\n",
      "Epoch308 Batch0 Loss: 169814.2188\n",
      "Epoch308 Batch2 Loss: 174596.2500\n",
      "Epoch308 Batch4 Loss: 148892.8125\n",
      "train Loss: 164777.5950\n",
      "Epoch 309/499\n",
      "----------\n",
      "Epoch309 Batch0 Loss: 157600.0000\n",
      "Epoch309 Batch2 Loss: 119863.2656\n",
      "Epoch309 Batch4 Loss: 137925.5781\n",
      "train Loss: 155882.6575\n",
      "Epoch 310/499\n",
      "----------\n",
      "Epoch310 Batch0 Loss: 200764.0312\n",
      "Epoch310 Batch2 Loss: 107636.1797\n",
      "Epoch310 Batch4 Loss: 132354.2500\n",
      "train Loss: 151716.8488\n",
      "Epoch 311/499\n",
      "----------\n",
      "Epoch311 Batch0 Loss: 207943.6719\n",
      "Epoch311 Batch2 Loss: 121557.2109\n",
      "Epoch311 Batch4 Loss: 129720.6172\n",
      "train Loss: 154753.3300\n",
      "Epoch 312/499\n",
      "----------\n",
      "Epoch312 Batch0 Loss: 230404.5938\n",
      "Epoch312 Batch2 Loss: 124540.5859\n",
      "Epoch312 Batch4 Loss: 143484.0938\n",
      "train Loss: 153535.5138\n",
      "Epoch 313/499\n",
      "----------\n",
      "Epoch313 Batch0 Loss: 225890.3281\n",
      "Epoch313 Batch2 Loss: 147783.5000\n",
      "Epoch313 Batch4 Loss: 187783.9688\n",
      "train Loss: 152036.9250\n",
      "Epoch 314/499\n",
      "----------\n",
      "Epoch314 Batch0 Loss: 192721.1875\n",
      "Epoch314 Batch2 Loss: 145153.7500\n",
      "Epoch314 Batch4 Loss: 189294.3750\n",
      "train Loss: 150811.3387\n",
      "Epoch 315/499\n",
      "----------\n",
      "Epoch315 Batch0 Loss: 154523.1562\n",
      "Epoch315 Batch2 Loss: 169945.6094\n",
      "Epoch315 Batch4 Loss: 169139.9375\n",
      "train Loss: 148998.4500\n",
      "Epoch 316/499\n",
      "----------\n",
      "Epoch316 Batch0 Loss: 148828.8750\n",
      "Epoch316 Batch2 Loss: 150786.4688\n",
      "Epoch316 Batch4 Loss: 161633.6875\n",
      "train Loss: 152315.0262\n",
      "Epoch 317/499\n",
      "----------\n",
      "Epoch317 Batch0 Loss: 136267.9844\n",
      "Epoch317 Batch2 Loss: 148616.3281\n",
      "Epoch317 Batch4 Loss: 113976.9766\n",
      "train Loss: 151947.6837\n",
      "Epoch 318/499\n",
      "----------\n",
      "Epoch318 Batch0 Loss: 128190.6328\n",
      "Epoch318 Batch2 Loss: 187707.3750\n",
      "Epoch318 Batch4 Loss: 96156.8984\n",
      "train Loss: 149674.1325\n",
      "Epoch 319/499\n",
      "----------\n",
      "Epoch319 Batch0 Loss: 125080.2734\n",
      "Epoch319 Batch2 Loss: 199103.5469\n",
      "Epoch319 Batch4 Loss: 114402.2031\n",
      "train Loss: 150171.9488\n",
      "Epoch 320/499\n",
      "----------\n",
      "Epoch320 Batch0 Loss: 121571.8594\n",
      "Epoch320 Batch2 Loss: 227835.7344\n",
      "Epoch320 Batch4 Loss: 122300.4062\n",
      "train Loss: 149752.1500\n",
      "Epoch 321/499\n",
      "----------\n",
      "Epoch321 Batch0 Loss: 144511.4219\n",
      "Epoch321 Batch2 Loss: 222070.0312\n",
      "Epoch321 Batch4 Loss: 150486.9844\n",
      "train Loss: 147275.2687\n",
      "Epoch 322/499\n",
      "----------\n",
      "Epoch322 Batch0 Loss: 190014.7969\n",
      "Epoch322 Batch2 Loss: 186830.5938\n",
      "Epoch322 Batch4 Loss: 146447.9531\n",
      "train Loss: 149274.7950\n",
      "Epoch 323/499\n",
      "----------\n",
      "Epoch323 Batch0 Loss: 184471.7656\n",
      "Epoch323 Batch2 Loss: 156277.3750\n",
      "Epoch323 Batch4 Loss: 163090.1406\n",
      "train Loss: 150843.8387\n",
      "Epoch 324/499\n",
      "----------\n",
      "Epoch324 Batch0 Loss: 178035.6094\n",
      "Epoch324 Batch2 Loss: 145342.7656\n",
      "Epoch324 Batch4 Loss: 150957.7969\n",
      "train Loss: 151929.0975\n",
      "Epoch 325/499\n",
      "----------\n",
      "Epoch325 Batch0 Loss: 151303.4062\n",
      "Epoch325 Batch2 Loss: 133160.2344\n",
      "Epoch325 Batch4 Loss: 143754.0156\n",
      "train Loss: 146947.6450\n",
      "Epoch 326/499\n",
      "----------\n",
      "Epoch326 Batch0 Loss: 109386.0938\n",
      "Epoch326 Batch2 Loss: 121553.3438\n",
      "Epoch326 Batch4 Loss: 174479.2500\n",
      "train Loss: 142793.4500\n",
      "Epoch 327/499\n",
      "----------\n",
      "Epoch327 Batch0 Loss: 92040.6719\n",
      "Epoch327 Batch2 Loss: 116928.4453\n",
      "Epoch327 Batch4 Loss: 187878.1406\n",
      "train Loss: 140440.4488\n",
      "Epoch 328/499\n",
      "----------\n",
      "Epoch328 Batch0 Loss: 97622.7734\n",
      "Epoch328 Batch2 Loss: 115069.4609\n",
      "Epoch328 Batch4 Loss: 197998.3438\n",
      "train Loss: 135330.0950\n",
      "Epoch 329/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch329 Batch0 Loss: 112798.6719\n",
      "Epoch329 Batch2 Loss: 118634.5000\n",
      "Epoch329 Batch4 Loss: 204486.7344\n",
      "train Loss: 133181.2275\n",
      "Epoch 330/499\n",
      "----------\n",
      "Epoch330 Batch0 Loss: 125031.6094\n",
      "Epoch330 Batch2 Loss: 165740.3281\n",
      "Epoch330 Batch4 Loss: 163157.0000\n",
      "train Loss: 134003.5700\n",
      "Epoch 331/499\n",
      "----------\n",
      "Epoch331 Batch0 Loss: 134438.6406\n",
      "Epoch331 Batch2 Loss: 165503.0625\n",
      "Epoch331 Batch4 Loss: 150765.2656\n",
      "train Loss: 134166.3975\n",
      "Epoch 332/499\n",
      "----------\n",
      "Epoch332 Batch0 Loss: 150540.1719\n",
      "Epoch332 Batch2 Loss: 161675.7656\n",
      "Epoch332 Batch4 Loss: 133636.2500\n",
      "train Loss: 136256.5088\n",
      "Epoch 333/499\n",
      "----------\n",
      "Epoch333 Batch0 Loss: 140115.7031\n",
      "Epoch333 Batch2 Loss: 139077.2812\n",
      "Epoch333 Batch4 Loss: 127019.1484\n",
      "train Loss: 136186.6663\n",
      "Epoch 334/499\n",
      "----------\n",
      "Epoch334 Batch0 Loss: 130592.9219\n",
      "Epoch334 Batch2 Loss: 103543.5156\n",
      "Epoch334 Batch4 Loss: 115541.6172\n",
      "train Loss: 130287.9075\n",
      "Epoch 335/499\n",
      "----------\n",
      "Epoch335 Batch0 Loss: 164760.7812\n",
      "Epoch335 Batch2 Loss: 87248.6719\n",
      "Epoch335 Batch4 Loss: 112982.9219\n",
      "train Loss: 128675.3625\n",
      "Epoch 336/499\n",
      "----------\n",
      "Epoch336 Batch0 Loss: 184908.2969\n",
      "Epoch336 Batch2 Loss: 91331.3906\n",
      "Epoch336 Batch4 Loss: 113749.5078\n",
      "train Loss: 132061.1713\n",
      "Epoch 337/499\n",
      "----------\n",
      "Epoch337 Batch0 Loss: 191817.2812\n",
      "Epoch337 Batch2 Loss: 111384.0312\n",
      "Epoch337 Batch4 Loss: 116882.7891\n",
      "train Loss: 129982.5688\n",
      "Epoch 338/499\n",
      "----------\n",
      "Epoch338 Batch0 Loss: 200243.7344\n",
      "Epoch338 Batch2 Loss: 122962.8203\n",
      "Epoch338 Batch4 Loss: 161538.6719\n",
      "train Loss: 131668.7025\n",
      "Epoch 339/499\n",
      "----------\n",
      "Epoch339 Batch0 Loss: 161298.0781\n",
      "Epoch339 Batch2 Loss: 126218.4141\n",
      "Epoch339 Batch4 Loss: 165214.3594\n",
      "train Loss: 129980.6700\n",
      "Epoch 340/499\n",
      "----------\n",
      "Epoch340 Batch0 Loss: 140575.9062\n",
      "Epoch340 Batch2 Loss: 147325.8906\n",
      "Epoch340 Batch4 Loss: 150930.9062\n",
      "train Loss: 131119.3025\n",
      "Epoch 341/499\n",
      "----------\n",
      "Epoch341 Batch0 Loss: 128675.6250\n",
      "Epoch341 Batch2 Loss: 134784.3594\n",
      "Epoch341 Batch4 Loss: 134630.0625\n",
      "train Loss: 130144.7787\n",
      "Epoch 342/499\n",
      "----------\n",
      "Epoch342 Batch0 Loss: 122201.7578\n",
      "Epoch342 Batch2 Loss: 124802.5234\n",
      "Epoch342 Batch4 Loss: 101579.1406\n",
      "train Loss: 133852.2675\n",
      "Epoch 343/499\n",
      "----------\n",
      "Epoch343 Batch0 Loss: 110479.4688\n",
      "Epoch343 Batch2 Loss: 166484.6562\n",
      "Epoch343 Batch4 Loss: 83798.5000\n",
      "train Loss: 130483.9475\n",
      "Epoch 344/499\n",
      "----------\n",
      "Epoch344 Batch0 Loss: 109835.6562\n",
      "Epoch344 Batch2 Loss: 180361.6562\n",
      "Epoch344 Batch4 Loss: 93593.0391\n",
      "train Loss: 131207.9213\n",
      "Epoch 345/499\n",
      "----------\n",
      "Epoch345 Batch0 Loss: 109292.3672\n",
      "Epoch345 Batch2 Loss: 197033.1094\n",
      "Epoch345 Batch4 Loss: 106737.0391\n",
      "train Loss: 130905.5125\n",
      "Epoch 346/499\n",
      "----------\n",
      "Epoch346 Batch0 Loss: 118011.5234\n",
      "Epoch346 Batch2 Loss: 194322.4531\n",
      "Epoch346 Batch4 Loss: 126187.9141\n",
      "train Loss: 125241.1062\n",
      "Epoch 347/499\n",
      "----------\n",
      "Epoch347 Batch0 Loss: 162344.2656\n",
      "Epoch347 Batch2 Loss: 160686.4219\n",
      "Epoch347 Batch4 Loss: 123493.1406\n",
      "train Loss: 128284.7663\n",
      "Epoch 348/499\n",
      "----------\n",
      "Epoch348 Batch0 Loss: 162676.0312\n",
      "Epoch348 Batch2 Loss: 141698.5312\n",
      "Epoch348 Batch4 Loss: 143263.9062\n",
      "train Loss: 131050.4888\n",
      "Epoch 349/499\n",
      "----------\n",
      "Epoch349 Batch0 Loss: 152064.8281\n",
      "Epoch349 Batch2 Loss: 128616.1016\n",
      "Epoch349 Batch4 Loss: 131188.5469\n",
      "train Loss: 130277.5450\n",
      "Epoch 350/499\n",
      "----------\n",
      "Epoch350 Batch0 Loss: 139490.0625\n",
      "Epoch350 Batch2 Loss: 118512.8906\n",
      "Epoch350 Batch4 Loss: 121663.7891\n",
      "train Loss: 129194.2300\n",
      "Epoch 351/499\n",
      "----------\n",
      "Epoch351 Batch0 Loss: 97541.6250\n",
      "Epoch351 Batch2 Loss: 106202.6719\n",
      "Epoch351 Batch4 Loss: 155291.8125\n",
      "train Loss: 126110.3112\n",
      "Epoch 352/499\n",
      "----------\n",
      "Epoch352 Batch0 Loss: 76512.9609\n",
      "Epoch352 Batch2 Loss: 108047.1719\n",
      "Epoch352 Batch4 Loss: 166666.9531\n",
      "train Loss: 124109.6337\n",
      "Epoch 353/499\n",
      "----------\n",
      "Epoch353 Batch0 Loss: 88645.1562\n",
      "Epoch353 Batch2 Loss: 105942.7891\n",
      "Epoch353 Batch4 Loss: 184850.8125\n",
      "train Loss: 123909.8062\n",
      "Epoch 354/499\n",
      "----------\n",
      "Epoch354 Batch0 Loss: 98276.5000\n",
      "Epoch354 Batch2 Loss: 109176.7578\n",
      "Epoch354 Batch4 Loss: 183907.6875\n",
      "train Loss: 120265.3100\n",
      "Epoch 355/499\n",
      "----------\n",
      "Epoch355 Batch0 Loss: 116472.5625\n",
      "Epoch355 Batch2 Loss: 151529.7031\n",
      "Epoch355 Batch4 Loss: 149884.5625\n",
      "train Loss: 122720.0300\n",
      "Epoch 356/499\n",
      "----------\n",
      "Epoch356 Batch0 Loss: 116922.0234\n",
      "Epoch356 Batch2 Loss: 152781.3125\n",
      "Epoch356 Batch4 Loss: 130187.5000\n",
      "train Loss: 117884.4850\n",
      "Epoch 357/499\n",
      "----------\n",
      "Epoch357 Batch0 Loss: 137095.8281\n",
      "Epoch357 Batch2 Loss: 144730.7812\n",
      "Epoch357 Batch4 Loss: 119560.2734\n",
      "train Loss: 123856.2762\n",
      "Epoch 358/499\n",
      "----------\n",
      "Epoch358 Batch0 Loss: 126961.0312\n",
      "Epoch358 Batch2 Loss: 125241.4688\n",
      "Epoch358 Batch4 Loss: 119353.1328\n",
      "train Loss: 122620.2487\n",
      "Epoch 359/499\n",
      "----------\n",
      "Epoch359 Batch0 Loss: 115857.9375\n",
      "Epoch359 Batch2 Loss: 94978.7109\n",
      "Epoch359 Batch4 Loss: 102850.0469\n",
      "train Loss: 117815.2250\n",
      "Epoch 360/499\n",
      "----------\n",
      "Epoch360 Batch0 Loss: 155601.7969\n",
      "Epoch360 Batch2 Loss: 77173.9062\n",
      "Epoch360 Batch4 Loss: 99708.8750\n",
      "train Loss: 118216.1525\n",
      "Epoch 361/499\n",
      "----------\n",
      "Epoch361 Batch0 Loss: 167739.5000\n",
      "Epoch361 Batch2 Loss: 84283.8672\n",
      "Epoch361 Batch4 Loss: 105392.5391\n",
      "train Loss: 121327.6400\n",
      "Epoch 362/499\n",
      "----------\n",
      "Epoch362 Batch0 Loss: 177231.4688\n",
      "Epoch362 Batch2 Loss: 104666.0000\n",
      "Epoch362 Batch4 Loss: 103212.3828\n",
      "train Loss: 119558.7675\n",
      "Epoch 363/499\n",
      "----------\n",
      "Epoch363 Batch0 Loss: 185886.0938\n",
      "Epoch363 Batch2 Loss: 113728.6484\n",
      "Epoch363 Batch4 Loss: 148128.3125\n",
      "train Loss: 121075.9925\n",
      "Epoch 364/499\n",
      "----------\n",
      "Epoch364 Batch0 Loss: 147371.9062\n",
      "Epoch364 Batch2 Loss: 115037.3125\n",
      "Epoch364 Batch4 Loss: 150792.6094\n",
      "train Loss: 117080.1800\n",
      "Epoch 365/499\n",
      "----------\n",
      "Epoch365 Batch0 Loss: 124224.9609\n",
      "Epoch365 Batch2 Loss: 132696.4219\n",
      "Epoch365 Batch4 Loss: 132314.0625\n",
      "train Loss: 116503.2950\n",
      "Epoch 366/499\n",
      "----------\n",
      "Epoch366 Batch0 Loss: 117069.9062\n",
      "Epoch366 Batch2 Loss: 117831.7109\n",
      "Epoch366 Batch4 Loss: 122255.5547\n",
      "train Loss: 116319.3550\n",
      "Epoch 367/499\n",
      "----------\n",
      "Epoch367 Batch0 Loss: 106870.1250\n",
      "Epoch367 Batch2 Loss: 110639.7109\n",
      "Epoch367 Batch4 Loss: 84062.9609\n",
      "train Loss: 115767.9037\n",
      "Epoch 368/499\n",
      "----------\n",
      "Epoch368 Batch0 Loss: 98377.7578\n",
      "Epoch368 Batch2 Loss: 141611.1875\n",
      "Epoch368 Batch4 Loss: 75419.2734\n",
      "train Loss: 113822.9088\n",
      "Epoch 369/499\n",
      "----------\n",
      "Epoch369 Batch0 Loss: 93164.7109\n",
      "Epoch369 Batch2 Loss: 159929.4531\n",
      "Epoch369 Batch4 Loss: 76378.0000\n",
      "train Loss: 114673.4050\n",
      "Epoch 370/499\n",
      "----------\n",
      "Epoch370 Batch0 Loss: 100004.4297\n",
      "Epoch370 Batch2 Loss: 163319.3750\n",
      "Epoch370 Batch4 Loss: 100566.0547\n",
      "train Loss: 115631.1350\n",
      "Epoch 371/499\n",
      "----------\n",
      "Epoch371 Batch0 Loss: 99064.5391\n",
      "Epoch371 Batch2 Loss: 192010.0000\n",
      "Epoch371 Batch4 Loss: 111632.5234\n",
      "train Loss: 115402.2700\n",
      "Epoch 372/499\n",
      "----------\n",
      "Epoch372 Batch0 Loss: 152240.7969\n",
      "Epoch372 Batch2 Loss: 145414.9844\n",
      "Epoch372 Batch4 Loss: 127409.3672\n",
      "train Loss: 121572.7688\n",
      "Epoch 373/499\n",
      "----------\n",
      "Epoch373 Batch0 Loss: 145922.5312\n",
      "Epoch373 Batch2 Loss: 137314.3750\n",
      "Epoch373 Batch4 Loss: 136946.8438\n",
      "train Loss: 124231.2475\n",
      "Epoch 374/499\n",
      "----------\n",
      "Epoch374 Batch0 Loss: 145919.0781\n",
      "Epoch374 Batch2 Loss: 125588.8828\n",
      "Epoch374 Batch4 Loss: 127307.5000\n",
      "train Loss: 125443.4138\n",
      "Epoch 375/499\n",
      "----------\n",
      "Epoch375 Batch0 Loss: 127708.0391\n",
      "Epoch375 Batch2 Loss: 112176.8516\n",
      "Epoch375 Batch4 Loss: 115500.4609\n",
      "train Loss: 121089.1150\n",
      "Epoch 376/499\n",
      "----------\n",
      "Epoch376 Batch0 Loss: 89068.7578\n",
      "Epoch376 Batch2 Loss: 104556.6172\n",
      "Epoch376 Batch4 Loss: 148849.9219\n",
      "train Loss: 120134.1037\n",
      "Epoch 377/499\n",
      "----------\n",
      "Epoch377 Batch0 Loss: 80287.4688\n",
      "Epoch377 Batch2 Loss: 100082.8359\n",
      "Epoch377 Batch4 Loss: 160442.5781\n",
      "train Loss: 119687.7563\n",
      "Epoch 378/499\n",
      "----------\n",
      "Epoch378 Batch0 Loss: 81974.4219\n",
      "Epoch378 Batch2 Loss: 100922.5078\n",
      "Epoch378 Batch4 Loss: 169152.2812\n",
      "train Loss: 116749.7563\n",
      "Epoch 379/499\n",
      "----------\n",
      "Epoch379 Batch0 Loss: 99811.0938\n",
      "Epoch379 Batch2 Loss: 101922.2891\n",
      "Epoch379 Batch4 Loss: 180409.0625\n",
      "train Loss: 116265.5963\n",
      "Epoch 380/499\n",
      "----------\n",
      "Epoch380 Batch0 Loss: 106135.7500\n",
      "Epoch380 Batch2 Loss: 144722.5625\n",
      "Epoch380 Batch4 Loss: 139907.7031\n",
      "train Loss: 114823.2550\n",
      "Epoch 381/499\n",
      "----------\n",
      "Epoch381 Batch0 Loss: 110132.8125\n",
      "Epoch381 Batch2 Loss: 144894.7344\n",
      "Epoch381 Batch4 Loss: 117496.7969\n",
      "train Loss: 110942.1837\n",
      "Epoch 382/499\n",
      "----------\n",
      "Epoch382 Batch0 Loss: 127667.9062\n",
      "Epoch382 Batch2 Loss: 127285.7891\n",
      "Epoch382 Batch4 Loss: 111120.7812\n",
      "train Loss: 111513.3150\n",
      "Epoch 383/499\n",
      "----------\n",
      "Epoch383 Batch0 Loss: 112121.5000\n",
      "Epoch383 Batch2 Loss: 116463.4062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch383 Batch4 Loss: 102675.5703\n",
      "train Loss: 109010.0312\n",
      "Epoch 384/499\n",
      "----------\n",
      "Epoch384 Batch0 Loss: 104081.2344\n",
      "Epoch384 Batch2 Loss: 81066.2891\n",
      "Epoch384 Batch4 Loss: 93458.6953\n",
      "train Loss: 103598.2238\n",
      "Epoch 385/499\n",
      "----------\n",
      "Epoch385 Batch0 Loss: 133411.0156\n",
      "Epoch385 Batch2 Loss: 70254.9062\n",
      "Epoch385 Batch4 Loss: 86813.4688\n",
      "train Loss: 103102.9288\n",
      "Epoch 386/499\n",
      "----------\n",
      "Epoch386 Batch0 Loss: 145850.2500\n",
      "Epoch386 Batch2 Loss: 73991.5625\n",
      "Epoch386 Batch4 Loss: 89372.6484\n",
      "train Loss: 106961.2787\n",
      "Epoch 387/499\n",
      "----------\n",
      "Epoch387 Batch0 Loss: 159793.0781\n",
      "Epoch387 Batch2 Loss: 87669.3750\n",
      "Epoch387 Batch4 Loss: 97684.7578\n",
      "train Loss: 108051.8525\n",
      "Epoch 388/499\n",
      "----------\n",
      "Epoch388 Batch0 Loss: 165865.1406\n",
      "Epoch388 Batch2 Loss: 105047.1484\n",
      "Epoch388 Batch4 Loss: 137429.5000\n",
      "train Loss: 111256.8025\n",
      "Epoch 389/499\n",
      "----------\n",
      "Epoch389 Batch0 Loss: 137996.6719\n",
      "Epoch389 Batch2 Loss: 108556.4219\n",
      "Epoch389 Batch4 Loss: 148749.9531\n",
      "train Loss: 111211.9812\n",
      "Epoch 390/499\n",
      "----------\n",
      "Epoch390 Batch0 Loss: 120289.0625\n",
      "Epoch390 Batch2 Loss: 130278.2969\n",
      "Epoch390 Batch4 Loss: 127771.2188\n",
      "train Loss: 113263.1212\n",
      "Epoch 391/499\n",
      "----------\n",
      "Epoch391 Batch0 Loss: 115127.3438\n",
      "Epoch391 Batch2 Loss: 113588.9375\n",
      "Epoch391 Batch4 Loss: 127839.0938\n",
      "train Loss: 115271.5813\n",
      "Epoch 392/499\n",
      "----------\n",
      "Epoch392 Batch0 Loss: 106633.2109\n",
      "Epoch392 Batch2 Loss: 110851.0703\n",
      "Epoch392 Batch4 Loss: 88102.3906\n",
      "train Loss: 118303.8325\n",
      "Epoch 393/499\n",
      "----------\n",
      "Epoch393 Batch0 Loss: 105691.3828\n",
      "Epoch393 Batch2 Loss: 139331.5469\n",
      "Epoch393 Batch4 Loss: 80474.5469\n",
      "train Loss: 117099.4000\n",
      "Epoch 394/499\n",
      "----------\n",
      "Epoch394 Batch0 Loss: 91243.8203\n",
      "Epoch394 Batch2 Loss: 163976.0469\n",
      "Epoch394 Batch4 Loss: 79693.2969\n",
      "train Loss: 115037.3150\n",
      "Epoch 395/499\n",
      "----------\n",
      "Epoch395 Batch0 Loss: 94796.7188\n",
      "Epoch395 Batch2 Loss: 166498.8906\n",
      "Epoch395 Batch4 Loss: 92678.1094\n",
      "train Loss: 112947.8075\n",
      "Epoch 396/499\n",
      "----------\n",
      "Epoch396 Batch0 Loss: 102484.1719\n",
      "Epoch396 Batch2 Loss: 167159.6719\n",
      "Epoch396 Batch4 Loss: 105210.7031\n",
      "train Loss: 106652.3112\n",
      "Epoch 397/499\n",
      "----------\n",
      "Epoch397 Batch0 Loss: 134168.7812\n",
      "Epoch397 Batch2 Loss: 135886.1562\n",
      "Epoch397 Batch4 Loss: 108035.9922\n",
      "train Loss: 108867.9975\n",
      "Epoch 398/499\n",
      "----------\n",
      "Epoch398 Batch0 Loss: 134299.7500\n",
      "Epoch398 Batch2 Loss: 113264.1250\n",
      "Epoch398 Batch4 Loss: 124713.9297\n",
      "train Loss: 108149.9200\n",
      "Epoch 399/499\n",
      "----------\n",
      "Epoch399 Batch0 Loss: 117393.2109\n",
      "Epoch399 Batch2 Loss: 106702.7109\n",
      "Epoch399 Batch4 Loss: 106739.0625\n",
      "train Loss: 105598.9837\n",
      "Epoch 400/499\n",
      "----------\n",
      "Epoch400 Batch0 Loss: 111427.9219\n",
      "Epoch400 Batch2 Loss: 95372.4297\n",
      "Epoch400 Batch4 Loss: 101154.2656\n",
      "train Loss: 104709.5375\n",
      "Epoch 401/499\n",
      "----------\n",
      "Epoch401 Batch0 Loss: 75000.0781\n",
      "Epoch401 Batch2 Loss: 91312.4688\n",
      "Epoch401 Batch4 Loss: 125187.8359\n",
      "train Loss: 103304.3050\n",
      "Epoch 402/499\n",
      "----------\n",
      "Epoch402 Batch0 Loss: 68637.8828\n",
      "Epoch402 Batch2 Loss: 83432.9219\n",
      "Epoch402 Batch4 Loss: 142189.7656\n",
      "train Loss: 104756.9525\n",
      "Epoch 403/499\n",
      "----------\n",
      "Epoch403 Batch0 Loss: 69785.3203\n",
      "Epoch403 Batch2 Loss: 86262.7812\n",
      "Epoch403 Batch4 Loss: 152173.3594\n",
      "train Loss: 101671.5925\n",
      "Epoch 404/499\n",
      "----------\n",
      "Epoch404 Batch0 Loss: 86232.3750\n",
      "Epoch404 Batch2 Loss: 95424.8203\n",
      "Epoch404 Batch4 Loss: 156773.5000\n",
      "train Loss: 104195.0675\n",
      "Epoch 405/499\n",
      "----------\n",
      "Epoch405 Batch0 Loss: 101052.8359\n",
      "Epoch405 Batch2 Loss: 127812.7109\n",
      "Epoch405 Batch4 Loss: 132079.0000\n",
      "train Loss: 106140.8562\n",
      "Epoch 406/499\n",
      "----------\n",
      "Epoch406 Batch0 Loss: 102529.3281\n",
      "Epoch406 Batch2 Loss: 139057.7812\n",
      "Epoch406 Batch4 Loss: 112456.8906\n",
      "train Loss: 103875.1837\n",
      "Epoch 407/499\n",
      "----------\n",
      "Epoch407 Batch0 Loss: 124543.9141\n",
      "Epoch407 Batch2 Loss: 118900.8750\n",
      "Epoch407 Batch4 Loss: 110022.0234\n",
      "train Loss: 107619.0100\n",
      "Epoch 408/499\n",
      "----------\n",
      "Epoch408 Batch0 Loss: 107672.5469\n",
      "Epoch408 Batch2 Loss: 115693.2734\n",
      "Epoch408 Batch4 Loss: 98067.9219\n",
      "train Loss: 105818.9200\n",
      "Epoch 409/499\n",
      "----------\n",
      "Epoch409 Batch0 Loss: 102193.0625\n",
      "Epoch409 Batch2 Loss: 76597.3438\n",
      "Epoch409 Batch4 Loss: 93020.3594\n",
      "train Loss: 101859.6325\n",
      "Epoch 410/499\n",
      "----------\n",
      "Epoch410 Batch0 Loss: 127993.5312\n",
      "Epoch410 Batch2 Loss: 67729.8750\n",
      "Epoch410 Batch4 Loss: 85618.7812\n",
      "train Loss: 99441.3138\n",
      "Epoch 411/499\n",
      "----------\n",
      "Epoch411 Batch0 Loss: 141575.6719\n",
      "Epoch411 Batch2 Loss: 73651.7891\n",
      "Epoch411 Batch4 Loss: 86045.2812\n",
      "train Loss: 103440.1925\n",
      "Epoch 412/499\n",
      "----------\n",
      "Epoch412 Batch0 Loss: 157557.1719\n",
      "Epoch412 Batch2 Loss: 83374.4219\n",
      "Epoch412 Batch4 Loss: 94313.1406\n",
      "train Loss: 103433.2413\n",
      "Epoch 413/499\n",
      "----------\n",
      "Epoch413 Batch0 Loss: 150506.8125\n",
      "Epoch413 Batch2 Loss: 102362.0625\n",
      "Epoch413 Batch4 Loss: 121727.3516\n",
      "train Loss: 102077.6425\n",
      "Epoch 414/499\n",
      "----------\n",
      "Epoch414 Batch0 Loss: 127946.9688\n",
      "Epoch414 Batch2 Loss: 96708.8203\n",
      "Epoch414 Batch4 Loss: 127326.9219\n",
      "train Loss: 100761.8913\n",
      "Epoch 415/499\n",
      "----------\n",
      "Epoch415 Batch0 Loss: 108697.7969\n",
      "Epoch415 Batch2 Loss: 114672.9297\n",
      "Epoch415 Batch4 Loss: 115880.9297\n",
      "train Loss: 99807.3650\n",
      "Epoch 416/499\n",
      "----------\n",
      "Epoch416 Batch0 Loss: 100615.7578\n",
      "Epoch416 Batch2 Loss: 103841.8984\n",
      "Epoch416 Batch4 Loss: 104487.7969\n",
      "train Loss: 100583.9688\n",
      "Epoch 417/499\n",
      "----------\n",
      "Epoch417 Batch0 Loss: 97087.0156\n",
      "Epoch417 Batch2 Loss: 94502.8281\n",
      "Epoch417 Batch4 Loss: 78752.7188\n",
      "train Loss: 105238.0025\n",
      "Epoch 418/499\n",
      "----------\n",
      "Epoch418 Batch0 Loss: 86959.1641\n",
      "Epoch418 Batch2 Loss: 126285.1484\n",
      "Epoch418 Batch4 Loss: 65347.6367\n",
      "train Loss: 99658.6056\n",
      "Epoch 419/499\n",
      "----------\n",
      "Epoch419 Batch0 Loss: 79512.4688\n",
      "Epoch419 Batch2 Loss: 136769.6875\n",
      "Epoch419 Batch4 Loss: 67369.9531\n",
      "train Loss: 98817.0538\n",
      "Epoch 420/499\n",
      "----------\n",
      "Epoch420 Batch0 Loss: 83228.0938\n",
      "Epoch420 Batch2 Loss: 144232.9375\n",
      "Epoch420 Batch4 Loss: 79913.5469\n",
      "train Loss: 98040.1588\n",
      "Epoch 421/499\n",
      "----------\n",
      "Epoch421 Batch0 Loss: 84999.3984\n",
      "Epoch421 Batch2 Loss: 149745.9844\n",
      "Epoch421 Batch4 Loss: 93828.2734\n",
      "train Loss: 94279.2525\n",
      "Epoch 422/499\n",
      "----------\n",
      "Epoch422 Batch0 Loss: 119450.1875\n",
      "Epoch422 Batch2 Loss: 121337.0781\n",
      "Epoch422 Batch4 Loss: 94693.2969\n",
      "train Loss: 97211.4050\n",
      "Epoch 423/499\n",
      "----------\n",
      "Epoch423 Batch0 Loss: 123941.1406\n",
      "Epoch423 Batch2 Loss: 106304.8828\n",
      "Epoch423 Batch4 Loss: 115278.0859\n",
      "train Loss: 101461.2663\n",
      "Epoch 424/499\n",
      "----------\n",
      "Epoch424 Batch0 Loss: 112928.5781\n",
      "Epoch424 Batch2 Loss: 104598.2031\n",
      "Epoch424 Batch4 Loss: 97983.1094\n",
      "train Loss: 100769.7775\n",
      "Epoch 425/499\n",
      "----------\n",
      "Epoch425 Batch0 Loss: 114494.0234\n",
      "Epoch425 Batch2 Loss: 89129.9453\n",
      "Epoch425 Batch4 Loss: 98406.0234\n",
      "train Loss: 101887.2250\n",
      "Epoch 426/499\n",
      "----------\n",
      "Epoch426 Batch0 Loss: 73380.0469\n",
      "Epoch426 Batch2 Loss: 86882.0781\n",
      "Epoch426 Batch4 Loss: 123396.8125\n",
      "train Loss: 99847.1288\n",
      "Epoch 427/499\n",
      "----------\n",
      "Epoch427 Batch0 Loss: 63256.8633\n",
      "Epoch427 Batch2 Loss: 80750.4141\n",
      "Epoch427 Batch4 Loss: 133520.1250\n",
      "train Loss: 99633.8294\n",
      "Epoch 428/499\n",
      "----------\n",
      "Epoch428 Batch0 Loss: 68404.5859\n",
      "Epoch428 Batch2 Loss: 81320.9922\n",
      "Epoch428 Batch4 Loss: 147099.0781\n",
      "train Loss: 97333.1588\n",
      "Epoch 429/499\n",
      "----------\n",
      "Epoch429 Batch0 Loss: 80512.3672\n",
      "Epoch429 Batch2 Loss: 86105.4766\n",
      "Epoch429 Batch4 Loss: 145999.5469\n",
      "train Loss: 96314.9837\n",
      "Epoch 430/499\n",
      "----------\n",
      "Epoch430 Batch0 Loss: 92580.9297\n",
      "Epoch430 Batch2 Loss: 117134.0625\n",
      "Epoch430 Batch4 Loss: 116409.5859\n",
      "train Loss: 95903.8650\n",
      "Epoch 431/499\n",
      "----------\n",
      "Epoch431 Batch0 Loss: 92495.0859\n",
      "Epoch431 Batch2 Loss: 121641.8359\n",
      "Epoch431 Batch4 Loss: 102279.4375\n",
      "train Loss: 93780.4894\n",
      "Epoch 432/499\n",
      "----------\n",
      "Epoch432 Batch0 Loss: 108768.5000\n",
      "Epoch432 Batch2 Loss: 110635.5938\n",
      "Epoch432 Batch4 Loss: 94206.6484\n",
      "train Loss: 95646.9394\n",
      "Epoch 433/499\n",
      "----------\n",
      "Epoch433 Batch0 Loss: 96825.1016\n",
      "Epoch433 Batch2 Loss: 101507.4531\n",
      "Epoch433 Batch4 Loss: 88235.8984\n",
      "train Loss: 96356.3788\n",
      "Epoch 434/499\n",
      "----------\n",
      "Epoch434 Batch0 Loss: 89604.0234\n",
      "Epoch434 Batch2 Loss: 77168.3438\n",
      "Epoch434 Batch4 Loss: 87305.9141\n",
      "train Loss: 93247.3300\n",
      "Epoch 435/499\n",
      "----------\n",
      "Epoch435 Batch0 Loss: 123578.4062\n",
      "Epoch435 Batch2 Loss: 65436.4062\n",
      "Epoch435 Batch4 Loss: 78923.5469\n",
      "train Loss: 96307.2400\n",
      "Epoch 436/499\n",
      "----------\n",
      "Epoch436 Batch0 Loss: 144616.6562\n",
      "Epoch436 Batch2 Loss: 68100.8984\n",
      "Epoch436 Batch4 Loss: 91502.1016\n",
      "train Loss: 102571.1163\n",
      "Epoch 437/499\n",
      "----------\n",
      "Epoch437 Batch0 Loss: 141560.8125\n",
      "Epoch437 Batch2 Loss: 88742.2891\n",
      "Epoch437 Batch4 Loss: 86720.6094\n",
      "train Loss: 99560.3387\n",
      "Epoch 438/499\n",
      "----------\n",
      "Epoch438 Batch0 Loss: 156497.4531\n",
      "Epoch438 Batch2 Loss: 99216.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch438 Batch4 Loss: 121869.6406\n",
      "train Loss: 102664.1425\n",
      "Epoch 439/499\n",
      "----------\n",
      "Epoch439 Batch0 Loss: 127247.9922\n",
      "Epoch439 Batch2 Loss: 91517.7500\n",
      "Epoch439 Batch4 Loss: 128825.2734\n",
      "train Loss: 99395.0806\n",
      "Epoch 440/499\n",
      "----------\n",
      "Epoch440 Batch0 Loss: 105364.9375\n",
      "Epoch440 Batch2 Loss: 115390.5156\n",
      "Epoch440 Batch4 Loss: 110063.1406\n",
      "train Loss: 97419.2050\n",
      "Epoch 441/499\n",
      "----------\n",
      "Epoch441 Batch0 Loss: 97971.4844\n",
      "Epoch441 Batch2 Loss: 99122.1719\n",
      "Epoch441 Batch4 Loss: 102128.7656\n",
      "train Loss: 97881.2075\n",
      "Epoch 442/499\n",
      "----------\n",
      "Epoch442 Batch0 Loss: 91969.4141\n",
      "Epoch442 Batch2 Loss: 90835.8906\n",
      "Epoch442 Batch4 Loss: 77022.7031\n",
      "train Loss: 98163.9375\n",
      "Epoch 443/499\n",
      "----------\n",
      "Epoch443 Batch0 Loss: 82144.7266\n",
      "Epoch443 Batch2 Loss: 121133.5859\n",
      "Epoch443 Batch4 Loss: 60294.9219\n",
      "train Loss: 95410.4125\n",
      "Epoch 444/499\n",
      "----------\n",
      "Epoch444 Batch0 Loss: 81729.6016\n",
      "Epoch444 Batch2 Loss: 126148.7422\n",
      "Epoch444 Batch4 Loss: 68247.2188\n",
      "train Loss: 96184.9250\n",
      "Epoch 445/499\n",
      "----------\n",
      "Epoch445 Batch0 Loss: 78545.1641\n",
      "Epoch445 Batch2 Loss: 142088.9375\n",
      "Epoch445 Batch4 Loss: 77883.5781\n",
      "train Loss: 94880.7337\n",
      "Epoch 446/499\n",
      "----------\n",
      "Epoch446 Batch0 Loss: 84750.7266\n",
      "Epoch446 Batch2 Loss: 141994.5156\n",
      "Epoch446 Batch4 Loss: 92230.7656\n",
      "train Loss: 91293.3875\n",
      "Epoch 447/499\n",
      "----------\n",
      "Epoch447 Batch0 Loss: 115183.4609\n",
      "Epoch447 Batch2 Loss: 121230.8516\n",
      "Epoch447 Batch4 Loss: 87432.6797\n",
      "train Loss: 93602.5887\n",
      "Epoch 448/499\n",
      "----------\n",
      "Epoch448 Batch0 Loss: 122802.7109\n",
      "Epoch448 Batch2 Loss: 98768.3828\n",
      "Epoch448 Batch4 Loss: 108238.9922\n",
      "train Loss: 96388.2988\n",
      "Epoch 449/499\n",
      "----------\n",
      "Epoch449 Batch0 Loss: 111040.9844\n",
      "Epoch449 Batch2 Loss: 92715.4375\n",
      "Epoch449 Batch4 Loss: 96485.9141\n",
      "train Loss: 95334.3100\n",
      "Epoch 450/499\n",
      "----------\n",
      "Epoch450 Batch0 Loss: 97700.4844\n",
      "Epoch450 Batch2 Loss: 89796.3906\n",
      "Epoch450 Batch4 Loss: 86243.5156\n",
      "train Loss: 93518.7337\n",
      "Epoch 451/499\n",
      "----------\n",
      "Epoch451 Batch0 Loss: 72996.0625\n",
      "Epoch451 Batch2 Loss: 83318.9375\n",
      "Epoch451 Batch4 Loss: 115031.4844\n",
      "train Loss: 94604.0175\n",
      "Epoch 452/499\n",
      "----------\n",
      "Epoch452 Batch0 Loss: 60962.3398\n",
      "Epoch452 Batch2 Loss: 79161.6328\n",
      "Epoch452 Batch4 Loss: 126782.1094\n",
      "train Loss: 94754.3819\n",
      "Epoch 453/499\n",
      "----------\n",
      "Epoch453 Batch0 Loss: 66823.9688\n",
      "Epoch453 Batch2 Loss: 75569.8516\n",
      "Epoch453 Batch4 Loss: 136433.1406\n",
      "train Loss: 91381.6212\n",
      "Epoch 454/499\n",
      "----------\n",
      "Epoch454 Batch0 Loss: 73324.1094\n",
      "Epoch454 Batch2 Loss: 81001.7422\n",
      "Epoch454 Batch4 Loss: 139815.9219\n",
      "train Loss: 90335.4288\n",
      "Epoch 455/499\n",
      "----------\n",
      "Epoch455 Batch0 Loss: 84680.8047\n",
      "Epoch455 Batch2 Loss: 116954.4375\n",
      "Epoch455 Batch4 Loss: 108255.2969\n",
      "train Loss: 91582.4162\n",
      "Epoch 456/499\n",
      "----------\n",
      "Epoch456 Batch0 Loss: 87760.2109\n",
      "Epoch456 Batch2 Loss: 112588.6562\n",
      "Epoch456 Batch4 Loss: 95059.4844\n",
      "train Loss: 87773.5706\n",
      "Epoch 457/499\n",
      "----------\n",
      "Epoch457 Batch0 Loss: 102484.2812\n",
      "Epoch457 Batch2 Loss: 104360.5391\n",
      "Epoch457 Batch4 Loss: 90237.2422\n",
      "train Loss: 90306.8581\n",
      "Epoch 458/499\n",
      "----------\n",
      "Epoch458 Batch0 Loss: 90474.3203\n",
      "Epoch458 Batch2 Loss: 97686.3516\n",
      "Epoch458 Batch4 Loss: 83455.2891\n",
      "train Loss: 89922.8850\n",
      "Epoch 459/499\n",
      "----------\n",
      "Epoch459 Batch0 Loss: 87175.2109\n",
      "Epoch459 Batch2 Loss: 69432.6484\n",
      "Epoch459 Batch4 Loss: 83071.9297\n",
      "train Loss: 87670.0538\n",
      "Epoch 460/499\n",
      "----------\n",
      "Epoch460 Batch0 Loss: 111227.6953\n",
      "Epoch460 Batch2 Loss: 62456.7031\n",
      "Epoch460 Batch4 Loss: 77129.1094\n",
      "train Loss: 88799.8887\n",
      "Epoch 461/499\n",
      "----------\n",
      "Epoch461 Batch0 Loss: 127546.5078\n",
      "Epoch461 Batch2 Loss: 65550.7266\n",
      "Epoch461 Batch4 Loss: 77265.9062\n",
      "train Loss: 92381.5000\n",
      "Epoch 462/499\n",
      "----------\n",
      "Epoch462 Batch0 Loss: 140118.1406\n",
      "Epoch462 Batch2 Loss: 75484.0000\n",
      "Epoch462 Batch4 Loss: 83077.4219\n",
      "train Loss: 92383.9637\n",
      "Epoch 463/499\n",
      "----------\n",
      "Epoch463 Batch0 Loss: 137069.1719\n",
      "Epoch463 Batch2 Loss: 84670.2344\n",
      "Epoch463 Batch4 Loss: 113689.3750\n",
      "train Loss: 91286.3112\n",
      "Epoch 464/499\n",
      "----------\n",
      "Epoch464 Batch0 Loss: 106133.9766\n",
      "Epoch464 Batch2 Loss: 87429.7031\n",
      "Epoch464 Batch4 Loss: 116164.9062\n",
      "train Loss: 88366.5606\n",
      "Epoch 465/499\n",
      "----------\n",
      "Epoch465 Batch0 Loss: 92940.9062\n",
      "Epoch465 Batch2 Loss: 101596.6953\n",
      "Epoch465 Batch4 Loss: 98671.0156\n",
      "train Loss: 87576.9394\n",
      "Epoch 466/499\n",
      "----------\n",
      "Epoch466 Batch0 Loss: 88955.1797\n",
      "Epoch466 Batch2 Loss: 86538.4219\n",
      "Epoch466 Batch4 Loss: 94770.5781\n",
      "train Loss: 87866.9913\n",
      "Epoch 467/499\n",
      "----------\n",
      "Epoch467 Batch0 Loss: 82323.1797\n",
      "Epoch467 Batch2 Loss: 81710.5781\n",
      "Epoch467 Batch4 Loss: 65899.3594\n",
      "train Loss: 87585.0813\n",
      "Epoch 468/499\n",
      "----------\n",
      "Epoch468 Batch0 Loss: 75994.1016\n",
      "Epoch468 Batch2 Loss: 106126.2891\n",
      "Epoch468 Batch4 Loss: 56321.3867\n",
      "train Loss: 85940.0244\n",
      "Epoch 469/499\n",
      "----------\n",
      "Epoch469 Batch0 Loss: 70154.1016\n",
      "Epoch469 Batch2 Loss: 116587.7266\n",
      "Epoch469 Batch4 Loss: 57274.3398\n",
      "train Loss: 84839.7156\n",
      "Epoch 470/499\n",
      "----------\n",
      "Epoch470 Batch0 Loss: 73376.2656\n",
      "Epoch470 Batch2 Loss: 124636.4062\n",
      "Epoch470 Batch4 Loss: 71746.4297\n",
      "train Loss: 86062.4487\n",
      "Epoch 471/499\n",
      "----------\n",
      "Epoch471 Batch0 Loss: 74146.4766\n",
      "Epoch471 Batch2 Loss: 138386.0312\n",
      "Epoch471 Batch4 Loss: 79079.0938\n",
      "train Loss: 83144.1656\n",
      "Epoch 472/499\n",
      "----------\n",
      "Epoch472 Batch0 Loss: 110824.6094\n",
      "Epoch472 Batch2 Loss: 102183.4453\n",
      "Epoch472 Batch4 Loss: 90047.5938\n",
      "train Loss: 87713.6387\n",
      "Epoch 473/499\n",
      "----------\n",
      "Epoch473 Batch0 Loss: 111188.0000\n",
      "Epoch473 Batch2 Loss: 99332.8125\n",
      "Epoch473 Batch4 Loss: 100875.8750\n",
      "train Loss: 91645.3769\n",
      "Epoch 474/499\n",
      "----------\n",
      "Epoch474 Batch0 Loss: 112207.2969\n",
      "Epoch474 Batch2 Loss: 94435.5625\n",
      "Epoch474 Batch4 Loss: 95880.3594\n",
      "train Loss: 96800.1800\n",
      "Epoch 475/499\n",
      "----------\n",
      "Epoch475 Batch0 Loss: 104149.7188\n",
      "Epoch475 Batch2 Loss: 88880.1484\n",
      "Epoch475 Batch4 Loss: 97342.9922\n",
      "train Loss: 98308.5638\n",
      "Epoch 476/499\n",
      "----------\n",
      "Epoch476 Batch0 Loss: 74845.3906\n",
      "Epoch476 Batch2 Loss: 86587.7891\n",
      "Epoch476 Batch4 Loss: 117748.7969\n",
      "train Loss: 97103.3650\n",
      "Epoch 477/499\n",
      "----------\n",
      "Epoch477 Batch0 Loss: 65922.8594\n",
      "Epoch477 Batch2 Loss: 81841.9531\n",
      "Epoch477 Batch4 Loss: 130944.7734\n",
      "train Loss: 97695.3112\n",
      "Epoch 478/499\n",
      "----------\n",
      "Epoch478 Batch0 Loss: 68736.2188\n",
      "Epoch478 Batch2 Loss: 81225.2266\n",
      "Epoch478 Batch4 Loss: 136631.8125\n",
      "train Loss: 93853.0675\n",
      "Epoch 479/499\n",
      "----------\n",
      "Epoch479 Batch0 Loss: 78922.4297\n",
      "Epoch479 Batch2 Loss: 80344.6719\n",
      "Epoch479 Batch4 Loss: 138981.9688\n",
      "train Loss: 90983.6588\n",
      "Epoch 480/499\n",
      "----------\n",
      "Epoch480 Batch0 Loss: 82137.4141\n",
      "Epoch480 Batch2 Loss: 109668.0781\n",
      "Epoch480 Batch4 Loss: 107869.3125\n",
      "train Loss: 88579.5587\n",
      "Epoch 481/499\n",
      "----------\n",
      "Epoch481 Batch0 Loss: 84943.8750\n",
      "Epoch481 Batch2 Loss: 109369.6953\n",
      "Epoch481 Batch4 Loss: 91351.7344\n",
      "train Loss: 84848.8019\n",
      "Epoch 482/499\n",
      "----------\n",
      "Epoch482 Batch0 Loss: 98640.5781\n",
      "Epoch482 Batch2 Loss: 99182.5625\n",
      "Epoch482 Batch4 Loss: 85086.5938\n",
      "train Loss: 86343.7350\n",
      "Epoch 483/499\n",
      "----------\n",
      "Epoch483 Batch0 Loss: 85621.8438\n",
      "Epoch483 Batch2 Loss: 88357.9766\n",
      "Epoch483 Batch4 Loss: 78308.9453\n",
      "train Loss: 84015.9438\n",
      "Epoch 484/499\n",
      "----------\n",
      "Epoch484 Batch0 Loss: 79089.4297\n",
      "Epoch484 Batch2 Loss: 66449.2422\n",
      "Epoch484 Batch4 Loss: 73313.0859\n",
      "train Loss: 81485.6962\n",
      "Epoch 485/499\n",
      "----------\n",
      "Epoch485 Batch0 Loss: 106630.7812\n",
      "Epoch485 Batch2 Loss: 53167.1211\n",
      "Epoch485 Batch4 Loss: 71534.6797\n",
      "train Loss: 81461.8494\n",
      "Epoch 486/499\n",
      "----------\n",
      "Epoch486 Batch0 Loss: 112755.9531\n",
      "Epoch486 Batch2 Loss: 61411.6367\n",
      "Epoch486 Batch4 Loss: 73382.0000\n",
      "train Loss: 85257.2606\n",
      "Epoch 487/499\n",
      "----------\n",
      "Epoch487 Batch0 Loss: 125612.0547\n",
      "Epoch487 Batch2 Loss: 71065.5000\n",
      "Epoch487 Batch4 Loss: 74050.0156\n",
      "train Loss: 84977.7675\n",
      "Epoch 488/499\n",
      "----------\n",
      "Epoch488 Batch0 Loss: 135540.8438\n",
      "Epoch488 Batch2 Loss: 79420.4609\n",
      "Epoch488 Batch4 Loss: 110782.8281\n",
      "train Loss: 88136.1362\n",
      "Epoch 489/499\n",
      "----------\n",
      "Epoch489 Batch0 Loss: 104686.2188\n",
      "Epoch489 Batch2 Loss: 82779.1406\n",
      "Epoch489 Batch4 Loss: 110771.3750\n",
      "train Loss: 84779.1700\n",
      "Epoch 490/499\n",
      "----------\n",
      "Epoch490 Batch0 Loss: 91558.4766\n",
      "Epoch490 Batch2 Loss: 97057.1328\n",
      "Epoch490 Batch4 Loss: 100119.0391\n",
      "train Loss: 85675.6856\n",
      "Epoch 491/499\n",
      "----------\n",
      "Epoch491 Batch0 Loss: 84086.9531\n",
      "Epoch491 Batch2 Loss: 84561.0547\n",
      "Epoch491 Batch4 Loss: 89594.1562\n",
      "train Loss: 84314.1588\n",
      "Epoch 492/499\n",
      "----------\n",
      "Epoch492 Batch0 Loss: 77507.8516\n",
      "Epoch492 Batch2 Loss: 77812.8359\n",
      "Epoch492 Batch4 Loss: 62825.6367\n",
      "train Loss: 84290.2231\n",
      "Epoch 493/499\n",
      "----------\n",
      "Epoch493 Batch0 Loss: 73440.4609\n",
      "Epoch493 Batch2 Loss: 100015.9375\n",
      "Epoch493 Batch4 Loss: 54021.4883\n",
      "train Loss: 81733.2406\n",
      "Epoch 494/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch494 Batch0 Loss: 66902.4609\n",
      "Epoch494 Batch2 Loss: 111201.6953\n",
      "Epoch494 Batch4 Loss: 56353.3359\n",
      "train Loss: 80857.2525\n",
      "Epoch 495/499\n",
      "----------\n",
      "Epoch495 Batch0 Loss: 68533.8359\n",
      "Epoch495 Batch2 Loss: 120699.2266\n",
      "Epoch495 Batch4 Loss: 67942.5312\n",
      "train Loss: 81700.6913\n",
      "Epoch 496/499\n",
      "----------\n",
      "Epoch496 Batch0 Loss: 73158.7109\n",
      "Epoch496 Batch2 Loss: 123939.8203\n",
      "Epoch496 Batch4 Loss: 77686.6172\n",
      "train Loss: 79059.1562\n",
      "Epoch 497/499\n",
      "----------\n",
      "Epoch497 Batch0 Loss: 103675.2031\n",
      "Epoch497 Batch2 Loss: 100358.8125\n",
      "Epoch497 Batch4 Loss: 79589.6016\n",
      "train Loss: 82805.1650\n",
      "Epoch 498/499\n",
      "----------\n",
      "Epoch498 Batch0 Loss: 104467.7031\n",
      "Epoch498 Batch2 Loss: 88875.0859\n",
      "Epoch498 Batch4 Loss: 93294.5938\n",
      "train Loss: 83748.0650\n",
      "Epoch 499/499\n",
      "----------\n",
      "Epoch499 Batch0 Loss: 94099.9219\n",
      "Epoch499 Batch2 Loss: 85716.4531\n",
      "Epoch499 Batch4 Loss: 82257.5547\n",
      "train Loss: 83432.0663\n"
     ]
    }
   ],
   "source": [
    "###################### Train Model #############################\n",
    "# Calculate total iter_num\n",
    "total_iter_num = 0\n",
    "\n",
    "for epoch in range(opt.num_epochs):#411, 411+opt.num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, opt.num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "    # Iterate over data.\n",
    "    for i in range(int(dataloader.size()/opt.batchSize)):\n",
    "        total_iter_num += 1\n",
    "        \n",
    "        # Get data\n",
    "        inputs, labels =  dataloader.get_batch()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # We need to flatten annotations and logits to apply index of valid annotations.\n",
    "        anno_flatten = flatten_annotations(labels)\n",
    "        index = get_valid_annotations_index(anno_flatten, mask_out_value=255)\n",
    "        anno_flatten_valid = torch.index_select(anno_flatten, 0, index)\n",
    "        \n",
    "        ## Create Graph ##\n",
    "        if graph_created == False:\n",
    "            graph_created = True\n",
    "            writer.add_graph(model, inputs, verbose=False)\n",
    "        \n",
    "        # Forward Prop\n",
    "        optimizer.zero_grad()\n",
    "        torch.set_grad_enabled(True)\n",
    "        logits = model(inputs)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        logits_flatten = flatten_logits(logits, number_of_classes=opt.num_classes)\n",
    "        logits_flatten_valid = torch.index_select(logits_flatten, 0, index)\n",
    "        loss = criterion(logits_flatten_valid, anno_flatten_valid)\n",
    "        \n",
    "        # Backward Prop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar('loss', loss.item(), total_iter_num)\n",
    "\n",
    "        \n",
    "        # Print image every N epochs\n",
    "        nTestInterval = 1\n",
    "        if (epoch % nTestInterval) == 0:\n",
    "            img_tensor = inputs[:3].clone().cpu()\n",
    "            \n",
    "            output_tensor = torch.unsqueeze(torch.max(logits[:3], 1)[1].detach().cpu().float(), 1)\n",
    "            output_tensor = torch.cat((output_tensor, output_tensor, output_tensor), 1)\n",
    "            \n",
    "            label_tensor = labels[:3].detach().cpu().float()\n",
    "            label_tensor = torch.cat((label_tensor, label_tensor, label_tensor), 1)\n",
    "            \n",
    "            images = []\n",
    "            for img, output, label in zip(img_tensor, output_tensor, label_tensor):\n",
    "                images.append(img)\n",
    "                images.append(output)\n",
    "                images.append(label)\n",
    "\n",
    "            grid_image = make_grid(images, 3, normalize=True, scale_each=False )\n",
    "            writer.add_image('Train', grid_image, epoch)\n",
    "        \n",
    "        if (i % 2 == 0):\n",
    "            print('Epoch{} Batch{} Loss: {:.4f}'.format(epoch, i, loss.item()))\n",
    "\n",
    "    epoch_loss = running_loss / (dataloader.size()/opt.batchSize)\n",
    "    writer.add_scalar('epoch_loss', epoch_loss, epoch)\n",
    "    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    directory = opt.logs_path+'/checkpoints/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    if (epoch % 5 == 0):\n",
    "        filename = opt.logs_path + '/checkpoints/checkpoint-epoch_{}.pth'.format(epoch,i)\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
