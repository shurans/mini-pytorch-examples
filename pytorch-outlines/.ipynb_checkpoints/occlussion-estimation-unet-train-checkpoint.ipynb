{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from data_loader import Dataset\n",
    "import models.unet_normals as unet\n",
    "from tensorboardX import SummaryWriter\n",
    "# import OpenEXR, Imath\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Options\n",
    "Set the various parameters:\n",
    "- dataroot: The folder where the training data is stored\n",
    "- file_list: List of filenames of images for training\n",
    "- batchSize: Batch size for model\n",
    "- shuffle: If true, will shuffle the dataset\n",
    "- phase: If 'train', then it's in training mode.\n",
    "- num_epochs: Number of epochs to train the model for\n",
    "- imsize: Dimensions of the image (square)\n",
    "- num_classes: Num of classes in the output\n",
    "- gpu: Which GPU device to use\n",
    "- logs_path: The path where the log files (tensorboard) will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPT():\n",
    "    def __init__(self):\n",
    "        self.dataroot = './data/'\n",
    "        self.file_list = './data/datalist'\n",
    "        self.batchSize = 16\n",
    "        self.shuffle = True\n",
    "        self.phase = 'train'\n",
    "        self.num_epochs = 500\n",
    "        self.imsize = 224\n",
    "        self.num_classes = int(2)\n",
    "        self.gpu = '0'\n",
    "        self.logs_path = 'logs/exp2'\n",
    "        self.use_pretrained = True\n",
    "\n",
    "opt = OPT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling the dataset\n"
     ]
    }
   ],
   "source": [
    "###################### Options #############################\n",
    "phase = opt.phase\n",
    "device = torch.device(\"cuda:\"+ opt.gpu if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###################### TensorBoardX #############################\n",
    "# if os.path.exists(opt.logs_path):\n",
    "#     raise Exception('The folder \\\"{}\\\" already exists! Define a new log path or delete old contents.'.format(opt.logs_path))\n",
    "    \n",
    "writer = SummaryWriter(opt.logs_path, comment='create-graph')\n",
    "graph_created = False\n",
    "\n",
    "###################### DataLoader #############################\n",
    "dataloader = Dataset(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "We use a UNet model. The last few layers of this model are modified to return a 3 channel image, containing the x,y,z values of surface normal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### ModelBuilder #############################\n",
    "model = unet.Unet(num_classes=opt.num_classes)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum').to(device)\n",
    "\n",
    "# Load weights from checkpoint\n",
    "if (opt.use_pretrained == True):\n",
    "    checkpoint_path = 'logs/exp1/checkpoints/checkpoint-epoch_405.pth'\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "###################### Setup Optimazation #############################\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "###################### Loss fuction #############################\n",
    "def flatten_logits(logits, number_of_classes):\n",
    "    \"\"\"Flattens the logits batch except for the logits dimension\"\"\"\n",
    "    logits_permuted = logits.permute(0, 2, 3, 1)\n",
    "    logits_permuted_cont = logits_permuted.contiguous()\n",
    "    logits_flatten = logits_permuted_cont.view(-1, number_of_classes)\n",
    "    return logits_flatten\n",
    "\n",
    "def flatten_annotations(annotations):\n",
    "    return annotations.view(-1)\n",
    "\n",
    "def get_valid_annotations_index(flatten_annotations, mask_out_value=255):\n",
    "    return torch.squeeze( torch.nonzero((flatten_annotations != mask_out_value )), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 411/499\n",
      "----------\n",
      "Epoch411 Batch0 Loss: 24014.5840\n",
      "Epoch411 Batch2 Loss: 30918.6602\n",
      "Epoch411 Batch4 Loss: 57753.9141\n",
      "train Loss: 50896.9784\n",
      "Epoch 412/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch412 Batch0 Loss: 32984.0469\n",
      "Epoch412 Batch2 Loss: 51051.0391\n",
      "Epoch412 Batch4 Loss: 54358.9141\n",
      "train Loss: 42741.1125\n",
      "Epoch 413/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch413 Batch0 Loss: 45512.9883\n",
      "Epoch413 Batch2 Loss: 41568.8477\n",
      "Epoch413 Batch4 Loss: 29749.9844\n",
      "train Loss: 38696.4941\n",
      "Epoch 414/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch414 Batch0 Loss: 34415.5977\n",
      "Epoch414 Batch2 Loss: 36843.5039\n",
      "Epoch414 Batch4 Loss: 17672.2773\n",
      "train Loss: 34737.9544\n",
      "Epoch 415/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch415 Batch0 Loss: 21073.9160\n",
      "Epoch415 Batch2 Loss: 45369.1875\n",
      "Epoch415 Batch4 Loss: 30016.0117\n",
      "train Loss: 29365.3391\n",
      "Epoch 416/499\n",
      "----------\n",
      "Epoch416 Batch0 Loss: 22702.3105\n",
      "shuffling the dataset\n",
      "Epoch416 Batch2 Loss: 17862.7070\n",
      "Epoch416 Batch4 Loss: 25138.8984\n",
      "train Loss: 26303.1103\n",
      "Epoch 417/499\n",
      "----------\n",
      "Epoch417 Batch0 Loss: 22987.7305\n",
      "shuffling the dataset\n",
      "Epoch417 Batch2 Loss: 18642.3359\n",
      "Epoch417 Batch4 Loss: 30773.9531\n",
      "train Loss: 25779.0278\n",
      "Epoch 418/499\n",
      "----------\n",
      "Epoch418 Batch0 Loss: 24652.6777\n",
      "shuffling the dataset\n",
      "Epoch418 Batch2 Loss: 29693.6152\n",
      "Epoch418 Batch4 Loss: 29044.2773\n",
      "train Loss: 22739.5284\n",
      "Epoch 419/499\n",
      "----------\n",
      "Epoch419 Batch0 Loss: 29332.3145\n",
      "shuffling the dataset\n",
      "Epoch419 Batch2 Loss: 22839.5957\n",
      "Epoch419 Batch4 Loss: 28044.2969\n",
      "train Loss: 23912.8456\n",
      "Epoch 420/499\n",
      "----------\n",
      "Epoch420 Batch0 Loss: 22165.0723\n",
      "shuffling the dataset\n",
      "Epoch420 Batch2 Loss: 15425.4639\n",
      "Epoch420 Batch4 Loss: 19573.4922\n",
      "train Loss: 20008.9767\n",
      "Epoch 421/499\n",
      "----------\n",
      "Epoch421 Batch0 Loss: 27974.2578\n",
      "shuffling the dataset\n",
      "Epoch421 Batch2 Loss: 22584.2324\n",
      "Epoch421 Batch4 Loss: 28970.3984\n",
      "train Loss: 22052.6450\n",
      "Epoch 422/499\n",
      "----------\n",
      "Epoch422 Batch0 Loss: 24929.7891\n",
      "shuffling the dataset\n",
      "Epoch422 Batch2 Loss: 19103.0566\n",
      "Epoch422 Batch4 Loss: 13848.0225\n",
      "train Loss: 21316.1439\n",
      "Epoch 423/499\n",
      "----------\n",
      "Epoch423 Batch0 Loss: 16394.4941\n",
      "shuffling the dataset\n",
      "Epoch423 Batch2 Loss: 25614.4824\n",
      "Epoch423 Batch4 Loss: 18676.1680\n",
      "train Loss: 19565.1178\n",
      "Epoch 424/499\n",
      "----------\n",
      "Epoch424 Batch0 Loss: 24954.0938\n",
      "Epoch424 Batch2 Loss: 23117.2930\n",
      "shuffling the dataset\n",
      "Epoch424 Batch4 Loss: 27443.0527\n",
      "train Loss: 18862.4863\n",
      "Epoch 425/499\n",
      "----------\n",
      "Epoch425 Batch0 Loss: 22277.6543\n",
      "Epoch425 Batch2 Loss: 20203.8027\n",
      "shuffling the dataset\n",
      "Epoch425 Batch4 Loss: 17170.5098\n",
      "train Loss: 21912.5641\n",
      "Epoch 426/499\n",
      "----------\n",
      "Epoch426 Batch0 Loss: 14514.3730\n",
      "Epoch426 Batch2 Loss: 24396.2285\n",
      "shuffling the dataset\n",
      "Epoch426 Batch4 Loss: 19302.6641\n",
      "train Loss: 19725.3394\n",
      "Epoch 427/499\n",
      "----------\n",
      "Epoch427 Batch0 Loss: 18783.9141\n",
      "Epoch427 Batch2 Loss: 14879.1201\n",
      "shuffling the dataset\n",
      "Epoch427 Batch4 Loss: 19465.4902\n",
      "train Loss: 18453.0750\n",
      "Epoch 428/499\n",
      "----------\n",
      "Epoch428 Batch0 Loss: 23412.5020\n",
      "Epoch428 Batch2 Loss: 19871.2246\n",
      "shuffling the dataset\n",
      "Epoch428 Batch4 Loss: 18658.0234\n",
      "train Loss: 19846.8191\n",
      "Epoch 429/499\n",
      "----------\n",
      "Epoch429 Batch0 Loss: 30420.8105\n",
      "Epoch429 Batch2 Loss: 27398.8184\n",
      "shuffling the dataset\n",
      "Epoch429 Batch4 Loss: 18706.2695\n",
      "train Loss: 22119.0395\n",
      "Epoch 430/499\n",
      "----------\n",
      "Epoch430 Batch0 Loss: 15781.6162\n",
      "Epoch430 Batch2 Loss: 21794.7070\n",
      "shuffling the dataset\n",
      "Epoch430 Batch4 Loss: 30053.6309\n",
      "train Loss: 25246.0486\n",
      "Epoch 431/499\n",
      "----------\n",
      "Epoch431 Batch0 Loss: 30404.3926\n",
      "Epoch431 Batch2 Loss: 22178.1191\n",
      "shuffling the dataset\n",
      "Epoch431 Batch4 Loss: 23327.4980\n",
      "train Loss: 24083.9256\n",
      "Epoch 432/499\n",
      "----------\n",
      "Epoch432 Batch0 Loss: 20274.4922\n",
      "Epoch432 Batch2 Loss: 26434.0781\n",
      "Epoch432 Batch4 Loss: 23415.8652\n",
      "shuffling the dataset\n",
      "train Loss: 22173.8656\n",
      "Epoch 433/499\n",
      "----------\n",
      "Epoch433 Batch0 Loss: 26926.9805\n",
      "Epoch433 Batch2 Loss: 26363.5996\n",
      "Epoch433 Batch4 Loss: 18796.5293\n",
      "shuffling the dataset\n",
      "train Loss: 23851.5200\n",
      "Epoch 434/499\n",
      "----------\n",
      "Epoch434 Batch0 Loss: 19475.3809\n",
      "Epoch434 Batch2 Loss: 25508.6094\n",
      "Epoch434 Batch4 Loss: 20293.6406\n",
      "shuffling the dataset\n",
      "train Loss: 22229.4850\n",
      "Epoch 435/499\n",
      "----------\n",
      "Epoch435 Batch0 Loss: 17951.3809\n",
      "Epoch435 Batch2 Loss: 23136.4004\n",
      "Epoch435 Batch4 Loss: 21019.1895\n",
      "shuffling the dataset\n",
      "train Loss: 22311.8709\n",
      "Epoch 436/499\n",
      "----------\n",
      "Epoch436 Batch0 Loss: 13543.9600\n",
      "Epoch436 Batch2 Loss: 25103.5801\n",
      "Epoch436 Batch4 Loss: 16458.2285\n",
      "train Loss: 21993.2392\n",
      "Epoch 437/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch437 Batch0 Loss: 16744.8691\n",
      "Epoch437 Batch2 Loss: 18785.9727\n",
      "Epoch437 Batch4 Loss: 17640.4141\n",
      "train Loss: 21967.7284\n",
      "Epoch 438/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch438 Batch0 Loss: 25285.8457\n",
      "Epoch438 Batch2 Loss: 20672.9609\n",
      "Epoch438 Batch4 Loss: 19691.8496\n",
      "train Loss: 20676.7145\n",
      "Epoch 439/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch439 Batch0 Loss: 18053.6934\n",
      "Epoch439 Batch2 Loss: 17671.2266\n",
      "Epoch439 Batch4 Loss: 29393.8730\n",
      "train Loss: 20317.0312\n",
      "Epoch 440/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch440 Batch0 Loss: 21287.6250\n",
      "Epoch440 Batch2 Loss: 22333.4824\n",
      "Epoch440 Batch4 Loss: 19185.2578\n",
      "train Loss: 20591.7531\n",
      "Epoch 441/499\n",
      "----------\n",
      "Epoch441 Batch0 Loss: 25473.8301\n",
      "shuffling the dataset\n",
      "Epoch441 Batch2 Loss: 29524.9688\n",
      "Epoch441 Batch4 Loss: 22934.6484\n",
      "train Loss: 20743.4956\n",
      "Epoch 442/499\n",
      "----------\n",
      "Epoch442 Batch0 Loss: 25850.3125\n",
      "shuffling the dataset\n",
      "Epoch442 Batch2 Loss: 17806.5723\n",
      "Epoch442 Batch4 Loss: 18643.5312\n",
      "train Loss: 21258.8575\n",
      "Epoch 443/499\n",
      "----------\n",
      "Epoch443 Batch0 Loss: 27503.4375\n",
      "shuffling the dataset\n",
      "Epoch443 Batch2 Loss: 17298.1816\n",
      "Epoch443 Batch4 Loss: 24055.3867\n",
      "train Loss: 22018.9703\n",
      "Epoch 444/499\n",
      "----------\n",
      "Epoch444 Batch0 Loss: 26127.4102\n",
      "shuffling the dataset\n",
      "Epoch444 Batch2 Loss: 14421.7764\n",
      "Epoch444 Batch4 Loss: 24725.9199\n",
      "train Loss: 22698.1180\n",
      "Epoch 445/499\n",
      "----------\n",
      "Epoch445 Batch0 Loss: 20626.6211\n",
      "shuffling the dataset\n",
      "Epoch445 Batch2 Loss: 26559.3965\n",
      "Epoch445 Batch4 Loss: 20001.2754\n",
      "train Loss: 23179.5891\n",
      "Epoch 446/499\n",
      "----------\n",
      "Epoch446 Batch0 Loss: 18000.7500\n",
      "shuffling the dataset\n",
      "Epoch446 Batch2 Loss: 29979.6719\n",
      "Epoch446 Batch4 Loss: 19318.6406\n",
      "train Loss: 21338.4775\n",
      "Epoch 447/499\n",
      "----------\n",
      "Epoch447 Batch0 Loss: 24247.6328\n",
      "shuffling the dataset\n",
      "Epoch447 Batch2 Loss: 20613.9004\n",
      "Epoch447 Batch4 Loss: 24814.6582\n",
      "train Loss: 21521.2012\n",
      "Epoch 448/499\n",
      "----------\n",
      "Epoch448 Batch0 Loss: 27301.3809\n",
      "shuffling the dataset\n",
      "Epoch448 Batch2 Loss: 17254.1992\n",
      "Epoch448 Batch4 Loss: 23855.5176\n",
      "train Loss: 21283.2319\n",
      "Epoch 449/499\n",
      "----------\n",
      "Epoch449 Batch0 Loss: 26052.9941\n",
      "Epoch449 Batch2 Loss: 25274.6406\n",
      "shuffling the dataset\n",
      "Epoch449 Batch4 Loss: 22254.8301\n",
      "train Loss: 21484.3887\n",
      "Epoch 450/499\n",
      "----------\n",
      "Epoch450 Batch0 Loss: 28425.1660\n",
      "Epoch450 Batch2 Loss: 20800.9531\n",
      "shuffling the dataset\n",
      "Epoch450 Batch4 Loss: 25120.5840\n",
      "train Loss: 22154.0847\n",
      "Epoch 451/499\n",
      "----------\n",
      "Epoch451 Batch0 Loss: 25110.3535\n",
      "Epoch451 Batch2 Loss: 18466.4746\n",
      "shuffling the dataset\n",
      "Epoch451 Batch4 Loss: 21573.5078\n",
      "train Loss: 18732.6844\n",
      "Epoch 452/499\n",
      "----------\n",
      "Epoch452 Batch0 Loss: 23435.9297\n",
      "Epoch452 Batch2 Loss: 31228.8594\n",
      "shuffling the dataset\n",
      "Epoch452 Batch4 Loss: 20699.4512\n",
      "train Loss: 19741.3914\n",
      "Epoch 453/499\n",
      "----------\n",
      "Epoch453 Batch0 Loss: 22229.9590\n",
      "Epoch453 Batch2 Loss: 23313.1426\n",
      "shuffling the dataset\n",
      "Epoch453 Batch4 Loss: 17384.8926\n",
      "train Loss: 20054.3272\n",
      "Epoch 454/499\n",
      "----------\n",
      "Epoch454 Batch0 Loss: 25247.2246\n",
      "Epoch454 Batch2 Loss: 21789.5625\n",
      "shuffling the dataset\n",
      "Epoch454 Batch4 Loss: 23764.7344\n",
      "train Loss: 22894.9463\n",
      "Epoch 455/499\n",
      "----------\n",
      "Epoch455 Batch0 Loss: 20203.8535\n",
      "Epoch455 Batch2 Loss: 19229.7910\n",
      "shuffling the dataset\n",
      "Epoch455 Batch4 Loss: 29759.3008\n",
      "train Loss: 22206.6534\n",
      "Epoch 456/499\n",
      "----------\n",
      "Epoch456 Batch0 Loss: 21756.7285\n",
      "Epoch456 Batch2 Loss: 15731.4219\n",
      "shuffling the dataset\n",
      "Epoch456 Batch4 Loss: 20874.2461\n",
      "train Loss: 18648.0294\n",
      "Epoch 457/499\n",
      "----------\n",
      "Epoch457 Batch0 Loss: 19973.6484\n",
      "Epoch457 Batch2 Loss: 24014.7012\n",
      "Epoch457 Batch4 Loss: 28582.1172\n",
      "shuffling the dataset\n",
      "train Loss: 20029.2694\n",
      "Epoch 458/499\n",
      "----------\n",
      "Epoch458 Batch0 Loss: 22418.1973\n",
      "Epoch458 Batch2 Loss: 15078.4395\n",
      "Epoch458 Batch4 Loss: 23490.5332\n",
      "shuffling the dataset\n",
      "train Loss: 19891.8022\n",
      "Epoch 459/499\n",
      "----------\n",
      "Epoch459 Batch0 Loss: 23557.2305\n",
      "Epoch459 Batch2 Loss: 13487.2520\n",
      "Epoch459 Batch4 Loss: 21934.0547\n",
      "shuffling the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 19246.5250\n",
      "Epoch 460/499\n",
      "----------\n",
      "Epoch460 Batch0 Loss: 17640.6992\n",
      "Epoch460 Batch2 Loss: 15992.4561\n",
      "Epoch460 Batch4 Loss: 27429.5039\n",
      "shuffling the dataset\n",
      "train Loss: 18975.7098\n",
      "Epoch 461/499\n",
      "----------\n",
      "Epoch461 Batch0 Loss: 14023.4863\n",
      "Epoch461 Batch2 Loss: 25314.2422\n",
      "Epoch461 Batch4 Loss: 26069.8867\n",
      "train Loss: 18998.0709\n",
      "Epoch 462/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch462 Batch0 Loss: 16489.0410\n",
      "Epoch462 Batch2 Loss: 19440.7070\n",
      "Epoch462 Batch4 Loss: 20333.5391\n",
      "train Loss: 19615.8581\n",
      "Epoch 463/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch463 Batch0 Loss: 14297.1455\n",
      "Epoch463 Batch2 Loss: 14138.3203\n",
      "Epoch463 Batch4 Loss: 18427.3867\n",
      "train Loss: 18739.7320\n",
      "Epoch 464/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch464 Batch0 Loss: 20317.9609\n",
      "Epoch464 Batch2 Loss: 16995.1543\n",
      "Epoch464 Batch4 Loss: 23076.9668\n",
      "train Loss: 19251.6334\n",
      "Epoch 465/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch465 Batch0 Loss: 22094.6309\n",
      "Epoch465 Batch2 Loss: 18682.4707\n",
      "Epoch465 Batch4 Loss: 19651.7090\n",
      "train Loss: 19727.2513\n",
      "Epoch 466/499\n",
      "----------\n",
      "Epoch466 Batch0 Loss: 21167.0664\n",
      "shuffling the dataset\n",
      "Epoch466 Batch2 Loss: 24890.7344\n",
      "Epoch466 Batch4 Loss: 26254.2715\n",
      "train Loss: 20231.0289\n",
      "Epoch 467/499\n",
      "----------\n",
      "Epoch467 Batch0 Loss: 19473.3438\n",
      "shuffling the dataset\n",
      "Epoch467 Batch2 Loss: 28518.9961\n",
      "Epoch467 Batch4 Loss: 17294.5430\n",
      "train Loss: 19809.1669\n",
      "Epoch 468/499\n",
      "----------\n",
      "Epoch468 Batch0 Loss: 18533.4062\n",
      "shuffling the dataset\n",
      "Epoch468 Batch2 Loss: 16201.9365\n",
      "Epoch468 Batch4 Loss: 22748.3496\n",
      "train Loss: 19824.5745\n",
      "Epoch 469/499\n",
      "----------\n",
      "Epoch469 Batch0 Loss: 24070.7168\n",
      "shuffling the dataset\n",
      "Epoch469 Batch2 Loss: 19854.6504\n",
      "Epoch469 Batch4 Loss: 30371.3828\n",
      "train Loss: 24714.0178\n",
      "Epoch 470/499\n",
      "----------\n",
      "Epoch470 Batch0 Loss: 27146.9570\n",
      "shuffling the dataset\n",
      "Epoch470 Batch2 Loss: 40724.2422\n",
      "Epoch470 Batch4 Loss: 23905.2812\n",
      "train Loss: 24330.3784\n",
      "Epoch 471/499\n",
      "----------\n",
      "Epoch471 Batch0 Loss: 33249.8086\n",
      "shuffling the dataset\n",
      "Epoch471 Batch2 Loss: 38898.7969\n",
      "Epoch471 Batch4 Loss: 22909.8340\n",
      "train Loss: 28571.7497\n",
      "Epoch 472/499\n",
      "----------\n",
      "Epoch472 Batch0 Loss: 23297.9727\n",
      "shuffling the dataset\n",
      "Epoch472 Batch2 Loss: 26107.8477\n",
      "Epoch472 Batch4 Loss: 21719.3652\n",
      "train Loss: 20436.8294\n",
      "Epoch 473/499\n",
      "----------\n",
      "Epoch473 Batch0 Loss: 32751.1934\n",
      "shuffling the dataset\n",
      "Epoch473 Batch2 Loss: 28014.5703\n",
      "Epoch473 Batch4 Loss: 21921.3516\n",
      "train Loss: 25393.4384\n",
      "Epoch 474/499\n",
      "----------\n",
      "Epoch474 Batch0 Loss: 22030.3320\n",
      "Epoch474 Batch2 Loss: 24095.5059\n",
      "shuffling the dataset\n",
      "Epoch474 Batch4 Loss: 23559.4824\n",
      "train Loss: 21090.1752\n",
      "Epoch 475/499\n",
      "----------\n",
      "Epoch475 Batch0 Loss: 18316.6699\n",
      "Epoch475 Batch2 Loss: 24771.9297\n",
      "shuffling the dataset\n",
      "Epoch475 Batch4 Loss: 19629.0918\n",
      "train Loss: 18392.3925\n",
      "Epoch 476/499\n",
      "----------\n",
      "Epoch476 Batch0 Loss: 20596.7383\n",
      "Epoch476 Batch2 Loss: 17400.5195\n",
      "shuffling the dataset\n",
      "Epoch476 Batch4 Loss: 15819.7002\n",
      "train Loss: 19092.6950\n",
      "Epoch 477/499\n",
      "----------\n",
      "Epoch477 Batch0 Loss: 22072.1074\n",
      "Epoch477 Batch2 Loss: 24868.7656\n",
      "shuffling the dataset\n",
      "Epoch477 Batch4 Loss: 26203.9863\n",
      "train Loss: 21663.8841\n",
      "Epoch 478/499\n",
      "----------\n",
      "Epoch478 Batch0 Loss: 18525.6660\n",
      "Epoch478 Batch2 Loss: 23852.9980\n",
      "shuffling the dataset\n",
      "Epoch478 Batch4 Loss: 24779.2070\n",
      "train Loss: 18118.2878\n",
      "Epoch 479/499\n",
      "----------\n",
      "Epoch479 Batch0 Loss: 27599.7949\n",
      "Epoch479 Batch2 Loss: 23061.2988\n",
      "shuffling the dataset\n",
      "Epoch479 Batch4 Loss: 24232.0273\n",
      "train Loss: 18942.8842\n",
      "Epoch 480/499\n",
      "----------\n",
      "Epoch480 Batch0 Loss: 16209.2285\n",
      "Epoch480 Batch2 Loss: 14103.7285\n",
      "shuffling the dataset\n",
      "Epoch480 Batch4 Loss: 22420.3828\n",
      "train Loss: 18592.6800\n",
      "Epoch 481/499\n",
      "----------\n",
      "Epoch481 Batch0 Loss: 19547.7188\n",
      "Epoch481 Batch2 Loss: 23534.5918\n",
      "shuffling the dataset\n",
      "Epoch481 Batch4 Loss: 12453.1445\n",
      "train Loss: 16763.4447\n",
      "Epoch 482/499\n",
      "----------\n",
      "Epoch482 Batch0 Loss: 16860.2402\n",
      "Epoch482 Batch2 Loss: 27281.2051\n",
      "Epoch482 Batch4 Loss: 20075.7461\n",
      "shuffling the dataset\n",
      "train Loss: 18539.3739\n",
      "Epoch 483/499\n",
      "----------\n",
      "Epoch483 Batch0 Loss: 20798.8164\n",
      "Epoch483 Batch2 Loss: 12046.9072\n",
      "Epoch483 Batch4 Loss: 22417.6133\n",
      "shuffling the dataset\n",
      "train Loss: 16729.2681\n",
      "Epoch 484/499\n",
      "----------\n",
      "Epoch484 Batch0 Loss: 18906.9941\n",
      "Epoch484 Batch2 Loss: 16836.5566\n",
      "Epoch484 Batch4 Loss: 23876.6289\n",
      "shuffling the dataset\n",
      "train Loss: 17436.0078\n",
      "Epoch 485/499\n",
      "----------\n",
      "Epoch485 Batch0 Loss: 18549.1016\n",
      "Epoch485 Batch2 Loss: 13732.2881\n",
      "Epoch485 Batch4 Loss: 22881.5117\n",
      "shuffling the dataset\n",
      "train Loss: 16978.6297\n",
      "Epoch 486/499\n",
      "----------\n",
      "Epoch486 Batch0 Loss: 14567.1787\n",
      "Epoch486 Batch2 Loss: 24296.5605\n",
      "Epoch486 Batch4 Loss: 11460.7236\n",
      "train Loss: 17302.5488\n",
      "Epoch 487/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch487 Batch0 Loss: 12613.5020\n",
      "Epoch487 Batch2 Loss: 14331.7051\n",
      "Epoch487 Batch4 Loss: 15030.0938\n",
      "train Loss: 16865.7459\n",
      "Epoch 488/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch488 Batch0 Loss: 17828.7598\n",
      "Epoch488 Batch2 Loss: 15537.5342\n",
      "Epoch488 Batch4 Loss: 11016.3945\n",
      "train Loss: 17071.4836\n",
      "Epoch 489/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch489 Batch0 Loss: 16882.8730\n",
      "Epoch489 Batch2 Loss: 12439.9297\n",
      "Epoch489 Batch4 Loss: 22984.9004\n",
      "train Loss: 16492.0192\n",
      "Epoch 490/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch490 Batch0 Loss: 19787.6211\n",
      "Epoch490 Batch2 Loss: 17930.6250\n",
      "Epoch490 Batch4 Loss: 23314.8984\n",
      "train Loss: 16974.1545\n",
      "Epoch 491/499\n",
      "----------\n",
      "Epoch491 Batch0 Loss: 23118.0137\n",
      "shuffling the dataset\n",
      "Epoch491 Batch2 Loss: 17120.3750\n",
      "Epoch491 Batch4 Loss: 16083.6484\n",
      "train Loss: 18135.4200\n",
      "Epoch 492/499\n",
      "----------\n",
      "Epoch492 Batch0 Loss: 18129.4824\n",
      "shuffling the dataset\n",
      "Epoch492 Batch2 Loss: 23301.1719\n",
      "Epoch492 Batch4 Loss: 21246.9746\n",
      "train Loss: 19200.7797\n",
      "Epoch 493/499\n",
      "----------\n",
      "Epoch493 Batch0 Loss: 13530.6123\n",
      "shuffling the dataset\n",
      "Epoch493 Batch2 Loss: 17741.9395\n",
      "Epoch493 Batch4 Loss: 19516.6230\n",
      "train Loss: 15100.6230\n",
      "Epoch 494/499\n",
      "----------\n",
      "Epoch494 Batch0 Loss: 29055.8770\n",
      "shuffling the dataset\n",
      "Epoch494 Batch2 Loss: 23009.1211\n",
      "Epoch494 Batch4 Loss: 22086.8691\n",
      "train Loss: 20421.5337\n",
      "Epoch 495/499\n",
      "----------\n",
      "Epoch495 Batch0 Loss: 14033.8721\n",
      "shuffling the dataset\n",
      "Epoch495 Batch2 Loss: 15468.5166\n",
      "Epoch495 Batch4 Loss: 29199.0176\n",
      "train Loss: 17914.0972\n",
      "Epoch 496/499\n",
      "----------\n",
      "Epoch496 Batch0 Loss: 30483.1934\n",
      "shuffling the dataset\n",
      "Epoch496 Batch2 Loss: 30513.7852\n",
      "Epoch496 Batch4 Loss: 52079.9883\n",
      "train Loss: 27683.7419\n",
      "Epoch 497/499\n",
      "----------\n",
      "Epoch497 Batch0 Loss: 84910.1719\n",
      "shuffling the dataset\n",
      "Epoch497 Batch2 Loss: 47955.1875\n",
      "Epoch497 Batch4 Loss: 96174.8594\n",
      "train Loss: 57860.9119\n",
      "Epoch 498/499\n",
      "----------\n",
      "Epoch498 Batch0 Loss: 65464.0117\n",
      "shuffling the dataset\n",
      "Epoch498 Batch2 Loss: 48083.7656\n",
      "Epoch498 Batch4 Loss: 38164.0000\n",
      "train Loss: 60841.7006\n",
      "Epoch 499/499\n",
      "----------\n",
      "Epoch499 Batch0 Loss: 73617.4766\n",
      "Epoch499 Batch2 Loss: 50087.1367\n",
      "shuffling the dataset\n",
      "Epoch499 Batch4 Loss: 45448.5078\n",
      "train Loss: 52050.8187\n",
      "Epoch 500/499\n",
      "----------\n",
      "Epoch500 Batch0 Loss: 42835.3750\n",
      "Epoch500 Batch2 Loss: 32759.8867\n",
      "shuffling the dataset\n",
      "Epoch500 Batch4 Loss: 32620.7676\n",
      "train Loss: 41734.7884\n",
      "Epoch 501/499\n",
      "----------\n",
      "Epoch501 Batch0 Loss: 39739.5898\n",
      "Epoch501 Batch2 Loss: 38048.3594\n",
      "shuffling the dataset\n",
      "Epoch501 Batch4 Loss: 34309.7148\n",
      "train Loss: 36886.4869\n",
      "Epoch 502/499\n",
      "----------\n",
      "Epoch502 Batch0 Loss: 35390.0586\n",
      "Epoch502 Batch2 Loss: 38287.5156\n",
      "shuffling the dataset\n",
      "Epoch502 Batch4 Loss: 34269.8594\n",
      "train Loss: 29499.8416\n",
      "Epoch 503/499\n",
      "----------\n",
      "Epoch503 Batch0 Loss: 22650.6230\n",
      "Epoch503 Batch2 Loss: 27192.0586\n",
      "shuffling the dataset\n",
      "Epoch503 Batch4 Loss: 32925.4727\n",
      "train Loss: 26203.9562\n",
      "Epoch 504/499\n",
      "----------\n",
      "Epoch504 Batch0 Loss: 22596.9492\n",
      "Epoch504 Batch2 Loss: 24929.0117\n",
      "shuffling the dataset\n",
      "Epoch504 Batch4 Loss: 19479.2910\n",
      "train Loss: 21871.2812\n",
      "Epoch 505/499\n",
      "----------\n",
      "Epoch505 Batch0 Loss: 13858.1191\n",
      "Epoch505 Batch2 Loss: 30186.8965\n",
      "shuffling the dataset\n",
      "Epoch505 Batch4 Loss: 18014.1230\n",
      "train Loss: 20796.7684\n",
      "Epoch 506/499\n",
      "----------\n",
      "Epoch506 Batch0 Loss: 16565.9688\n",
      "Epoch506 Batch2 Loss: 19226.7969\n",
      "shuffling the dataset\n",
      "Epoch506 Batch4 Loss: 26164.6016\n",
      "train Loss: 19193.6109\n",
      "Epoch 507/499\n",
      "----------\n",
      "Epoch507 Batch0 Loss: 19299.3125\n",
      "Epoch507 Batch2 Loss: 18689.6406\n",
      "Epoch507 Batch4 Loss: 20817.0742\n",
      "shuffling the dataset\n",
      "train Loss: 18331.5319\n",
      "Epoch 508/499\n",
      "----------\n",
      "Epoch508 Batch0 Loss: 17926.2578\n",
      "Epoch508 Batch2 Loss: 22803.9023\n",
      "Epoch508 Batch4 Loss: 19171.7656\n",
      "shuffling the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 18234.2836\n",
      "Epoch 509/499\n",
      "----------\n",
      "Epoch509 Batch0 Loss: 20100.2285\n",
      "Epoch509 Batch2 Loss: 14096.3086\n",
      "Epoch509 Batch4 Loss: 15514.2969\n",
      "shuffling the dataset\n",
      "train Loss: 15448.0827\n",
      "Epoch 510/499\n",
      "----------\n",
      "Epoch510 Batch0 Loss: 14056.1611\n",
      "Epoch510 Batch2 Loss: 10259.4141\n",
      "Epoch510 Batch4 Loss: 22803.6738\n",
      "shuffling the dataset\n",
      "train Loss: 15683.4144\n",
      "Epoch 511/499\n",
      "----------\n",
      "Epoch511 Batch0 Loss: 13170.0635\n",
      "Epoch511 Batch2 Loss: 14827.2637\n",
      "Epoch511 Batch4 Loss: 11317.6992\n",
      "train Loss: 15257.8714\n",
      "Epoch 512/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch512 Batch0 Loss: 10377.0439\n",
      "Epoch512 Batch2 Loss: 15412.4863\n",
      "Epoch512 Batch4 Loss: 15908.9512\n",
      "train Loss: 14807.8194\n",
      "Epoch 513/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch513 Batch0 Loss: 11034.1914\n",
      "Epoch513 Batch2 Loss: 16755.4570\n",
      "Epoch513 Batch4 Loss: 9373.5527\n",
      "train Loss: 13524.0623\n",
      "Epoch 514/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch514 Batch0 Loss: 20788.6172\n",
      "Epoch514 Batch2 Loss: 20590.1641\n",
      "Epoch514 Batch4 Loss: 11381.9414\n",
      "train Loss: 14614.8425\n",
      "Epoch 515/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch515 Batch0 Loss: 15931.0000\n",
      "Epoch515 Batch2 Loss: 14017.7529\n",
      "Epoch515 Batch4 Loss: 15431.0176\n",
      "train Loss: 15893.0248\n",
      "Epoch 516/499\n",
      "----------\n",
      "Epoch516 Batch0 Loss: 7671.7349\n",
      "shuffling the dataset\n",
      "Epoch516 Batch2 Loss: 10513.1895\n",
      "Epoch516 Batch4 Loss: 16631.2324\n",
      "train Loss: 13041.7598\n",
      "Epoch 517/499\n",
      "----------\n",
      "Epoch517 Batch0 Loss: 15168.2744\n",
      "shuffling the dataset\n",
      "Epoch517 Batch2 Loss: 14856.0693\n",
      "Epoch517 Batch4 Loss: 17093.6406\n",
      "train Loss: 14395.3761\n",
      "Epoch 518/499\n",
      "----------\n",
      "Epoch518 Batch0 Loss: 17865.6387\n",
      "shuffling the dataset\n",
      "Epoch518 Batch2 Loss: 19254.7520\n",
      "Epoch518 Batch4 Loss: 13892.4600\n",
      "train Loss: 16153.6530\n",
      "Epoch 519/499\n",
      "----------\n",
      "Epoch519 Batch0 Loss: 12753.2939\n",
      "shuffling the dataset\n",
      "Epoch519 Batch2 Loss: 17690.0195\n",
      "Epoch519 Batch4 Loss: 15980.4766\n",
      "train Loss: 13364.4338\n",
      "Epoch 520/499\n",
      "----------\n",
      "Epoch520 Batch0 Loss: 14971.0967\n",
      "shuffling the dataset\n",
      "Epoch520 Batch2 Loss: 9821.2793\n",
      "Epoch520 Batch4 Loss: 15473.9590\n",
      "train Loss: 14772.8273\n",
      "Epoch 521/499\n",
      "----------\n",
      "Epoch521 Batch0 Loss: 23682.8457\n",
      "shuffling the dataset\n",
      "Epoch521 Batch2 Loss: 21076.1602\n",
      "Epoch521 Batch4 Loss: 11701.9619\n",
      "train Loss: 17173.3503\n",
      "Epoch 522/499\n",
      "----------\n",
      "Epoch522 Batch0 Loss: 24733.7305\n",
      "shuffling the dataset\n",
      "Epoch522 Batch2 Loss: 17015.1562\n",
      "Epoch522 Batch4 Loss: 17210.1562\n",
      "train Loss: 18080.1680\n",
      "Epoch 523/499\n",
      "----------\n",
      "Epoch523 Batch0 Loss: 11146.2871\n",
      "shuffling the dataset\n",
      "Epoch523 Batch2 Loss: 13399.9961\n",
      "Epoch523 Batch4 Loss: 10475.7275\n",
      "train Loss: 12492.4964\n",
      "Epoch 524/499\n",
      "----------\n",
      "Epoch524 Batch0 Loss: 19365.4727\n",
      "Epoch524 Batch2 Loss: 12797.6494\n",
      "shuffling the dataset\n",
      "Epoch524 Batch4 Loss: 25972.5098\n",
      "train Loss: 18348.5375\n",
      "Epoch 525/499\n",
      "----------\n",
      "Epoch525 Batch0 Loss: 15612.9443\n",
      "Epoch525 Batch2 Loss: 12177.2744\n",
      "shuffling the dataset\n",
      "Epoch525 Batch4 Loss: 19387.2539\n",
      "train Loss: 15878.0641\n",
      "Epoch 526/499\n",
      "----------\n",
      "Epoch526 Batch0 Loss: 18218.8770\n",
      "Epoch526 Batch2 Loss: 18294.0605\n",
      "shuffling the dataset\n",
      "Epoch526 Batch4 Loss: 13395.2646\n",
      "train Loss: 15565.9709\n",
      "Epoch 527/499\n",
      "----------\n",
      "Epoch527 Batch0 Loss: 13143.4785\n",
      "Epoch527 Batch2 Loss: 12040.5557\n",
      "shuffling the dataset\n",
      "Epoch527 Batch4 Loss: 16616.6758\n",
      "train Loss: 16562.1917\n",
      "Epoch 528/499\n",
      "----------\n",
      "Epoch528 Batch0 Loss: 18647.7129\n",
      "Epoch528 Batch2 Loss: 13208.3857\n",
      "shuffling the dataset\n",
      "Epoch528 Batch4 Loss: 18537.0273\n",
      "train Loss: 16963.4081\n",
      "Epoch 529/499\n",
      "----------\n",
      "Epoch529 Batch0 Loss: 13159.6631\n",
      "Epoch529 Batch2 Loss: 17892.6055\n",
      "shuffling the dataset\n",
      "Epoch529 Batch4 Loss: 10842.0273\n",
      "train Loss: 13593.0398\n",
      "Epoch 530/499\n",
      "----------\n",
      "Epoch530 Batch0 Loss: 19114.5449\n",
      "Epoch530 Batch2 Loss: 15457.1318\n",
      "shuffling the dataset\n",
      "Epoch530 Batch4 Loss: 17578.2285\n",
      "train Loss: 17656.6073\n",
      "Epoch 531/499\n",
      "----------\n",
      "Epoch531 Batch0 Loss: 19757.7090\n",
      "Epoch531 Batch2 Loss: 17961.9922\n",
      "shuffling the dataset\n",
      "Epoch531 Batch4 Loss: 19267.8477\n",
      "train Loss: 17263.9603\n",
      "Epoch 532/499\n",
      "----------\n",
      "Epoch532 Batch0 Loss: 13803.7725\n",
      "Epoch532 Batch2 Loss: 20507.1621\n",
      "Epoch532 Batch4 Loss: 19785.9375\n",
      "shuffling the dataset\n",
      "train Loss: 16171.2744\n",
      "Epoch 533/499\n",
      "----------\n",
      "Epoch533 Batch0 Loss: 9438.4414\n",
      "Epoch533 Batch2 Loss: 16793.1543\n",
      "Epoch533 Batch4 Loss: 16291.9043\n",
      "shuffling the dataset\n",
      "train Loss: 17309.5997\n",
      "Epoch 534/499\n",
      "----------\n",
      "Epoch534 Batch0 Loss: 6959.8599\n",
      "Epoch534 Batch2 Loss: 19635.7090\n",
      "Epoch534 Batch4 Loss: 15942.2373\n",
      "shuffling the dataset\n",
      "train Loss: 15756.7843\n",
      "Epoch 535/499\n",
      "----------\n",
      "Epoch535 Batch0 Loss: 15787.5850\n",
      "Epoch535 Batch2 Loss: 19733.7305\n",
      "Epoch535 Batch4 Loss: 13375.4531\n",
      "shuffling the dataset\n",
      "train Loss: 15005.8433\n",
      "Epoch 536/499\n",
      "----------\n",
      "Epoch536 Batch0 Loss: 11775.7578\n",
      "Epoch536 Batch2 Loss: 12927.7275\n",
      "Epoch536 Batch4 Loss: 18257.7148\n",
      "train Loss: 14709.4002\n",
      "Epoch 537/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch537 Batch0 Loss: 10225.0801\n",
      "Epoch537 Batch2 Loss: 25185.2207\n",
      "Epoch537 Batch4 Loss: 16421.2949\n",
      "train Loss: 14182.2875\n",
      "Epoch 538/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch538 Batch0 Loss: 13396.4033\n",
      "Epoch538 Batch2 Loss: 11844.4463\n",
      "Epoch538 Batch4 Loss: 11856.1699\n",
      "train Loss: 13103.3570\n",
      "Epoch 539/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch539 Batch0 Loss: 21704.6309\n",
      "Epoch539 Batch2 Loss: 13553.6035\n",
      "Epoch539 Batch4 Loss: 11098.4189\n",
      "train Loss: 15269.0877\n",
      "Epoch 540/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch540 Batch0 Loss: 11919.8672\n",
      "Epoch540 Batch2 Loss: 10983.7988\n",
      "Epoch540 Batch4 Loss: 19319.1289\n",
      "train Loss: 14007.5220\n",
      "Epoch 541/499\n",
      "----------\n",
      "Epoch541 Batch0 Loss: 9901.2930\n",
      "shuffling the dataset\n",
      "Epoch541 Batch2 Loss: 15116.4570\n",
      "Epoch541 Batch4 Loss: 15829.7607\n",
      "train Loss: 13708.4050\n",
      "Epoch 542/499\n",
      "----------\n",
      "Epoch542 Batch0 Loss: 15469.2656\n",
      "shuffling the dataset\n",
      "Epoch542 Batch2 Loss: 16163.7227\n",
      "Epoch542 Batch4 Loss: 12761.5205\n",
      "train Loss: 15173.3148\n",
      "Epoch 543/499\n",
      "----------\n",
      "Epoch543 Batch0 Loss: 13901.0840\n",
      "shuffling the dataset\n",
      "Epoch543 Batch2 Loss: 16211.2510\n",
      "Epoch543 Batch4 Loss: 17730.6094\n",
      "train Loss: 15772.7536\n",
      "Epoch 544/499\n",
      "----------\n",
      "Epoch544 Batch0 Loss: 20505.8223\n",
      "shuffling the dataset\n",
      "Epoch544 Batch2 Loss: 17413.5098\n",
      "Epoch544 Batch4 Loss: 14893.8291\n",
      "train Loss: 16307.1380\n",
      "Epoch 545/499\n",
      "----------\n",
      "Epoch545 Batch0 Loss: 14603.3438\n",
      "shuffling the dataset\n",
      "Epoch545 Batch2 Loss: 20481.6309\n",
      "Epoch545 Batch4 Loss: 11897.9893\n",
      "train Loss: 14077.5944\n",
      "Epoch 546/499\n",
      "----------\n",
      "Epoch546 Batch0 Loss: 27847.9141\n",
      "shuffling the dataset\n",
      "Epoch546 Batch2 Loss: 9189.5186\n",
      "Epoch546 Batch4 Loss: 17184.0781\n",
      "train Loss: 16969.1455\n",
      "Epoch 547/499\n",
      "----------\n",
      "Epoch547 Batch0 Loss: 13967.7412\n",
      "shuffling the dataset\n",
      "Epoch547 Batch2 Loss: 9266.4258\n",
      "Epoch547 Batch4 Loss: 13249.2568\n",
      "train Loss: 14185.5300\n",
      "Epoch 548/499\n",
      "----------\n",
      "Epoch548 Batch0 Loss: 15182.2510\n",
      "shuffling the dataset\n",
      "Epoch548 Batch2 Loss: 12908.5752\n",
      "Epoch548 Batch4 Loss: 15557.2314\n",
      "train Loss: 14772.1445\n",
      "Epoch 549/499\n",
      "----------\n",
      "Epoch549 Batch0 Loss: 12315.5986\n",
      "Epoch549 Batch2 Loss: 17124.9727\n",
      "shuffling the dataset\n",
      "Epoch549 Batch4 Loss: 11238.2559\n",
      "train Loss: 14468.0881\n",
      "Epoch 550/499\n",
      "----------\n",
      "Epoch550 Batch0 Loss: 17737.3867\n",
      "Epoch550 Batch2 Loss: 12407.8516\n",
      "shuffling the dataset\n",
      "Epoch550 Batch4 Loss: 9651.7969\n",
      "train Loss: 13087.2527\n",
      "Epoch 551/499\n",
      "----------\n",
      "Epoch551 Batch0 Loss: 18816.5137\n",
      "Epoch551 Batch2 Loss: 13397.3545\n",
      "shuffling the dataset\n",
      "Epoch551 Batch4 Loss: 10872.9629\n",
      "train Loss: 14410.7970\n",
      "Epoch 552/499\n",
      "----------\n",
      "Epoch552 Batch0 Loss: 23526.7598\n",
      "Epoch552 Batch2 Loss: 12159.5039\n",
      "shuffling the dataset\n",
      "Epoch552 Batch4 Loss: 16361.5020\n",
      "train Loss: 15301.4889\n",
      "Epoch 553/499\n",
      "----------\n",
      "Epoch553 Batch0 Loss: 13609.2920\n",
      "Epoch553 Batch2 Loss: 16122.3896\n",
      "shuffling the dataset\n",
      "Epoch553 Batch4 Loss: 8755.5605\n",
      "train Loss: 12651.8502\n",
      "Epoch 554/499\n",
      "----------\n",
      "Epoch554 Batch0 Loss: 17639.0918\n",
      "Epoch554 Batch2 Loss: 21677.0840\n",
      "shuffling the dataset\n",
      "Epoch554 Batch4 Loss: 12100.1758\n",
      "train Loss: 14887.2647\n",
      "Epoch 555/499\n",
      "----------\n",
      "Epoch555 Batch0 Loss: 18907.5215\n",
      "Epoch555 Batch2 Loss: 12183.1719\n",
      "shuffling the dataset\n",
      "Epoch555 Batch4 Loss: 15390.1768\n",
      "train Loss: 14985.7817\n",
      "Epoch 556/499\n",
      "----------\n",
      "Epoch556 Batch0 Loss: 9312.3125\n",
      "Epoch556 Batch2 Loss: 16019.9717\n",
      "shuffling the dataset\n",
      "Epoch556 Batch4 Loss: 13624.0000\n",
      "train Loss: 13207.2086\n",
      "Epoch 557/499\n",
      "----------\n",
      "Epoch557 Batch0 Loss: 13776.3359\n",
      "Epoch557 Batch2 Loss: 13883.2393\n",
      "Epoch557 Batch4 Loss: 17279.9590\n",
      "shuffling the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 13245.3581\n",
      "Epoch 558/499\n",
      "----------\n",
      "Epoch558 Batch0 Loss: 18090.0430\n",
      "Epoch558 Batch2 Loss: 17582.8203\n",
      "Epoch558 Batch4 Loss: 13758.9229\n",
      "shuffling the dataset\n",
      "train Loss: 14086.5508\n",
      "Epoch 559/499\n",
      "----------\n",
      "Epoch559 Batch0 Loss: 14885.0479\n",
      "Epoch559 Batch2 Loss: 13656.2090\n",
      "Epoch559 Batch4 Loss: 7759.0327\n",
      "shuffling the dataset\n",
      "train Loss: 11937.6543\n",
      "Epoch 560/499\n",
      "----------\n",
      "Epoch560 Batch0 Loss: 12279.1416\n",
      "Epoch560 Batch2 Loss: 15011.6680\n",
      "Epoch560 Batch4 Loss: 16130.4365\n",
      "shuffling the dataset\n",
      "train Loss: 12990.3364\n",
      "Epoch 561/499\n",
      "----------\n",
      "Epoch561 Batch0 Loss: 9330.2510\n",
      "Epoch561 Batch2 Loss: 13766.3154\n",
      "Epoch561 Batch4 Loss: 15321.4893\n",
      "train Loss: 12108.1702\n",
      "Epoch 562/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch562 Batch0 Loss: 11555.2148\n",
      "Epoch562 Batch2 Loss: 10505.3457\n",
      "Epoch562 Batch4 Loss: 19568.9512\n",
      "train Loss: 12557.6739\n",
      "Epoch 563/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch563 Batch0 Loss: 9585.8252\n",
      "Epoch563 Batch2 Loss: 9510.2949\n",
      "Epoch563 Batch4 Loss: 13469.5869\n",
      "train Loss: 11532.3466\n",
      "Epoch 564/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch564 Batch0 Loss: 17010.7832\n",
      "Epoch564 Batch2 Loss: 14592.6953\n",
      "Epoch564 Batch4 Loss: 13025.8350\n",
      "train Loss: 11695.7417\n",
      "Epoch 565/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch565 Batch0 Loss: 22807.0176\n",
      "Epoch565 Batch2 Loss: 15829.6836\n",
      "Epoch565 Batch4 Loss: 10000.6113\n",
      "train Loss: 14232.5597\n",
      "Epoch 566/499\n",
      "----------\n",
      "Epoch566 Batch0 Loss: 15466.6611\n",
      "shuffling the dataset\n",
      "Epoch566 Batch2 Loss: 16230.2305\n",
      "Epoch566 Batch4 Loss: 8349.1738\n",
      "train Loss: 12770.9122\n",
      "Epoch 567/499\n",
      "----------\n",
      "Epoch567 Batch0 Loss: 16514.1816\n",
      "shuffling the dataset\n",
      "Epoch567 Batch2 Loss: 11891.5918\n",
      "Epoch567 Batch4 Loss: 15952.8418\n",
      "train Loss: 13477.7809\n",
      "Epoch 568/499\n",
      "----------\n",
      "Epoch568 Batch0 Loss: 14814.3066\n",
      "shuffling the dataset\n",
      "Epoch568 Batch2 Loss: 14616.3799\n",
      "Epoch568 Batch4 Loss: 10873.4756\n",
      "train Loss: 13014.7170\n",
      "Epoch 569/499\n",
      "----------\n",
      "Epoch569 Batch0 Loss: 8882.0547\n",
      "shuffling the dataset\n",
      "Epoch569 Batch2 Loss: 10461.0742\n",
      "Epoch569 Batch4 Loss: 13784.4277\n",
      "train Loss: 13275.6214\n",
      "Epoch 570/499\n",
      "----------\n",
      "Epoch570 Batch0 Loss: 13021.0791\n",
      "shuffling the dataset\n",
      "Epoch570 Batch2 Loss: 15279.0059\n",
      "Epoch570 Batch4 Loss: 11441.8232\n",
      "train Loss: 14760.5177\n",
      "Epoch 571/499\n",
      "----------\n",
      "Epoch571 Batch0 Loss: 11377.9951\n",
      "shuffling the dataset\n",
      "Epoch571 Batch2 Loss: 8809.6992\n",
      "Epoch571 Batch4 Loss: 10807.5947\n",
      "train Loss: 12153.8591\n",
      "Epoch 572/499\n",
      "----------\n",
      "Epoch572 Batch0 Loss: 11981.8125\n",
      "shuffling the dataset\n",
      "Epoch572 Batch2 Loss: 13963.3076\n",
      "Epoch572 Batch4 Loss: 15587.0518\n",
      "train Loss: 13490.0133\n",
      "Epoch 573/499\n",
      "----------\n",
      "Epoch573 Batch0 Loss: 7099.7378\n",
      "shuffling the dataset\n",
      "Epoch573 Batch2 Loss: 19247.2266\n",
      "Epoch573 Batch4 Loss: 10340.8467\n",
      "train Loss: 12163.6946\n",
      "Epoch 574/499\n",
      "----------\n",
      "Epoch574 Batch0 Loss: 13345.9404\n",
      "Epoch574 Batch2 Loss: 18096.8262\n",
      "shuffling the dataset\n",
      "Epoch574 Batch4 Loss: 9149.4932\n",
      "train Loss: 13823.9878\n",
      "Epoch 575/499\n",
      "----------\n",
      "Epoch575 Batch0 Loss: 14659.7158\n",
      "Epoch575 Batch2 Loss: 18258.0508\n",
      "shuffling the dataset\n",
      "Epoch575 Batch4 Loss: 11091.3164\n",
      "train Loss: 14822.0550\n",
      "Epoch 576/499\n",
      "----------\n",
      "Epoch576 Batch0 Loss: 17492.5801\n",
      "Epoch576 Batch2 Loss: 13111.6572\n",
      "shuffling the dataset\n",
      "Epoch576 Batch4 Loss: 15035.3730\n",
      "train Loss: 14963.4483\n",
      "Epoch 577/499\n",
      "----------\n",
      "Epoch577 Batch0 Loss: 17522.9609\n",
      "Epoch577 Batch2 Loss: 14729.7393\n",
      "shuffling the dataset\n",
      "Epoch577 Batch4 Loss: 16070.9561\n",
      "train Loss: 16041.5686\n",
      "Epoch 578/499\n",
      "----------\n",
      "Epoch578 Batch0 Loss: 19531.1699\n",
      "Epoch578 Batch2 Loss: 13992.1992\n",
      "shuffling the dataset\n",
      "Epoch578 Batch4 Loss: 17536.1895\n",
      "train Loss: 15034.9208\n",
      "Epoch 579/499\n",
      "----------\n",
      "Epoch579 Batch0 Loss: 18623.1797\n",
      "Epoch579 Batch2 Loss: 12723.7861\n",
      "shuffling the dataset\n",
      "Epoch579 Batch4 Loss: 18522.7578\n",
      "train Loss: 15588.1306\n",
      "Epoch 580/499\n",
      "----------\n",
      "Epoch580 Batch0 Loss: 11207.2148\n",
      "Epoch580 Batch2 Loss: 11675.1123\n",
      "shuffling the dataset\n",
      "Epoch580 Batch4 Loss: 13519.6123\n",
      "train Loss: 13606.6447\n",
      "Epoch 581/499\n",
      "----------\n",
      "Epoch581 Batch0 Loss: 11052.8623\n",
      "Epoch581 Batch2 Loss: 16163.7324\n",
      "shuffling the dataset\n",
      "Epoch581 Batch4 Loss: 15079.1045\n",
      "train Loss: 15027.7125\n",
      "Epoch 582/499\n",
      "----------\n",
      "Epoch582 Batch0 Loss: 18905.6875\n",
      "Epoch582 Batch2 Loss: 8461.1533\n",
      "Epoch582 Batch4 Loss: 16354.5107\n",
      "shuffling the dataset\n",
      "train Loss: 13860.0534\n",
      "Epoch 583/499\n",
      "----------\n",
      "Epoch583 Batch0 Loss: 18940.7539\n",
      "Epoch583 Batch2 Loss: 16273.1416\n",
      "Epoch583 Batch4 Loss: 13693.6943\n",
      "shuffling the dataset\n",
      "train Loss: 15604.8573\n",
      "Epoch 584/499\n",
      "----------\n",
      "Epoch584 Batch0 Loss: 16737.9785\n",
      "Epoch584 Batch2 Loss: 22899.5957\n",
      "Epoch584 Batch4 Loss: 17242.9961\n",
      "shuffling the dataset\n",
      "train Loss: 16070.3116\n",
      "Epoch 585/499\n",
      "----------\n",
      "Epoch585 Batch0 Loss: 14058.4277\n",
      "Epoch585 Batch2 Loss: 15353.5547\n",
      "Epoch585 Batch4 Loss: 16162.8145\n",
      "shuffling the dataset\n",
      "train Loss: 14928.9802\n",
      "Epoch 586/499\n",
      "----------\n",
      "Epoch586 Batch0 Loss: 18355.3223\n",
      "Epoch586 Batch2 Loss: 12030.9600\n",
      "Epoch586 Batch4 Loss: 17045.1953\n",
      "train Loss: 14231.2114\n",
      "Epoch 587/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch587 Batch0 Loss: 13157.6738\n",
      "Epoch587 Batch2 Loss: 12144.1494\n",
      "Epoch587 Batch4 Loss: 16114.3496\n",
      "train Loss: 14130.6161\n",
      "Epoch 588/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch588 Batch0 Loss: 11482.3164\n",
      "Epoch588 Batch2 Loss: 15980.8408\n",
      "Epoch588 Batch4 Loss: 14166.3486\n",
      "train Loss: 12427.5616\n",
      "Epoch 589/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch589 Batch0 Loss: 13987.2002\n",
      "Epoch589 Batch2 Loss: 14820.3730\n",
      "Epoch589 Batch4 Loss: 18493.6465\n",
      "train Loss: 13791.4242\n",
      "Epoch 590/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch590 Batch0 Loss: 7755.7246\n",
      "Epoch590 Batch2 Loss: 12922.8037\n",
      "Epoch590 Batch4 Loss: 11900.7598\n",
      "train Loss: 11888.1384\n",
      "Epoch 591/499\n",
      "----------\n",
      "Epoch591 Batch0 Loss: 13458.8701\n",
      "shuffling the dataset\n",
      "Epoch591 Batch2 Loss: 8550.9238\n",
      "Epoch591 Batch4 Loss: 10004.2930\n",
      "train Loss: 12601.1392\n",
      "Epoch 592/499\n",
      "----------\n",
      "Epoch592 Batch0 Loss: 14199.4033\n",
      "shuffling the dataset\n",
      "Epoch592 Batch2 Loss: 11228.2217\n",
      "Epoch592 Batch4 Loss: 9884.2744\n",
      "train Loss: 12621.4013\n",
      "Epoch 593/499\n",
      "----------\n",
      "Epoch593 Batch0 Loss: 13596.1865\n",
      "shuffling the dataset\n",
      "Epoch593 Batch2 Loss: 12836.0195\n",
      "Epoch593 Batch4 Loss: 8675.3076\n",
      "train Loss: 10872.2783\n",
      "Epoch 594/499\n",
      "----------\n",
      "Epoch594 Batch0 Loss: 15372.1211\n",
      "shuffling the dataset\n",
      "Epoch594 Batch2 Loss: 9001.3672\n",
      "Epoch594 Batch4 Loss: 18791.7793\n",
      "train Loss: 12534.2175\n",
      "Epoch 595/499\n",
      "----------\n",
      "Epoch595 Batch0 Loss: 11930.8018\n",
      "shuffling the dataset\n",
      "Epoch595 Batch2 Loss: 14840.5088\n",
      "Epoch595 Batch4 Loss: 12959.0771\n",
      "train Loss: 11449.3669\n",
      "Epoch 596/499\n",
      "----------\n",
      "Epoch596 Batch0 Loss: 19217.1895\n",
      "shuffling the dataset\n",
      "Epoch596 Batch2 Loss: 8289.5371\n",
      "Epoch596 Batch4 Loss: 17032.6035\n",
      "train Loss: 12531.7191\n",
      "Epoch 597/499\n",
      "----------\n",
      "Epoch597 Batch0 Loss: 14494.5732\n",
      "shuffling the dataset\n",
      "Epoch597 Batch2 Loss: 11828.3301\n",
      "Epoch597 Batch4 Loss: 10580.1982\n",
      "train Loss: 12431.9473\n",
      "Epoch 598/499\n",
      "----------\n",
      "Epoch598 Batch0 Loss: 17698.6680\n",
      "shuffling the dataset\n",
      "Epoch598 Batch2 Loss: 10153.9424\n",
      "Epoch598 Batch4 Loss: 9376.6816\n",
      "train Loss: 11816.1666\n",
      "Epoch 599/499\n",
      "----------\n",
      "Epoch599 Batch0 Loss: 10093.7539\n",
      "Epoch599 Batch2 Loss: 16557.8027\n",
      "shuffling the dataset\n",
      "Epoch599 Batch4 Loss: 11937.6514\n",
      "train Loss: 11203.6533\n",
      "Epoch 600/499\n",
      "----------\n",
      "Epoch600 Batch0 Loss: 9734.4463\n",
      "Epoch600 Batch2 Loss: 16875.9258\n",
      "shuffling the dataset\n",
      "Epoch600 Batch4 Loss: 9444.8506\n",
      "train Loss: 12588.9194\n",
      "Epoch 601/499\n",
      "----------\n",
      "Epoch601 Batch0 Loss: 16735.1074\n",
      "Epoch601 Batch2 Loss: 11675.6934\n",
      "shuffling the dataset\n",
      "Epoch601 Batch4 Loss: 12173.0410\n",
      "train Loss: 12404.0249\n",
      "Epoch 602/499\n",
      "----------\n",
      "Epoch602 Batch0 Loss: 17724.1934\n",
      "Epoch602 Batch2 Loss: 11558.8887\n",
      "shuffling the dataset\n",
      "Epoch602 Batch4 Loss: 8292.1660\n",
      "train Loss: 12173.4317\n",
      "Epoch 603/499\n",
      "----------\n",
      "Epoch603 Batch0 Loss: 9404.2373\n",
      "Epoch603 Batch2 Loss: 14765.0254\n",
      "shuffling the dataset\n",
      "Epoch603 Batch4 Loss: 14585.4629\n",
      "train Loss: 11930.7016\n",
      "Epoch 604/499\n",
      "----------\n",
      "Epoch604 Batch0 Loss: 12817.1475\n",
      "Epoch604 Batch2 Loss: 17107.1250\n",
      "shuffling the dataset\n",
      "Epoch604 Batch4 Loss: 7610.4570\n",
      "train Loss: 11258.8281\n",
      "Epoch 605/499\n",
      "----------\n",
      "Epoch605 Batch0 Loss: 9557.3076\n",
      "Epoch605 Batch2 Loss: 10083.6699\n",
      "shuffling the dataset\n",
      "Epoch605 Batch4 Loss: 17484.8887\n",
      "train Loss: 11981.7342\n",
      "Epoch 606/499\n",
      "----------\n",
      "Epoch606 Batch0 Loss: 9298.9219\n",
      "Epoch606 Batch2 Loss: 15485.6758\n",
      "shuffling the dataset\n",
      "Epoch606 Batch4 Loss: 13855.6582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 11446.6784\n",
      "Epoch 607/499\n",
      "----------\n",
      "Epoch607 Batch0 Loss: 15105.9316\n",
      "Epoch607 Batch2 Loss: 10706.2236\n",
      "Epoch607 Batch4 Loss: 12817.7402\n",
      "shuffling the dataset\n",
      "train Loss: 11977.9925\n",
      "Epoch 608/499\n",
      "----------\n",
      "Epoch608 Batch0 Loss: 12658.3203\n",
      "Epoch608 Batch2 Loss: 12255.4785\n",
      "Epoch608 Batch4 Loss: 12180.7832\n",
      "shuffling the dataset\n",
      "train Loss: 10912.0884\n",
      "Epoch 609/499\n",
      "----------\n",
      "Epoch609 Batch0 Loss: 9738.4385\n",
      "Epoch609 Batch2 Loss: 11498.6152\n",
      "Epoch609 Batch4 Loss: 14327.1016\n",
      "shuffling the dataset\n",
      "train Loss: 11279.3427\n",
      "Epoch 610/499\n",
      "----------\n",
      "Epoch610 Batch0 Loss: 9586.9414\n",
      "Epoch610 Batch2 Loss: 9511.2041\n",
      "Epoch610 Batch4 Loss: 15859.9307\n",
      "shuffling the dataset\n",
      "train Loss: 11105.9966\n",
      "Epoch 611/499\n",
      "----------\n",
      "Epoch611 Batch0 Loss: 14110.4766\n",
      "Epoch611 Batch2 Loss: 14357.6816\n",
      "Epoch611 Batch4 Loss: 10147.3086\n",
      "train Loss: 10640.1177\n",
      "Epoch 612/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch612 Batch0 Loss: 9306.7881\n",
      "Epoch612 Batch2 Loss: 14848.7705\n",
      "Epoch612 Batch4 Loss: 7734.8896\n",
      "train Loss: 9939.1536\n",
      "Epoch 613/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch613 Batch0 Loss: 15906.8105\n",
      "Epoch613 Batch2 Loss: 12209.5684\n",
      "Epoch613 Batch4 Loss: 9232.2168\n",
      "train Loss: 10888.4892\n",
      "Epoch 614/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch614 Batch0 Loss: 11588.3350\n",
      "Epoch614 Batch2 Loss: 10800.5723\n",
      "Epoch614 Batch4 Loss: 12246.2520\n",
      "train Loss: 11426.3550\n",
      "Epoch 615/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch615 Batch0 Loss: 9688.5068\n",
      "Epoch615 Batch2 Loss: 13328.8320\n",
      "Epoch615 Batch4 Loss: 9518.5303\n",
      "train Loss: 10895.8136\n",
      "Epoch 616/499\n",
      "----------\n",
      "Epoch616 Batch0 Loss: 12871.0127\n",
      "shuffling the dataset\n",
      "Epoch616 Batch2 Loss: 9968.4980\n",
      "Epoch616 Batch4 Loss: 10312.1543\n",
      "train Loss: 10777.9039\n",
      "Epoch 617/499\n",
      "----------\n",
      "Epoch617 Batch0 Loss: 17364.8594\n",
      "shuffling the dataset\n",
      "Epoch617 Batch2 Loss: 16380.6846\n",
      "Epoch617 Batch4 Loss: 14619.1387\n",
      "train Loss: 14275.9713\n",
      "Epoch 618/499\n",
      "----------\n",
      "Epoch618 Batch0 Loss: 13868.0547\n",
      "shuffling the dataset\n",
      "Epoch618 Batch2 Loss: 12751.7158\n",
      "Epoch618 Batch4 Loss: 20393.9727\n",
      "train Loss: 15958.7581\n",
      "Epoch 619/499\n",
      "----------\n",
      "Epoch619 Batch0 Loss: 23157.7734\n",
      "shuffling the dataset\n",
      "Epoch619 Batch2 Loss: 18572.5762\n",
      "Epoch619 Batch4 Loss: 17258.3125\n",
      "train Loss: 19965.9709\n",
      "Epoch 620/499\n",
      "----------\n",
      "Epoch620 Batch0 Loss: 22656.9062\n",
      "shuffling the dataset\n",
      "Epoch620 Batch2 Loss: 19549.8652\n",
      "Epoch620 Batch4 Loss: 29327.0254\n",
      "train Loss: 20206.5120\n",
      "Epoch 621/499\n",
      "----------\n",
      "Epoch621 Batch0 Loss: 21990.4297\n",
      "shuffling the dataset\n",
      "Epoch621 Batch2 Loss: 21352.8652\n",
      "Epoch621 Batch4 Loss: 19150.2402\n",
      "train Loss: 19790.9898\n",
      "Epoch 622/499\n",
      "----------\n",
      "Epoch622 Batch0 Loss: 14750.6045\n",
      "shuffling the dataset\n",
      "Epoch622 Batch2 Loss: 16003.6387\n",
      "Epoch622 Batch4 Loss: 19697.0078\n",
      "train Loss: 18116.8177\n",
      "Epoch 623/499\n",
      "----------\n",
      "Epoch623 Batch0 Loss: 14135.1777\n",
      "shuffling the dataset\n",
      "Epoch623 Batch2 Loss: 16987.8809\n",
      "Epoch623 Batch4 Loss: 15217.3291\n",
      "train Loss: 14065.1711\n",
      "Epoch 624/499\n",
      "----------\n",
      "Epoch624 Batch0 Loss: 12608.9258\n",
      "Epoch624 Batch2 Loss: 20015.6230\n",
      "shuffling the dataset\n",
      "Epoch624 Batch4 Loss: 14551.5020\n",
      "train Loss: 14283.0319\n",
      "Epoch 625/499\n",
      "----------\n",
      "Epoch625 Batch0 Loss: 13863.2334\n",
      "Epoch625 Batch2 Loss: 14448.2773\n",
      "shuffling the dataset\n",
      "Epoch625 Batch4 Loss: 18949.9648\n",
      "train Loss: 13760.9830\n",
      "Epoch 626/499\n",
      "----------\n",
      "Epoch626 Batch0 Loss: 13700.8145\n",
      "Epoch626 Batch2 Loss: 9871.2666\n",
      "shuffling the dataset\n",
      "Epoch626 Batch4 Loss: 14172.1221\n",
      "train Loss: 11772.4089\n",
      "Epoch 627/499\n",
      "----------\n",
      "Epoch627 Batch0 Loss: 12831.3945\n",
      "Epoch627 Batch2 Loss: 12348.3516\n",
      "shuffling the dataset\n",
      "Epoch627 Batch4 Loss: 12097.2705\n",
      "train Loss: 11957.4513\n",
      "Epoch 628/499\n",
      "----------\n",
      "Epoch628 Batch0 Loss: 16270.1504\n",
      "Epoch628 Batch2 Loss: 8393.4863\n",
      "shuffling the dataset\n",
      "Epoch628 Batch4 Loss: 12208.9023\n",
      "train Loss: 12573.7481\n",
      "Epoch 629/499\n",
      "----------\n",
      "Epoch629 Batch0 Loss: 11313.8193\n",
      "Epoch629 Batch2 Loss: 7445.1519\n",
      "shuffling the dataset\n",
      "Epoch629 Batch4 Loss: 16844.5059\n",
      "train Loss: 10272.0084\n",
      "Epoch 630/499\n",
      "----------\n",
      "Epoch630 Batch0 Loss: 10311.8027\n",
      "Epoch630 Batch2 Loss: 7667.3135\n",
      "shuffling the dataset\n",
      "Epoch630 Batch4 Loss: 8791.3799\n",
      "train Loss: 9549.4000\n",
      "Epoch 631/499\n",
      "----------\n",
      "Epoch631 Batch0 Loss: 12772.5811\n",
      "Epoch631 Batch2 Loss: 9930.9277\n",
      "shuffling the dataset\n",
      "Epoch631 Batch4 Loss: 17207.3359\n",
      "train Loss: 11754.2369\n",
      "Epoch 632/499\n",
      "----------\n",
      "Epoch632 Batch0 Loss: 7934.7095\n",
      "Epoch632 Batch2 Loss: 7550.4849\n",
      "Epoch632 Batch4 Loss: 11414.0342\n",
      "shuffling the dataset\n",
      "train Loss: 9327.2983\n",
      "Epoch 633/499\n",
      "----------\n",
      "Epoch633 Batch0 Loss: 14366.7227\n",
      "Epoch633 Batch2 Loss: 11352.2041\n",
      "Epoch633 Batch4 Loss: 12579.5469\n",
      "shuffling the dataset\n",
      "train Loss: 10531.8803\n",
      "Epoch 634/499\n",
      "----------\n",
      "Epoch634 Batch0 Loss: 7283.7227\n",
      "Epoch634 Batch2 Loss: 8541.4922\n",
      "Epoch634 Batch4 Loss: 12398.9902\n",
      "shuffling the dataset\n",
      "train Loss: 9121.1100\n",
      "Epoch 635/499\n",
      "----------\n",
      "Epoch635 Batch0 Loss: 8178.0171\n",
      "Epoch635 Batch2 Loss: 17862.9102\n",
      "Epoch635 Batch4 Loss: 4665.6855\n",
      "shuffling the dataset\n",
      "train Loss: 10126.3921\n",
      "Epoch 636/499\n",
      "----------\n",
      "Epoch636 Batch0 Loss: 6759.0601\n",
      "Epoch636 Batch2 Loss: 15270.7021\n",
      "Epoch636 Batch4 Loss: 12080.3887\n",
      "train Loss: 9937.9032\n",
      "Epoch 637/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch637 Batch0 Loss: 8921.7432\n",
      "Epoch637 Batch2 Loss: 7947.4116\n",
      "Epoch637 Batch4 Loss: 9576.1816\n",
      "train Loss: 9857.8340\n",
      "Epoch 638/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch638 Batch0 Loss: 9832.0459\n",
      "Epoch638 Batch2 Loss: 10935.5957\n",
      "Epoch638 Batch4 Loss: 10345.2051\n",
      "train Loss: 9666.0613\n",
      "Epoch 639/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch639 Batch0 Loss: 8344.7959\n",
      "Epoch639 Batch2 Loss: 11654.7617\n",
      "Epoch639 Batch4 Loss: 8733.8330\n",
      "train Loss: 9312.8272\n",
      "Epoch 640/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch640 Batch0 Loss: 12334.3662\n",
      "Epoch640 Batch2 Loss: 9264.1689\n",
      "Epoch640 Batch4 Loss: 9806.9121\n",
      "train Loss: 10002.2127\n",
      "Epoch 641/499\n",
      "----------\n",
      "Epoch641 Batch0 Loss: 10127.1572\n",
      "shuffling the dataset\n",
      "Epoch641 Batch2 Loss: 5276.5703\n",
      "Epoch641 Batch4 Loss: 12610.5049\n",
      "train Loss: 10045.8403\n",
      "Epoch 642/499\n",
      "----------\n",
      "Epoch642 Batch0 Loss: 11572.6748\n",
      "shuffling the dataset\n",
      "Epoch642 Batch2 Loss: 9427.3096\n",
      "Epoch642 Batch4 Loss: 12124.2949\n",
      "train Loss: 10114.5364\n",
      "Epoch 643/499\n",
      "----------\n",
      "Epoch643 Batch0 Loss: 11961.1475\n",
      "shuffling the dataset\n",
      "Epoch643 Batch2 Loss: 6420.6909\n",
      "Epoch643 Batch4 Loss: 14203.7139\n",
      "train Loss: 10379.3643\n",
      "Epoch 644/499\n",
      "----------\n",
      "Epoch644 Batch0 Loss: 10173.7979\n",
      "shuffling the dataset\n",
      "Epoch644 Batch2 Loss: 13281.3086\n",
      "Epoch644 Batch4 Loss: 7073.1797\n",
      "train Loss: 10267.5881\n",
      "Epoch 645/499\n",
      "----------\n",
      "Epoch645 Batch0 Loss: 7365.7388\n",
      "shuffling the dataset\n",
      "Epoch645 Batch2 Loss: 8001.9277\n",
      "Epoch645 Batch4 Loss: 15745.2441\n",
      "train Loss: 10251.3976\n",
      "Epoch 646/499\n",
      "----------\n",
      "Epoch646 Batch0 Loss: 7531.7817\n",
      "shuffling the dataset\n",
      "Epoch646 Batch2 Loss: 10398.1250\n",
      "Epoch646 Batch4 Loss: 9395.1357\n",
      "train Loss: 10336.5256\n",
      "Epoch 647/499\n",
      "----------\n",
      "Epoch647 Batch0 Loss: 9665.2891\n",
      "shuffling the dataset\n",
      "Epoch647 Batch2 Loss: 10886.3271\n",
      "Epoch647 Batch4 Loss: 11623.4209\n",
      "train Loss: 10526.1523\n",
      "Epoch 648/499\n",
      "----------\n",
      "Epoch648 Batch0 Loss: 8914.6875\n",
      "shuffling the dataset\n",
      "Epoch648 Batch2 Loss: 9645.9531\n",
      "Epoch648 Batch4 Loss: 10333.9307\n",
      "train Loss: 8553.9284\n",
      "Epoch 649/499\n",
      "----------\n",
      "Epoch649 Batch0 Loss: 13970.8984\n",
      "Epoch649 Batch2 Loss: 8991.8047\n",
      "shuffling the dataset\n",
      "Epoch649 Batch4 Loss: 10468.2734\n",
      "train Loss: 10689.9964\n",
      "Epoch 650/499\n",
      "----------\n",
      "Epoch650 Batch0 Loss: 9124.3975\n",
      "Epoch650 Batch2 Loss: 8223.8945\n",
      "shuffling the dataset\n",
      "Epoch650 Batch4 Loss: 10447.6172\n",
      "train Loss: 9000.8558\n",
      "Epoch 651/499\n",
      "----------\n",
      "Epoch651 Batch0 Loss: 12533.6758\n",
      "Epoch651 Batch2 Loss: 11309.6445\n",
      "shuffling the dataset\n",
      "Epoch651 Batch4 Loss: 13261.2402\n",
      "train Loss: 10345.7802\n",
      "Epoch 652/499\n",
      "----------\n",
      "Epoch652 Batch0 Loss: 9946.5898\n",
      "Epoch652 Batch2 Loss: 6978.3291\n",
      "shuffling the dataset\n",
      "Epoch652 Batch4 Loss: 9749.6016\n",
      "train Loss: 9178.4948\n",
      "Epoch 653/499\n",
      "----------\n",
      "Epoch653 Batch0 Loss: 9439.5469\n",
      "Epoch653 Batch2 Loss: 14660.4619\n",
      "shuffling the dataset\n",
      "Epoch653 Batch4 Loss: 11343.5303\n",
      "train Loss: 9889.1590\n",
      "Epoch 654/499\n",
      "----------\n",
      "Epoch654 Batch0 Loss: 9152.8809\n",
      "Epoch654 Batch2 Loss: 9773.4121\n",
      "shuffling the dataset\n",
      "Epoch654 Batch4 Loss: 7924.5879\n",
      "train Loss: 8823.7712\n",
      "Epoch 655/499\n",
      "----------\n",
      "Epoch655 Batch0 Loss: 11650.8545\n",
      "Epoch655 Batch2 Loss: 18276.8516\n",
      "shuffling the dataset\n",
      "Epoch655 Batch4 Loss: 6122.9414\n",
      "train Loss: 10937.7095\n",
      "Epoch 656/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch656 Batch0 Loss: 6246.4346\n",
      "Epoch656 Batch2 Loss: 10850.4658\n",
      "shuffling the dataset\n",
      "Epoch656 Batch4 Loss: 11612.1914\n",
      "train Loss: 9445.6611\n",
      "Epoch 657/499\n",
      "----------\n",
      "Epoch657 Batch0 Loss: 10370.4863\n",
      "Epoch657 Batch2 Loss: 8282.4102\n",
      "Epoch657 Batch4 Loss: 9181.8555\n",
      "shuffling the dataset\n",
      "train Loss: 8800.0930\n",
      "Epoch 658/499\n",
      "----------\n",
      "Epoch658 Batch0 Loss: 13994.4736\n",
      "Epoch658 Batch2 Loss: 9232.5283\n",
      "Epoch658 Batch4 Loss: 7377.8745\n",
      "shuffling the dataset\n",
      "train Loss: 9889.3301\n",
      "Epoch 659/499\n",
      "----------\n",
      "Epoch659 Batch0 Loss: 5958.0713\n",
      "Epoch659 Batch2 Loss: 7716.4067\n",
      "Epoch659 Batch4 Loss: 10799.3477\n",
      "shuffling the dataset\n",
      "train Loss: 9240.3866\n",
      "Epoch 660/499\n",
      "----------\n",
      "Epoch660 Batch0 Loss: 16178.2725\n",
      "Epoch660 Batch2 Loss: 6497.5811\n",
      "Epoch660 Batch4 Loss: 14184.0254\n",
      "shuffling the dataset\n",
      "train Loss: 9446.2759\n",
      "Epoch 661/499\n",
      "----------\n",
      "Epoch661 Batch0 Loss: 6836.8125\n",
      "Epoch661 Batch2 Loss: 9950.1250\n",
      "Epoch661 Batch4 Loss: 11626.1299\n",
      "train Loss: 9858.4358\n",
      "Epoch 662/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch662 Batch0 Loss: 9366.3564\n",
      "Epoch662 Batch2 Loss: 8791.2773\n",
      "Epoch662 Batch4 Loss: 13907.1084\n",
      "train Loss: 9746.5936\n",
      "Epoch 663/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch663 Batch0 Loss: 8051.1123\n",
      "Epoch663 Batch2 Loss: 8742.6201\n",
      "Epoch663 Batch4 Loss: 11500.5312\n",
      "train Loss: 9872.5500\n",
      "Epoch 664/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch664 Batch0 Loss: 8062.1333\n",
      "Epoch664 Batch2 Loss: 11977.2002\n",
      "Epoch664 Batch4 Loss: 8411.3691\n",
      "train Loss: 9268.3150\n",
      "Epoch 665/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch665 Batch0 Loss: 11697.7568\n",
      "Epoch665 Batch2 Loss: 13842.4336\n",
      "Epoch665 Batch4 Loss: 12152.4355\n",
      "train Loss: 10628.7545\n",
      "Epoch 666/499\n",
      "----------\n",
      "Epoch666 Batch0 Loss: 8691.2822\n",
      "shuffling the dataset\n",
      "Epoch666 Batch2 Loss: 9137.2344\n",
      "Epoch666 Batch4 Loss: 15313.6650\n",
      "train Loss: 11487.9953\n",
      "Epoch 667/499\n",
      "----------\n",
      "Epoch667 Batch0 Loss: 11918.3096\n",
      "shuffling the dataset\n",
      "Epoch667 Batch2 Loss: 11435.1777\n",
      "Epoch667 Batch4 Loss: 14317.3174\n",
      "train Loss: 12469.6262\n",
      "Epoch 668/499\n",
      "----------\n",
      "Epoch668 Batch0 Loss: 11650.7607\n",
      "shuffling the dataset\n",
      "Epoch668 Batch2 Loss: 13678.5771\n",
      "Epoch668 Batch4 Loss: 14479.8408\n",
      "train Loss: 12665.5870\n",
      "Epoch 669/499\n",
      "----------\n",
      "Epoch669 Batch0 Loss: 11023.4277\n",
      "shuffling the dataset\n",
      "Epoch669 Batch2 Loss: 15964.0293\n",
      "Epoch669 Batch4 Loss: 9611.4756\n",
      "train Loss: 11777.6797\n",
      "Epoch 670/499\n",
      "----------\n",
      "Epoch670 Batch0 Loss: 17586.3594\n",
      "shuffling the dataset\n",
      "Epoch670 Batch2 Loss: 11674.1455\n",
      "Epoch670 Batch4 Loss: 10898.4609\n",
      "train Loss: 12785.3727\n",
      "Epoch 671/499\n",
      "----------\n",
      "Epoch671 Batch0 Loss: 10928.3066\n",
      "shuffling the dataset\n",
      "Epoch671 Batch2 Loss: 18541.3184\n",
      "Epoch671 Batch4 Loss: 13406.0156\n",
      "train Loss: 11751.6927\n",
      "Epoch 672/499\n",
      "----------\n",
      "Epoch672 Batch0 Loss: 12474.4990\n",
      "shuffling the dataset\n",
      "Epoch672 Batch2 Loss: 10259.0342\n",
      "Epoch672 Batch4 Loss: 8037.8325\n",
      "train Loss: 10444.5487\n",
      "Epoch 673/499\n",
      "----------\n",
      "Epoch673 Batch0 Loss: 11748.6709\n",
      "shuffling the dataset\n",
      "Epoch673 Batch2 Loss: 10019.7402\n",
      "Epoch673 Batch4 Loss: 9227.1094\n",
      "train Loss: 10246.7862\n",
      "Epoch 674/499\n",
      "----------\n",
      "Epoch674 Batch0 Loss: 10176.0586\n",
      "Epoch674 Batch2 Loss: 12965.2539\n",
      "shuffling the dataset\n",
      "Epoch674 Batch4 Loss: 6937.6455\n",
      "train Loss: 9434.7391\n",
      "Epoch 675/499\n",
      "----------\n",
      "Epoch675 Batch0 Loss: 11308.1523\n",
      "Epoch675 Batch2 Loss: 8978.0098\n",
      "shuffling the dataset\n",
      "Epoch675 Batch4 Loss: 9023.7080\n",
      "train Loss: 9604.7267\n",
      "Epoch 676/499\n",
      "----------\n",
      "Epoch676 Batch0 Loss: 8569.5850\n",
      "Epoch676 Batch2 Loss: 9458.9385\n",
      "shuffling the dataset\n",
      "Epoch676 Batch4 Loss: 9900.5547\n",
      "train Loss: 10007.6799\n",
      "Epoch 677/499\n",
      "----------\n",
      "Epoch677 Batch0 Loss: 12368.3369\n",
      "Epoch677 Batch2 Loss: 11035.0146\n",
      "shuffling the dataset\n",
      "Epoch677 Batch4 Loss: 9542.3623\n",
      "train Loss: 9919.6059\n",
      "Epoch 678/499\n",
      "----------\n",
      "Epoch678 Batch0 Loss: 12407.3008\n",
      "Epoch678 Batch2 Loss: 8826.1621\n",
      "shuffling the dataset\n",
      "Epoch678 Batch4 Loss: 9893.4209\n",
      "train Loss: 11364.6091\n",
      "Epoch 679/499\n",
      "----------\n",
      "Epoch679 Batch0 Loss: 10241.8984\n",
      "Epoch679 Batch2 Loss: 14159.6631\n",
      "shuffling the dataset\n",
      "Epoch679 Batch4 Loss: 11543.6416\n",
      "train Loss: 11358.3288\n",
      "Epoch 680/499\n",
      "----------\n",
      "Epoch680 Batch0 Loss: 14123.6514\n",
      "Epoch680 Batch2 Loss: 9886.9395\n",
      "shuffling the dataset\n",
      "Epoch680 Batch4 Loss: 8317.0342\n",
      "train Loss: 10920.1473\n",
      "Epoch 681/499\n",
      "----------\n",
      "Epoch681 Batch0 Loss: 14699.7754\n",
      "Epoch681 Batch2 Loss: 11193.8555\n",
      "shuffling the dataset\n",
      "Epoch681 Batch4 Loss: 10841.2383\n",
      "train Loss: 12488.7083\n",
      "Epoch 682/499\n",
      "----------\n",
      "Epoch682 Batch0 Loss: 9436.0400\n",
      "Epoch682 Batch2 Loss: 9656.5039\n",
      "Epoch682 Batch4 Loss: 11385.3496\n",
      "shuffling the dataset\n",
      "train Loss: 10745.5330\n",
      "Epoch 683/499\n",
      "----------\n",
      "Epoch683 Batch0 Loss: 9300.6709\n",
      "Epoch683 Batch2 Loss: 11739.1387\n",
      "Epoch683 Batch4 Loss: 11694.3096\n",
      "shuffling the dataset\n",
      "train Loss: 11850.1441\n",
      "Epoch 684/499\n",
      "----------\n",
      "Epoch684 Batch0 Loss: 13657.4209\n",
      "Epoch684 Batch2 Loss: 10639.4199\n",
      "Epoch684 Batch4 Loss: 5006.3467\n",
      "shuffling the dataset\n",
      "train Loss: 10664.3269\n",
      "Epoch 685/499\n",
      "----------\n",
      "Epoch685 Batch0 Loss: 12604.9082\n",
      "Epoch685 Batch2 Loss: 8822.1465\n",
      "Epoch685 Batch4 Loss: 11626.3271\n",
      "shuffling the dataset\n",
      "train Loss: 10989.7491\n",
      "Epoch 686/499\n",
      "----------\n",
      "Epoch686 Batch0 Loss: 15022.2109\n",
      "Epoch686 Batch2 Loss: 7507.6553\n",
      "Epoch686 Batch4 Loss: 11663.3428\n",
      "train Loss: 10921.7369\n",
      "Epoch 687/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch687 Batch0 Loss: 11782.1357\n",
      "Epoch687 Batch2 Loss: 16088.4219\n",
      "Epoch687 Batch4 Loss: 12520.5361\n",
      "train Loss: 11843.6017\n",
      "Epoch 688/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch688 Batch0 Loss: 9704.0127\n",
      "Epoch688 Batch2 Loss: 9193.7266\n",
      "Epoch688 Batch4 Loss: 15528.9697\n",
      "train Loss: 11989.5028\n",
      "Epoch 689/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch689 Batch0 Loss: 8039.4473\n",
      "Epoch689 Batch2 Loss: 10810.9961\n",
      "Epoch689 Batch4 Loss: 10071.3604\n",
      "train Loss: 9868.9051\n",
      "Epoch 690/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch690 Batch0 Loss: 11489.9941\n",
      "Epoch690 Batch2 Loss: 10898.4102\n",
      "Epoch690 Batch4 Loss: 12633.5791\n",
      "train Loss: 10828.3984\n",
      "Epoch 691/499\n",
      "----------\n",
      "Epoch691 Batch0 Loss: 12123.6318\n",
      "shuffling the dataset\n",
      "Epoch691 Batch2 Loss: 17803.3867\n",
      "Epoch691 Batch4 Loss: 13869.4648\n",
      "train Loss: 11375.5700\n",
      "Epoch 692/499\n",
      "----------\n",
      "Epoch692 Batch0 Loss: 7101.1743\n",
      "shuffling the dataset\n",
      "Epoch692 Batch2 Loss: 8882.3613\n",
      "Epoch692 Batch4 Loss: 9361.4697\n",
      "train Loss: 9343.0395\n",
      "Epoch 693/499\n",
      "----------\n",
      "Epoch693 Batch0 Loss: 12036.1475\n",
      "shuffling the dataset\n",
      "Epoch693 Batch2 Loss: 13408.2812\n",
      "Epoch693 Batch4 Loss: 7769.2080\n",
      "train Loss: 10828.8239\n",
      "Epoch 694/499\n",
      "----------\n",
      "Epoch694 Batch0 Loss: 6512.6987\n",
      "shuffling the dataset\n",
      "Epoch694 Batch2 Loss: 6514.4053\n",
      "Epoch694 Batch4 Loss: 13999.7051\n",
      "train Loss: 9468.3340\n",
      "Epoch 695/499\n",
      "----------\n",
      "Epoch695 Batch0 Loss: 10293.0361\n",
      "shuffling the dataset\n",
      "Epoch695 Batch2 Loss: 7453.7910\n",
      "Epoch695 Batch4 Loss: 7841.2095\n",
      "train Loss: 9275.2345\n",
      "Epoch 696/499\n",
      "----------\n",
      "Epoch696 Batch0 Loss: 7930.4224\n",
      "shuffling the dataset\n",
      "Epoch696 Batch2 Loss: 9163.6387\n",
      "Epoch696 Batch4 Loss: 12398.6025\n",
      "train Loss: 9200.5476\n",
      "Epoch 697/499\n",
      "----------\n",
      "Epoch697 Batch0 Loss: 7086.5615\n",
      "shuffling the dataset\n",
      "Epoch697 Batch2 Loss: 9524.6211\n",
      "Epoch697 Batch4 Loss: 8123.3359\n",
      "train Loss: 8893.0633\n",
      "Epoch 698/499\n",
      "----------\n",
      "Epoch698 Batch0 Loss: 6615.1260\n",
      "shuffling the dataset\n",
      "Epoch698 Batch2 Loss: 9196.9980\n",
      "Epoch698 Batch4 Loss: 10051.6396\n",
      "train Loss: 8397.8139\n",
      "Epoch 699/499\n",
      "----------\n",
      "Epoch699 Batch0 Loss: 6685.5254\n",
      "Epoch699 Batch2 Loss: 8231.1924\n",
      "shuffling the dataset\n",
      "Epoch699 Batch4 Loss: 5366.7324\n",
      "train Loss: 7528.5362\n",
      "Epoch 700/499\n",
      "----------\n",
      "Epoch700 Batch0 Loss: 7428.4390\n",
      "Epoch700 Batch2 Loss: 8963.2305\n",
      "shuffling the dataset\n",
      "Epoch700 Batch4 Loss: 7315.2139\n",
      "train Loss: 7408.8660\n",
      "Epoch 701/499\n",
      "----------\n",
      "Epoch701 Batch0 Loss: 8015.8149\n",
      "Epoch701 Batch2 Loss: 9971.5977\n",
      "shuffling the dataset\n",
      "Epoch701 Batch4 Loss: 5516.6157\n",
      "train Loss: 7180.4207\n",
      "Epoch 702/499\n",
      "----------\n",
      "Epoch702 Batch0 Loss: 9421.3008\n",
      "Epoch702 Batch2 Loss: 7291.1211\n",
      "shuffling the dataset\n",
      "Epoch702 Batch4 Loss: 7318.1548\n",
      "train Loss: 6960.4834\n",
      "Epoch 703/499\n",
      "----------\n",
      "Epoch703 Batch0 Loss: 9025.5273\n",
      "Epoch703 Batch2 Loss: 9427.5439\n",
      "shuffling the dataset\n",
      "Epoch703 Batch4 Loss: 6352.6875\n",
      "train Loss: 6365.2823\n",
      "Epoch 704/499\n",
      "----------\n",
      "Epoch704 Batch0 Loss: 9469.5840\n",
      "Epoch704 Batch2 Loss: 8083.4756\n",
      "shuffling the dataset\n",
      "Epoch704 Batch4 Loss: 5434.8721\n",
      "train Loss: 6759.6041\n",
      "Epoch 705/499\n",
      "----------\n",
      "Epoch705 Batch0 Loss: 8201.7969\n",
      "Epoch705 Batch2 Loss: 7452.7363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling the dataset\n",
      "Epoch705 Batch4 Loss: 7270.1172\n",
      "train Loss: 6638.0120\n",
      "Epoch 706/499\n",
      "----------\n",
      "Epoch706 Batch0 Loss: 4600.6460\n",
      "Epoch706 Batch2 Loss: 10230.2832\n",
      "shuffling the dataset\n",
      "Epoch706 Batch4 Loss: 7021.0098\n",
      "train Loss: 6502.9798\n",
      "Epoch 707/499\n",
      "----------\n",
      "Epoch707 Batch0 Loss: 4907.7246\n",
      "Epoch707 Batch2 Loss: 7152.8545\n",
      "Epoch707 Batch4 Loss: 5622.7944\n",
      "shuffling the dataset\n",
      "train Loss: 5820.6613\n",
      "Epoch 708/499\n",
      "----------\n",
      "Epoch708 Batch0 Loss: 7380.8081\n",
      "Epoch708 Batch2 Loss: 7737.0366\n",
      "Epoch708 Batch4 Loss: 7875.7900\n",
      "shuffling the dataset\n",
      "train Loss: 6517.7902\n",
      "Epoch 709/499\n",
      "----------\n",
      "Epoch709 Batch0 Loss: 9564.9902\n",
      "Epoch709 Batch2 Loss: 4486.6113\n",
      "Epoch709 Batch4 Loss: 5620.5059\n",
      "shuffling the dataset\n",
      "train Loss: 6100.5180\n",
      "Epoch 710/499\n",
      "----------\n",
      "Epoch710 Batch0 Loss: 3892.2122\n",
      "Epoch710 Batch2 Loss: 4199.7314\n",
      "Epoch710 Batch4 Loss: 5145.4521\n",
      "shuffling the dataset\n",
      "train Loss: 6211.7681\n",
      "Epoch 711/499\n",
      "----------\n",
      "Epoch711 Batch0 Loss: 6750.0991\n",
      "Epoch711 Batch2 Loss: 4796.5757\n",
      "Epoch711 Batch4 Loss: 6602.4375\n",
      "train Loss: 6327.8552\n",
      "Epoch 712/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch712 Batch0 Loss: 5224.6919\n",
      "Epoch712 Batch2 Loss: 6643.7642\n",
      "Epoch712 Batch4 Loss: 7282.3076\n",
      "train Loss: 6306.6774\n",
      "Epoch 713/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch713 Batch0 Loss: 4972.0596\n",
      "Epoch713 Batch2 Loss: 4952.3267\n",
      "Epoch713 Batch4 Loss: 6091.1060\n",
      "train Loss: 6180.6614\n",
      "Epoch 714/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch714 Batch0 Loss: 7475.3716\n",
      "Epoch714 Batch2 Loss: 7031.2056\n",
      "Epoch714 Batch4 Loss: 6421.6812\n",
      "train Loss: 6606.4722\n",
      "Epoch 715/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch715 Batch0 Loss: 8925.0771\n",
      "Epoch715 Batch2 Loss: 7598.8672\n",
      "Epoch715 Batch4 Loss: 3797.5081\n",
      "train Loss: 6956.0949\n",
      "Epoch 716/499\n",
      "----------\n",
      "Epoch716 Batch0 Loss: 6714.0981\n",
      "shuffling the dataset\n",
      "Epoch716 Batch2 Loss: 3713.3127\n",
      "Epoch716 Batch4 Loss: 6145.7705\n",
      "train Loss: 7034.1900\n",
      "Epoch 717/499\n",
      "----------\n",
      "Epoch717 Batch0 Loss: 6017.9912\n",
      "shuffling the dataset\n",
      "Epoch717 Batch2 Loss: 8312.5498\n",
      "Epoch717 Batch4 Loss: 7144.8955\n",
      "train Loss: 6893.5182\n",
      "Epoch 718/499\n",
      "----------\n",
      "Epoch718 Batch0 Loss: 10005.7021\n",
      "shuffling the dataset\n",
      "Epoch718 Batch2 Loss: 8185.3008\n",
      "Epoch718 Batch4 Loss: 8947.7207\n",
      "train Loss: 7980.7827\n",
      "Epoch 719/499\n",
      "----------\n",
      "Epoch719 Batch0 Loss: 9213.6973\n",
      "shuffling the dataset\n",
      "Epoch719 Batch2 Loss: 10760.3555\n",
      "Epoch719 Batch4 Loss: 6777.8857\n",
      "train Loss: 7540.2716\n",
      "Epoch 720/499\n",
      "----------\n",
      "Epoch720 Batch0 Loss: 8736.3818\n",
      "shuffling the dataset\n",
      "Epoch720 Batch2 Loss: 8408.1357\n",
      "Epoch720 Batch4 Loss: 8343.0850\n",
      "train Loss: 7271.8380\n",
      "Epoch 721/499\n",
      "----------\n",
      "Epoch721 Batch0 Loss: 2715.3826\n",
      "shuffling the dataset\n",
      "Epoch721 Batch2 Loss: 10779.5967\n",
      "Epoch721 Batch4 Loss: 5746.7329\n",
      "train Loss: 7867.8154\n",
      "Epoch 722/499\n",
      "----------\n",
      "Epoch722 Batch0 Loss: 10383.0898\n",
      "shuffling the dataset\n",
      "Epoch722 Batch2 Loss: 7550.5142\n",
      "Epoch722 Batch4 Loss: 7444.7476\n",
      "train Loss: 8928.7570\n",
      "Epoch 723/499\n",
      "----------\n",
      "Epoch723 Batch0 Loss: 10831.8799\n",
      "shuffling the dataset\n",
      "Epoch723 Batch2 Loss: 13841.3789\n",
      "Epoch723 Batch4 Loss: 9258.9033\n",
      "train Loss: 9398.3828\n",
      "Epoch 724/499\n",
      "----------\n",
      "Epoch724 Batch0 Loss: 11609.7920\n",
      "Epoch724 Batch2 Loss: 7503.2939\n",
      "shuffling the dataset\n",
      "Epoch724 Batch4 Loss: 9713.4131\n",
      "train Loss: 9729.8259\n",
      "Epoch 725/499\n",
      "----------\n",
      "Epoch725 Batch0 Loss: 7550.3008\n",
      "Epoch725 Batch2 Loss: 12271.8018\n",
      "shuffling the dataset\n",
      "Epoch725 Batch4 Loss: 11302.1406\n",
      "train Loss: 10103.6255\n",
      "Epoch 726/499\n",
      "----------\n",
      "Epoch726 Batch0 Loss: 8705.4033\n",
      "Epoch726 Batch2 Loss: 11372.5020\n",
      "shuffling the dataset\n",
      "Epoch726 Batch4 Loss: 6985.1802\n",
      "train Loss: 9073.1616\n",
      "Epoch 727/499\n",
      "----------\n",
      "Epoch727 Batch0 Loss: 10089.6016\n",
      "Epoch727 Batch2 Loss: 13102.9385\n",
      "shuffling the dataset\n",
      "Epoch727 Batch4 Loss: 12130.7236\n",
      "train Loss: 10458.4578\n",
      "Epoch 728/499\n",
      "----------\n",
      "Epoch728 Batch0 Loss: 9700.8594\n",
      "Epoch728 Batch2 Loss: 10598.5186\n",
      "shuffling the dataset\n",
      "Epoch728 Batch4 Loss: 15920.5527\n",
      "train Loss: 10177.1531\n",
      "Epoch 729/499\n",
      "----------\n",
      "Epoch729 Batch0 Loss: 6186.1055\n",
      "Epoch729 Batch2 Loss: 11099.3389\n",
      "shuffling the dataset\n",
      "Epoch729 Batch4 Loss: 10706.4043\n",
      "train Loss: 9727.2789\n",
      "Epoch 730/499\n",
      "----------\n",
      "Epoch730 Batch0 Loss: 8856.9258\n",
      "Epoch730 Batch2 Loss: 9060.1934\n",
      "shuffling the dataset\n",
      "Epoch730 Batch4 Loss: 9190.9512\n",
      "train Loss: 10820.2641\n",
      "Epoch 731/499\n",
      "----------\n",
      "Epoch731 Batch0 Loss: 7215.6309\n",
      "Epoch731 Batch2 Loss: 10319.3467\n",
      "shuffling the dataset\n",
      "Epoch731 Batch4 Loss: 13096.7734\n",
      "train Loss: 10689.5391\n",
      "Epoch 732/499\n",
      "----------\n",
      "Epoch732 Batch0 Loss: 8625.0049\n",
      "Epoch732 Batch2 Loss: 10914.0850\n",
      "Epoch732 Batch4 Loss: 11611.9307\n",
      "shuffling the dataset\n",
      "train Loss: 10462.3028\n",
      "Epoch 733/499\n",
      "----------\n",
      "Epoch733 Batch0 Loss: 13859.9814\n",
      "Epoch733 Batch2 Loss: 9451.6484\n",
      "Epoch733 Batch4 Loss: 11137.7920\n",
      "shuffling the dataset\n",
      "train Loss: 10570.8197\n",
      "Epoch 734/499\n",
      "----------\n",
      "Epoch734 Batch0 Loss: 7765.1323\n",
      "Epoch734 Batch2 Loss: 10640.6299\n",
      "Epoch734 Batch4 Loss: 14018.5244\n",
      "shuffling the dataset\n",
      "train Loss: 11134.3627\n",
      "Epoch 735/499\n",
      "----------\n",
      "Epoch735 Batch0 Loss: 10027.5254\n",
      "Epoch735 Batch2 Loss: 12616.3174\n",
      "Epoch735 Batch4 Loss: 7012.5205\n",
      "shuffling the dataset\n",
      "train Loss: 9804.4881\n",
      "Epoch 736/499\n",
      "----------\n",
      "Epoch736 Batch0 Loss: 9305.0010\n",
      "Epoch736 Batch2 Loss: 11645.5107\n",
      "Epoch736 Batch4 Loss: 6194.7856\n",
      "train Loss: 9547.7470\n",
      "Epoch 737/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch737 Batch0 Loss: 10276.8857\n",
      "Epoch737 Batch2 Loss: 9519.4053\n",
      "Epoch737 Batch4 Loss: 8806.9531\n",
      "train Loss: 8652.4870\n",
      "Epoch 738/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch738 Batch0 Loss: 9868.1221\n",
      "Epoch738 Batch2 Loss: 10044.6133\n",
      "Epoch738 Batch4 Loss: 7584.2573\n",
      "train Loss: 9118.3408\n",
      "Epoch 739/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch739 Batch0 Loss: 5879.2285\n",
      "Epoch739 Batch2 Loss: 7623.6216\n",
      "Epoch739 Batch4 Loss: 9284.2031\n",
      "train Loss: 7609.9870\n",
      "Epoch 740/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch740 Batch0 Loss: 7798.4922\n",
      "Epoch740 Batch2 Loss: 5694.8501\n",
      "Epoch740 Batch4 Loss: 8706.8193\n",
      "train Loss: 7631.5852\n",
      "Epoch 741/499\n",
      "----------\n",
      "Epoch741 Batch0 Loss: 7355.9932\n",
      "shuffling the dataset\n",
      "Epoch741 Batch2 Loss: 7068.0151\n",
      "Epoch741 Batch4 Loss: 8056.5483\n",
      "train Loss: 7456.2398\n",
      "Epoch 742/499\n",
      "----------\n",
      "Epoch742 Batch0 Loss: 5398.8428\n",
      "shuffling the dataset\n",
      "Epoch742 Batch2 Loss: 9372.0723\n",
      "Epoch742 Batch4 Loss: 10377.1689\n",
      "train Loss: 7403.3075\n",
      "Epoch 743/499\n",
      "----------\n",
      "Epoch743 Batch0 Loss: 8159.4941\n",
      "shuffling the dataset\n",
      "Epoch743 Batch2 Loss: 8582.1758\n",
      "Epoch743 Batch4 Loss: 5825.6787\n",
      "train Loss: 6897.2398\n",
      "Epoch 744/499\n",
      "----------\n",
      "Epoch744 Batch0 Loss: 10980.6318\n",
      "shuffling the dataset\n",
      "Epoch744 Batch2 Loss: 8197.5898\n",
      "Epoch744 Batch4 Loss: 10717.3467\n",
      "train Loss: 8598.5825\n",
      "Epoch 745/499\n",
      "----------\n",
      "Epoch745 Batch0 Loss: 8511.3672\n",
      "shuffling the dataset\n",
      "Epoch745 Batch2 Loss: 5979.0615\n",
      "Epoch745 Batch4 Loss: 9402.2012\n",
      "train Loss: 8050.9596\n",
      "Epoch 746/499\n",
      "----------\n",
      "Epoch746 Batch0 Loss: 9736.4443\n",
      "shuffling the dataset\n",
      "Epoch746 Batch2 Loss: 7308.8833\n",
      "Epoch746 Batch4 Loss: 7623.5225\n",
      "train Loss: 9076.4129\n",
      "Epoch 747/499\n",
      "----------\n",
      "Epoch747 Batch0 Loss: 8061.7422\n",
      "shuffling the dataset\n",
      "Epoch747 Batch2 Loss: 10048.7852\n",
      "Epoch747 Batch4 Loss: 11062.7910\n",
      "train Loss: 9772.4595\n",
      "Epoch 748/499\n",
      "----------\n",
      "Epoch748 Batch0 Loss: 13687.9238\n",
      "shuffling the dataset\n",
      "Epoch748 Batch2 Loss: 7858.7192\n",
      "Epoch748 Batch4 Loss: 16389.3887\n",
      "train Loss: 10559.5991\n",
      "Epoch 749/499\n",
      "----------\n",
      "Epoch749 Batch0 Loss: 10376.8691\n",
      "Epoch749 Batch2 Loss: 12237.4580\n",
      "shuffling the dataset\n",
      "Epoch749 Batch4 Loss: 8503.1660\n",
      "train Loss: 10654.7989\n",
      "Epoch 750/499\n",
      "----------\n",
      "Epoch750 Batch0 Loss: 16677.0059\n",
      "Epoch750 Batch2 Loss: 11742.0518\n",
      "shuffling the dataset\n",
      "Epoch750 Batch4 Loss: 5625.9668\n",
      "train Loss: 11649.4230\n",
      "Epoch 751/499\n",
      "----------\n",
      "Epoch751 Batch0 Loss: 11077.7852\n",
      "Epoch751 Batch2 Loss: 13890.2041\n",
      "shuffling the dataset\n",
      "Epoch751 Batch4 Loss: 13882.2080\n",
      "train Loss: 12487.7952\n",
      "Epoch 752/499\n",
      "----------\n",
      "Epoch752 Batch0 Loss: 15109.3652\n",
      "Epoch752 Batch2 Loss: 15797.2500\n",
      "shuffling the dataset\n",
      "Epoch752 Batch4 Loss: 12689.5234\n",
      "train Loss: 12384.6392\n",
      "Epoch 753/499\n",
      "----------\n",
      "Epoch753 Batch0 Loss: 8574.7510\n",
      "Epoch753 Batch2 Loss: 12475.4854\n",
      "shuffling the dataset\n",
      "Epoch753 Batch4 Loss: 13582.4814\n",
      "train Loss: 9613.4448\n",
      "Epoch 754/499\n",
      "----------\n",
      "Epoch754 Batch0 Loss: 10316.3926\n",
      "Epoch754 Batch2 Loss: 13093.9482\n",
      "shuffling the dataset\n",
      "Epoch754 Batch4 Loss: 10179.0176\n",
      "train Loss: 10402.8386\n",
      "Epoch 755/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch755 Batch0 Loss: 8668.6592\n",
      "Epoch755 Batch2 Loss: 9391.1426\n",
      "shuffling the dataset\n",
      "Epoch755 Batch4 Loss: 8636.6680\n",
      "train Loss: 8771.2872\n",
      "Epoch 756/499\n",
      "----------\n",
      "Epoch756 Batch0 Loss: 9747.7490\n",
      "Epoch756 Batch2 Loss: 11132.3789\n",
      "shuffling the dataset\n",
      "Epoch756 Batch4 Loss: 9039.7490\n",
      "train Loss: 8089.5930\n",
      "Epoch 757/499\n",
      "----------\n",
      "Epoch757 Batch0 Loss: 6957.4893\n",
      "Epoch757 Batch2 Loss: 12575.8242\n",
      "Epoch757 Batch4 Loss: 6204.0742\n",
      "shuffling the dataset\n",
      "train Loss: 7844.5730\n",
      "Epoch 758/499\n",
      "----------\n",
      "Epoch758 Batch0 Loss: 4643.0957\n",
      "Epoch758 Batch2 Loss: 7882.3848\n",
      "Epoch758 Batch4 Loss: 10398.5117\n",
      "shuffling the dataset\n",
      "train Loss: 7208.1891\n",
      "Epoch 759/499\n",
      "----------\n",
      "Epoch759 Batch0 Loss: 7092.7407\n",
      "Epoch759 Batch2 Loss: 11150.9189\n",
      "Epoch759 Batch4 Loss: 6409.9629\n",
      "shuffling the dataset\n",
      "train Loss: 6461.5420\n",
      "Epoch 760/499\n",
      "----------\n",
      "Epoch760 Batch0 Loss: 5018.5933\n",
      "Epoch760 Batch2 Loss: 9921.5225\n",
      "Epoch760 Batch4 Loss: 5797.2261\n",
      "shuffling the dataset\n",
      "train Loss: 6576.9380\n",
      "Epoch 761/499\n",
      "----------\n",
      "Epoch761 Batch0 Loss: 8207.2109\n",
      "Epoch761 Batch2 Loss: 4306.8394\n",
      "Epoch761 Batch4 Loss: 6277.2036\n",
      "train Loss: 6346.8500\n",
      "Epoch 762/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch762 Batch0 Loss: 4711.9312\n",
      "Epoch762 Batch2 Loss: 7243.1479\n",
      "Epoch762 Batch4 Loss: 4787.9478\n",
      "train Loss: 6053.8723\n",
      "Epoch 763/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch763 Batch0 Loss: 6232.3848\n",
      "Epoch763 Batch2 Loss: 7939.4863\n",
      "Epoch763 Batch4 Loss: 5260.3721\n",
      "train Loss: 6186.3363\n",
      "Epoch 764/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch764 Batch0 Loss: 5430.1934\n",
      "Epoch764 Batch2 Loss: 6213.9688\n",
      "Epoch764 Batch4 Loss: 4438.6147\n",
      "train Loss: 5976.4175\n",
      "Epoch 765/499\n",
      "----------\n",
      "shuffling the dataset\n",
      "Epoch765 Batch0 Loss: 6179.4292\n",
      "Epoch765 Batch2 Loss: 7003.5679\n",
      "Epoch765 Batch4 Loss: 4736.4502\n",
      "train Loss: 5458.6075\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unknown error -1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e8f1aa297b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/checkpoints/checkpoint-epoch_{}.pth'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-cuda9.0/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-cuda9.0/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-cuda9.0/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-cuda9.0/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mserialized_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unknown error -1"
     ]
    }
   ],
   "source": [
    "###################### Train Model #############################\n",
    "# Calculate total iter_num\n",
    "total_iter_num = 2467\n",
    "\n",
    "for epoch in range(411, 411+opt.num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, opt.num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "    # Iterate over data.\n",
    "    for i in range(int(dataloader.size()/opt.batchSize)):\n",
    "        total_iter_num += 1\n",
    "        \n",
    "        # Get data\n",
    "        inputs, labels =  dataloader.get_batch()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # We need to flatten annotations and logits to apply index of valid annotations.\n",
    "        anno_flatten = flatten_annotations(labels)\n",
    "        index = get_valid_annotations_index(anno_flatten, mask_out_value=255)\n",
    "        anno_flatten_valid = torch.index_select(anno_flatten, 0, index)\n",
    "        \n",
    "        ## Create Graph ##\n",
    "        if graph_created == False:\n",
    "            graph_created = True\n",
    "            writer.add_graph(model, inputs, verbose=False)\n",
    "        \n",
    "        # Forward Prop\n",
    "        optimizer.zero_grad()\n",
    "        torch.set_grad_enabled(True)\n",
    "        logits = model(inputs)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        logits_flatten = flatten_logits(logits, number_of_classes=opt.num_classes)\n",
    "        logits_flatten_valid = torch.index_select(logits_flatten, 0, index)\n",
    "        loss = criterion(logits_flatten_valid, anno_flatten_valid)\n",
    "        \n",
    "        # Backward Prop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar('loss', loss.item(), total_iter_num)\n",
    "        \n",
    "        if (i % 2 == 0):\n",
    "            print('Epoch{} Batch{} Loss: {:.4f}'.format(epoch, i, loss.item()))\n",
    "\n",
    "    epoch_loss = running_loss / (dataloader.size()/opt.batchSize)\n",
    "    writer.add_scalar('epoch_loss', epoch_loss, epoch)\n",
    "    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    directory = opt.logs_path+'/checkpoints/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    if (epoch % 5 == 0):\n",
    "        filename = opt.logs_path + '/checkpoints/checkpoint-epoch_{}.pth'.format(epoch,i)\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "checkpoint_path = opt.logs_path + '/checkpoints/checkpoint-epoch_{}.pth'.format(epoch,i)\n",
    "print(\"Initializing weights from: {}...\".format(checkpoint_path))\n",
    "# net.load_state_dict(torch.load(checkpoint_path))  # Load all tensors onto the CPU\n",
    "\n",
    "# Load all tensors onto the CPU\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "  \n",
    "model.eval()\n",
    "\n",
    "for ii, sample_batched in enumerate(dataloader.size()/opt.batchSize):\n",
    "    \n",
    "    # Get data\n",
    "    inputs, labels =  dataloader.get_batch()\n",
    "\n",
    "    # Forward pass of the mini-batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Forward Prop\n",
    "    optimizer.zero_grad()\n",
    "    torch.set_grad_enabled(True)\n",
    "    logits = model(inputs)\n",
    "\n",
    "    predictions = torch.max(logits[:3], 1)[1].detach().cpu().numpy()\n",
    "\n",
    "#     output_rgb = utils.decode_seg_map_sequence(predictions)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax0 = plt.subplot(121)\n",
    "    ax1 = plt.subplot(122)\n",
    "\n",
    "    output_rgb = output_rgb.detach().cpu().numpy().squeeze(0)\n",
    "    output_rgb = np.transpose(output_rgb, (1, 2, 0))\n",
    "    # print(output_rgb.shape)\n",
    "\n",
    "    ax0.imshow(rgb_img)\n",
    "    ax0.set_title('Source RGB Image')  # subplot 211 title\n",
    "    ax1.imshow(output_rgb)\n",
    "    ax1.set_title('Predicted Normals')\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    fig.savefig('data/results/%04d-results.png' % (ii))\n",
    "    plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
